{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/ps-s6e1-ridge-xgb-fe?scriptVersionId=289994921\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Acknowledgement: \n\n[https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe](https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe)\n[https://www.kaggle.com/code/act18l/s6e1-single-xgb-add-categorymean](https://www.kaggle.com/code/act18l/s6e1-single-xgb-add-categorymean)","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries and Loading the Data ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import root_mean_squared_error,mean_absolute_error\nfrom sklearn.preprocessing import TargetEncoder\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\ntrain_file = \"/kaggle/input/playground-series-s6e1/train.csv\"\ntest_file = \"/kaggle/input/playground-series-s6e1/test.csv\"\noriginal_file = \"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\noriginal_df = pd.read_csv(original_file)\n\nsubmission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n\nTARGET = \"exam_score\"\nID_COL = \"id\"\n\ntrain_df.shape, test_df.shape, original_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:57:04.993147Z","iopub.execute_input":"2026-01-04T10:57:04.993493Z","iopub.status.idle":"2026-01-04T10:57:15.626487Z","shell.execute_reply.started":"2026-01-04T10:57:04.993462Z","shell.execute_reply":"2026-01-04T10:57:15.625704Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"((630000, 13), (270000, 12), (20000, 13))"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"### Base features","metadata":{}},{"cell_type":"code","source":"base_features = [col for col in train_df.columns if col not in [TARGET, ID_COL]]\n\n\nCATS = train_df.select_dtypes(\"object\").columns.to_list()\nprint(\"CATS:\", CATS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:57:15.628002Z","iopub.execute_input":"2026-01-04T10:57:15.628282Z","iopub.status.idle":"2026-01-04T10:57:15.666349Z","shell.execute_reply.started":"2026-01-04T10:57:15.628261Z","shell.execute_reply":"2026-01-04T10:57:15.665642Z"}},"outputs":[{"name":"stdout","text":"CATS: ['gender', 'course', 'internet_access', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"\n\nclass CategoryMeanTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, cat_cols=None):\n        self.cat_cols = cat_cols\n        self.mappings_ = {}\n    \n    def fit(self, X, y):\n        X = X.copy()\n        if self.cat_cols is None:\n            self.cat_cols = X.select_dtypes(include=['category', 'object']).columns.tolist()\n        self.mappings_ = {}\n        for col in self.cat_cols:\n            df_temp = pd.DataFrame({col: X[col], 'y': y})\n            group_means = df_temp.groupby(col, dropna=False)['y'].mean()\n            sorted_categories = group_means.sort_values().index\n            self.mappings_[col] = {cat: i for i, cat in enumerate(sorted_categories)}\n        return self\n\n    def transform(self, X, y=None):\n        X = X.copy()\n        for col, mapping in self.mappings_.items():\n            if col in X.columns:\n                X[col] = X[col].map(mapping).astype(np.float32)\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:57:15.667349Z","iopub.execute_input":"2026-01-04T10:57:15.667713Z","iopub.status.idle":"2026-01-04T10:57:15.674719Z","shell.execute_reply.started":"2026-01-04T10:57:15.667685Z","shell.execute_reply":"2026-01-04T10:57:15.673943Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def preprocess(df):\n\n    df_temp = df.copy()\n    eps = 1e-5\n    \n    sh_pos = df_temp['study_hours'].clip(lower=0)\n    ca_pos = df_temp['class_attendance'].clip(lower=0)\n    sl_pos = df_temp['sleep_hours'].clip(lower=0)\n    ag_pos = df_temp['age'].clip(lower=0)\n    \n    df_temp['study_hours_squared'] = df_temp['study_hours'] ** 2\n    df_temp['study_hours_cubed'] = df_temp['study_hours'] ** 3\n    df_temp['study_hours_quartic'] = df_temp['study_hours'] ** 4\n    df_temp['class_attendance_squared'] = df_temp['class_attendance'] ** 2\n    df_temp['class_attendance_cubed'] = df_temp['class_attendance'] ** 3\n    df_temp['sleep_hours_squared'] = df_temp['sleep_hours'] ** 2\n    df_temp['sleep_hours_cubed'] = df_temp['sleep_hours'] ** 3\n    df_temp['age_squared'] = df_temp['age'] ** 2\n    df_temp['age_cubed'] = df_temp['age'] ** 3\n    \n    df_temp['log_study_hours'] = np.log1p(sh_pos)\n    df_temp['log_class_attendance'] = np.log1p(ca_pos)\n    df_temp['log_sleep_hours'] = np.log1p(sl_pos)\n    df_temp['sqrt_study_hours'] = np.sqrt(sh_pos)\n    df_temp['sqrt_class_attendance'] = np.sqrt(ca_pos)\n    \n    df_temp['inv_sleep'] = 1.0 / (sl_pos + 1.0)\n    df_temp['inv_study'] = 1.0 / (sh_pos + 1.0)\n    df_temp['inv_attendance'] = 1.0 / (ca_pos + 1.0)\n    \n    df_temp['study_tanh'] = np.tanh(df_temp['study_hours'] / 10.0)\n    df_temp['sleep_tanh'] = np.tanh(df_temp['sleep_hours'] / 10.0)\n    df_temp['attendance_tanh'] = np.tanh(df_temp['class_attendance'] / 100.0)\n    \n    df_temp['study_sigmoid'] = 1.0 / (1.0 + np.exp(-(df_temp['study_hours'] - 5.0)))\n    df_temp['sleep_sigmoid'] = 1.0 / (1.0 + np.exp(-(df_temp['sleep_hours'] - 7.0)))\n    df_temp['attendance_sigmoid'] = 1.0 / (1.0 + np.exp(-(df_temp['class_attendance'] - 85.0) / 8.0))\n    \n    df_temp['study_hours_times_attendance'] = df_temp['study_hours'] * df_temp['class_attendance']\n    df_temp['study_hours_times_sleep'] = df_temp['study_hours'] * df_temp['sleep_hours']\n    df_temp['attendance_times_sleep'] = df_temp['class_attendance'] * df_temp['sleep_hours']\n    df_temp['age_times_study_hours'] = df_temp['age'] * df_temp['study_hours']\n    df_temp['age_times_attendance'] = df_temp['age'] * df_temp['class_attendance']\n    df_temp['age_times_sleep_hours'] = df_temp['age'] * df_temp['sleep_hours']\n    \n    df_temp['study_center_5'] = df_temp['study_hours'] - 5.0\n    df_temp['sleep_center_7'] = df_temp['sleep_hours'] - 7.0\n    df_temp['att_center_85'] = df_temp['class_attendance'] - 85.0\n    df_temp['study_center_sq'] = df_temp['study_center_5'] ** 2\n    df_temp['sleep_center_sq'] = df_temp['sleep_center_7'] ** 2\n    df_temp['att_center_sq'] = df_temp['att_center_85'] ** 2\n    \n    df_temp['study_hours_over_sleep'] = df_temp['study_hours'] / (df_temp['sleep_hours'] + eps)\n    df_temp['attendance_over_sleep'] = df_temp['class_attendance'] / (df_temp['sleep_hours'] + eps)\n    df_temp['attendance_over_study'] = df_temp['class_attendance'] / (df_temp['study_hours'] + eps)\n    df_temp['sleep_over_study'] = df_temp['sleep_hours'] / (df_temp['study_hours'] + eps)\n    df_temp['study_over_age'] = df_temp['study_hours'] / (df_temp['age'] + eps)\n    df_temp['attendance_over_age'] = df_temp['class_attendance'] / (df_temp['age'] + eps)\n    \n    df_temp['study_hours_clip'] = df_temp['study_hours'].clip(0, 12)\n    df_temp['sleep_hours_clip'] = df_temp['sleep_hours'].clip(0, 12)\n    df_temp['attendance_clip'] = df_temp['class_attendance'].clip(0, 100)\n    \n    df_temp['sleep_gap_8'] = (df_temp['sleep_hours'] - 8.0).abs()\n    df_temp['sleep_gap_7'] = (df_temp['sleep_hours'] - 7.0).abs()\n    df_temp['attendance_gap_100'] = (df_temp['class_attendance'] - 100.0).abs()\n    df_temp['attendance_gap_90'] = (df_temp['class_attendance'] - 90.0).abs()\n    df_temp['study_gap_6'] = (df_temp['study_hours'] - 6.0).abs()\n    df_temp['study_gap_8'] = (df_temp['study_hours'] - 8.0).abs()\n    \n    df_temp['age_bin_num'] = pd.cut(df_temp['age'], bins=[0, 17, 19, 21, 23, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n    df_temp['study_bin_num'] = pd.cut(df_temp['study_hours'], bins=[-1, 2, 4, 6, 8, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n    df_temp['sleep_bin_num'] = pd.cut(df_temp['sleep_hours'], bins=[-1, 5, 6, 7, 8, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n    df_temp['attendance_bin_num'] = pd.cut(df_temp['class_attendance'], bins=[-1, 60, 75, 85, 95, 101], labels=[0, 1, 2, 3, 4]).astype(float)\n    \n\n    sleep_quality_map = {'poor': 0, 'average': 1, 'good': 2}\n    facility_rating_map = {'low': 0, 'medium': 1, 'high': 2}\n    exam_difficulty_map = {'easy': 0, 'moderate': 1, 'hard': 2}\n    gender_map = {'male': 0, 'female': 1}\n    internet_access_map = {'no': 0, 'yes': 1}\n    \n\n    df_temp['sleep_quality_numeric'] = df_temp['sleep_quality'].map(sleep_quality_map).fillna(1).astype(int)\n    df_temp['facility_rating_numeric'] = df_temp['facility_rating'].map(facility_rating_map).fillna(1).astype(int)\n    df_temp['exam_difficulty_numeric'] = df_temp['exam_difficulty'].map(exam_difficulty_map).fillna(1).astype(int)\n    df_temp['gender_numeric'] = df_temp['gender'].map(gender_map).fillna(0).astype(int) if 'gender' in df_temp.columns else 0\n    df_temp['internet_access_numeric'] = df_temp['internet_access'].map(internet_access_map).fillna(0).astype(int) if 'internet_access' in df_temp.columns else 0\n    \n  \n    if 'study_method' in df_temp.columns:\n        study_methods = df_temp['study_method'].unique()\n        study_method_map = {method: i for i, method in enumerate(sorted(study_methods))}\n        df_temp['study_method_numeric'] = df_temp['study_method'].map(study_method_map).fillna(0).astype(int)\n    else:\n        df_temp['study_method_numeric'] = 0\n    \n    if 'course' in df_temp.columns:\n        courses = df_temp['course'].unique()\n        course_map = {course: i for i, course in enumerate(sorted(courses))}\n        df_temp['course_numeric'] = df_temp['course'].map(course_map).fillna(0).astype(int)\n    else:\n        df_temp['course_numeric'] = 0\n    \n\n    df_temp['study_hours_times_sleep_quality'] = df_temp['study_hours'] * df_temp['sleep_quality_numeric']\n    df_temp['attendance_times_facility'] = df_temp['class_attendance'] * df_temp['facility_rating_numeric']\n    df_temp['sleep_hours_times_difficulty'] = df_temp['sleep_hours'] * df_temp['exam_difficulty_numeric']\n    \n    df_temp['facility_x_sleepq'] = df_temp['facility_rating_numeric'] * df_temp['sleep_quality_numeric']\n    df_temp['difficulty_x_facility'] = df_temp['exam_difficulty_numeric'] * df_temp['facility_rating_numeric']\n    df_temp['difficulty_x_sleepq'] = df_temp['exam_difficulty_numeric'] * df_temp['sleep_quality_numeric']\n    \n    df_temp['high_att_low_sleep'] = ((df_temp['class_attendance'] >= 90) & (df_temp['sleep_hours'] <= 6)).astype(int)\n    df_temp['high_att_high_study'] = ((df_temp['class_attendance'] >= 90) & (df_temp['study_hours'] >= 6)).astype(int)\n    df_temp['low_att_high_study'] = ((df_temp['class_attendance'] <= 60) & (df_temp['study_hours'] >= 7)).astype(int)\n    df_temp['ideal_sleep_flag'] = ((df_temp['sleep_hours'] >= 7) & (df_temp['sleep_hours'] <= 9)).astype(int)\n    df_temp['short_sleep_flag'] = (df_temp['sleep_hours'] <= 5.5).astype(int)\n    df_temp['high_study_flag'] = (df_temp['study_hours'] >= 7).astype(int)\n    \n    df_temp['efficiency'] = (df_temp['study_hours'] * df_temp['class_attendance']) / (df_temp['sleep_hours'] + 1)\n    df_temp['efficiency2'] = (df_temp['study_hours_clip'] * df_temp['attendance_clip']) / (df_temp['sleep_hours_clip'] + 1)\n    df_temp['weighted_sum'] = (0.06 * df_temp['class_attendance'] + 2.0 * df_temp['study_hours'] + 1.2 * df_temp['sleep_hours'])\n    df_temp['weighted_sum_x_difficulty'] = df_temp['weighted_sum'] * (1.0 + 0.2 * df_temp['exam_difficulty_numeric'])\n\n    df_temp['study_rank'] = sh_pos.rank(pct=True)\n    df_temp['attendance_rank'] = ca_pos.rank(pct=True)\n    df_temp['sleep_rank'] = sl_pos.rank(pct=True)\n    df_temp['age_rank'] = ag_pos.rank(pct=True)\n\n    df_temp['study_z'] = (sh_pos - sh_pos.mean()) / (sh_pos.std() + eps)\n    df_temp['attendance_z'] = (ca_pos - ca_pos.mean()) / (ca_pos.std() + eps)\n    df_temp['sleep_z'] = (sl_pos - sl_pos.mean()) / (sl_pos.std() + eps)\n\n    df_temp['harmonic_effort'] = 3 / (\n        (1 / (sh_pos + eps)) +\n        (1 / (ca_pos + eps)) +\n        (1 / (sl_pos + eps))\n    )\n\n    df_temp['geo_effort'] = (\n        (sh_pos + 1) *\n        (ca_pos + 1) *\n        (sl_pos + 1)\n    ) ** (1 / 3)\n\n    df_temp['study_above_6'] = np.maximum(0, sh_pos - 6)\n    df_temp['study_above_8'] = np.maximum(0, sh_pos - 8)\n    df_temp['sleep_below_6'] = np.maximum(0, 6 - sl_pos)\n    df_temp['attendance_below_75'] = np.maximum(0, 75 - ca_pos)\n\n    df_temp['log_study_sleep_ratio'] = np.log1p(sh_pos) - np.log1p(sl_pos)\n    df_temp['log_att_study_ratio'] = np.log1p(ca_pos) - np.log1p(sh_pos)\n    \n\n    df_temp['study_method_x_study_hours'] = df_temp['study_hours'] * df_temp['study_method_numeric']\n    df_temp['course_difficulty'] = df_temp['course_numeric'] * df_temp['exam_difficulty_numeric']\n    df_temp['internet_x_efficiency'] = (df_temp['study_hours'] * df_temp['class_attendance'] / \n                                        (df_temp['sleep_hours'] + 1)) * df_temp['internet_access_numeric']\n    \n\n    df_temp['sqrt_study_hours_x_attendance'] = df_temp['sqrt_study_hours'] * df_temp['class_attendance']\n    df_temp['efficiency_cubed'] = ((df_temp['study_hours'] * df_temp['class_attendance'] / \n                                    (df_temp['sleep_hours'] + 1)) ** 1.5).fillna(0)\n    \n\n    df_temp['sleep_quality_x_study_z'] = df_temp['sleep_quality_numeric'] * df_temp['study_z']\n    df_temp['facility_x_attendance'] = df_temp['facility_rating_numeric'] * df_temp['class_attendance']\n    \n\n    df_temp['study_hours_x_sleep_quality'] = df_temp['study_hours'] * df_temp['sleep_quality_numeric']\n    df_temp['attendance_x_internet'] = df_temp['class_attendance'] * df_temp['internet_access_numeric']\n    df_temp['course_x_attendance'] = df_temp['course_numeric'] * df_temp['class_attendance']\n    df_temp['study_method_x_efficiency'] = df_temp['study_method_numeric'] * df_temp['efficiency']\n    \n\n    \n    numeric_features = [\n        'study_hours_squared', 'study_hours_cubed', 'study_hours_quartic',\n        'class_attendance_squared', 'class_attendance_cubed',\n        'sleep_hours_squared', 'sleep_hours_cubed',\n        'age_squared', 'age_cubed',\n        'log_study_hours', 'log_class_attendance', 'log_sleep_hours',\n        'sqrt_study_hours', 'sqrt_class_attendance',\n        'inv_sleep', 'inv_study', 'inv_attendance',\n        'study_tanh', 'sleep_tanh', 'attendance_tanh',\n        'study_sigmoid', 'sleep_sigmoid', 'attendance_sigmoid',\n        'study_hours_times_attendance', 'study_hours_times_sleep', 'attendance_times_sleep',\n        'age_times_study_hours', 'age_times_attendance', 'age_times_sleep_hours',\n        'study_center_5', 'sleep_center_7', 'att_center_85',\n        'study_center_sq', 'sleep_center_sq', 'att_center_sq',\n        'study_hours_over_sleep', 'attendance_over_sleep',\n        'attendance_over_study', 'sleep_over_study',\n        'study_over_age', 'attendance_over_age',\n        'study_hours_clip', 'sleep_hours_clip', 'attendance_clip',\n        'sleep_gap_8', 'sleep_gap_7',\n        'attendance_gap_100', 'attendance_gap_90',\n        'study_gap_6', 'study_gap_8',\n        'age_bin_num', 'study_bin_num', 'sleep_bin_num', 'attendance_bin_num',\n        'sleep_quality_numeric', 'facility_rating_numeric', 'exam_difficulty_numeric',\n        'study_hours_times_sleep_quality', 'attendance_times_facility', 'sleep_hours_times_difficulty',\n        'facility_x_sleepq', 'difficulty_x_facility', 'difficulty_x_sleepq',\n        'high_att_low_sleep', 'high_att_high_study', 'low_att_high_study',\n        'ideal_sleep_flag', 'short_sleep_flag', 'high_study_flag',\n        'efficiency', 'efficiency2',\n        'weighted_sum', 'weighted_sum_x_difficulty',\n        'study_rank', 'attendance_rank', 'sleep_rank', 'age_rank',\n        'study_z', 'attendance_z', 'sleep_z',\n        'harmonic_effort', 'geo_effort',\n        'study_above_6', 'study_above_8',\n        'sleep_below_6', 'attendance_below_75',\n        'log_study_sleep_ratio', 'log_att_study_ratio',\n        'study_method_x_study_hours', 'course_difficulty', 'internet_x_efficiency',\n        'sqrt_study_hours_x_attendance', 'efficiency_cubed',\n        'sleep_quality_x_study_z', 'facility_x_attendance',\n        'study_hours_x_sleep_quality', 'attendance_x_internet',\n        'course_x_attendance', 'study_method_x_efficiency',\n    ]\n    \n    return df_temp[base_features + numeric_features], numeric_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:57:15.675866Z","iopub.execute_input":"2026-01-04T10:57:15.67617Z","iopub.status.idle":"2026-01-04T10:57:15.85817Z","shell.execute_reply.started":"2026-01-04T10:57:15.676142Z","shell.execute_reply":"2026-01-04T10:57:15.857299Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Preprocessing and Preparing the Data","metadata":{}},{"cell_type":"code","source":"X_raw, numeric_cols = preprocess(train_df)\ny = train_df[TARGET].reset_index(drop=True)\n\nX_test_raw, _ = preprocess(test_df)\nX_orig_raw, _ = preprocess(original_df)\ny_orig = original_df[TARGET].reset_index(drop=True)\n\n\ny = y.clip(0, 100)\ny_orig = y_orig.clip(0, 100)\n\nfull_data = pd.concat([X_raw, X_test_raw, X_orig_raw], axis=0, ignore_index=True)\n\n\nfor col in numeric_cols:\n    full_data[col] = full_data[col].astype(float)\n\n\nX = full_data.iloc[:len(train_df)].copy()\nX_test = full_data.iloc[len(train_df):len(train_df) + len(test_df)].copy()\nX_original = full_data.iloc[len(train_df) + len(test_df):].copy()\n\nprint(f\"Feature shapes - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T10:57:15.860008Z","iopub.execute_input":"2026-01-04T10:57:15.860249Z","iopub.status.idle":"2026-01-04T10:57:19.715853Z","shell.execute_reply.started":"2026-01-04T10:57:15.860228Z","shell.execute_reply":"2026-01-04T10:57:19.715023Z"}},"outputs":[{"name":"stdout","text":"Feature shapes - X: (630000, 110), X_test: (270000, 110), X_original: (20000, 110)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Ridge Regression ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"IMPROVED RIDGE REGRESSION WITH OPTIMIZATION\")\nprint(\"=\"*50)\n\nFOLDS = 10\ny_bins = pd.qcut(y, q=10, labels=False, duplicates='drop').astype(int)\nkf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1003)\n\nscalers_ridge = []\nN_SAMPLES_TRAIN = X.shape[0]\nN_SAMPLES_TEST = X_test.shape[0]\n\noof_pred_lr = np.zeros(N_SAMPLES_TRAIN)\ntest_preds_lr = np.zeros((N_SAMPLES_TEST, FOLDS))\norig_preds_lr = np.zeros(X_original.shape[0])\n\nfold_rmse_lr = []\nlr_models = []\ntarget_encoders = []\n\n\nalphas = np.concatenate([\n    np.logspace(-3, -1, 20),      \n    np.logspace(-1, 1, 30),      \n    np.logspace(1, 3, 20),        \n])\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y_bins), start=1):\n    print(f\"\\nTraining fold {fold} (Ridge)...\")\n    \n    X_train_fold, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n    \n \n    X_train_combined = pd.concat([X_train_fold, X_original], axis=0)\n    y_train_combined = pd.concat([y_train_fold, y_orig], axis=0)\n    \n\n    target_encoder = TargetEncoder(smooth='auto', target_type='continuous')\n    \n    X_train_encoded = X_train_combined.copy()\n    X_val_encoded = X_val.copy()\n    X_test_encoded = X_test.copy()\n    \n    X_train_encoded[CATS] = target_encoder.fit_transform(X_train_combined[CATS], y_train_combined)\n    X_val_encoded[CATS] = target_encoder.transform(X_val[CATS])\n    X_test_encoded[CATS] = target_encoder.transform(X_test[CATS])\n    \n\n    scaler = StandardScaler()\n    \n    X_train_scaled = X_train_encoded.copy()\n    X_val_scaled = X_val_encoded.copy()\n    X_test_scaled = X_test_encoded.copy()\n    \n    X_train_scaled[:] = scaler.fit_transform(X_train_encoded)\n    X_val_scaled[:] = scaler.transform(X_val_encoded)\n    X_test_scaled[:] = scaler.transform(X_test_encoded)\n    \n    scalers_ridge.append(scaler)\n    \n\n\n    lr_model = RidgeCV(\n        alphas=alphas, \n        cv=10,  \n        scoring='neg_mean_squared_error', \n        alpha_per_target=False\n    )\n    lr_model.fit(X_train_scaled, y_train_combined.to_numpy().ravel())\n    lr_models.append(lr_model)\n    target_encoders.append(target_encoder)\n    \n\n    lr_val_pred = lr_model.predict(X_val_scaled)\n    lr_test_pred = lr_model.predict(X_test_scaled)\n    lr_orig_pred = lr_model.predict(X_train_scaled.iloc[-X_original.shape[0]:])\n    \n\n    lr_val_pred = np.clip(lr_val_pred, 0, 100)\n    lr_test_pred = np.clip(lr_test_pred, 0, 100)\n    lr_orig_pred = np.clip(lr_orig_pred, 0, 100)\n    \n    oof_pred_lr[val_index] = lr_val_pred\n    test_preds_lr[:, fold - 1] = lr_test_pred\n    orig_preds_lr += lr_orig_pred / FOLDS\n    \n    rmse_lr = root_mean_squared_error(y_val, lr_val_pred)\n    fold_rmse_lr.append(rmse_lr)\n    \n    print(f\"  Fold {fold} RMSE: {rmse_lr:.6f}\")\n    print(f\"  Alpha selected: {lr_model.alpha_:.6f}\")\n    \n\nridge_oof_rmse = root_mean_squared_error(y, oof_pred_lr)\n\nprint(f\"\\n\" + \"=\"*50)\nprint(\"RIDGE REGRESSION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Ridge OOF RMSE: {ridge_oof_rmse:.6f}\")\nprint(f\"Ridge Fold RMSE Mean: {np.mean(fold_rmse_lr):.6f} ± {np.std(fold_rmse_lr):.6f}\")\nprint(f\"Min fold RMSE: {np.min(fold_rmse_lr):.6f}\")\nprint(f\"Max fold RMSE: {np.max(fold_rmse_lr):.6f}\")\n\n\nprint(f\"\\n\" + \"=\"*50)\nprint(\" ELASTICNET COMPARISON\")\nprint(\"=\"*50)\n\noof_pred_elastic = np.zeros(N_SAMPLES_TRAIN)\ntest_preds_elastic = np.zeros((N_SAMPLES_TEST, FOLDS))\nfold_rmse_elastic = []\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y_bins), start=1):\n    X_train_fold, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n    \n    X_train_combined = pd.concat([X_train_fold, X_original], axis=0)\n    y_train_combined = pd.concat([y_train_fold, y_orig], axis=0)\n    \n    target_encoder = TargetEncoder(smooth='auto', target_type='continuous')\n    \n    X_train_encoded = X_train_combined.copy()\n    X_val_encoded = X_val.copy()\n    X_test_encoded = X_test.copy()\n    \n    X_train_encoded[CATS] = target_encoder.fit_transform(X_train_combined[CATS], y_train_combined)\n    X_val_encoded[CATS] = target_encoder.transform(X_val[CATS])\n    X_test_encoded[CATS] = target_encoder.transform(X_test[CATS])\n    \n    scaler = StandardScaler()\n    \n    X_train_scaled = scaler.fit_transform(X_train_encoded)\n    X_val_scaled = scaler.transform(X_val_encoded)\n    X_test_scaled = scaler.transform(X_test_encoded)\n    \n    \n    elastic_model = ElasticNetCV(\n        l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n        alphas=np.logspace(-3, 2, 100),\n        cv=10,\n        max_iter=10000,\n        random_state=42\n    )\n    elastic_model.fit(X_train_scaled, y_train_combined.to_numpy().ravel())\n    \n    elastic_val_pred = np.clip(elastic_model.predict(X_val_scaled), 0, 100)\n    elastic_test_pred = np.clip(elastic_model.predict(X_test_scaled), 0, 100)\n    \n    oof_pred_elastic[val_index] = elastic_val_pred\n    test_preds_elastic[:, fold - 1] = elastic_test_pred\n    \n    rmse_elastic = root_mean_squared_error(y_val, elastic_val_pred)\n    fold_rmse_elastic.append(rmse_elastic)\n\nelastic_oof_rmse = root_mean_squared_error(y, oof_pred_elastic)\n\nprint(f\"ElasticNet OOF RMSE: {elastic_oof_rmse:.6f}\")\nprint(f\"ElasticNet Fold RMSE Mean: {np.mean(fold_rmse_elastic):.6f} ± {np.std(fold_rmse_elastic):.6f}\")\n\n\nprint(f\"\\n\" + \"=\"*50)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*50)\n\nif elastic_oof_rmse < ridge_oof_rmse:\n    print(f\"   ElasticNet is better!\")\n    print(f\"   Ridge: {ridge_oof_rmse:.6f}\")\n    print(f\"   ElasticNet: {elastic_oof_rmse:.6f}\")\n    print(f\"   Improvement: {ridge_oof_rmse - elastic_oof_rmse:.6f}\")\n    \n  \n    oof_pred_lr = oof_pred_elastic\n    test_preds_lr = test_preds_elastic\n    linear_oof_rmse = elastic_oof_rmse\nelse:\n    print(f\"   Ridge is better!\")\n    print(f\"   Ridge: {ridge_oof_rmse:.6f}\")\n    print(f\"   ElasticNet: {elastic_oof_rmse:.6f}\")\n    \n    linear_oof_rmse = ridge_oof_rmse\n\nprint(f\"\\n  Final Linear Model RMSE: {linear_oof_rmse:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T11:10:20.473369Z","iopub.execute_input":"2026-01-04T11:10:20.474015Z","execution_failed":"2026-01-04T13:48:44.802Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nIMPROVED RIDGE REGRESSION WITH OPTIMIZATION\n==================================================\n\nTraining fold 1 (Ridge)...\n  Fold 1 RMSE: 8.833441\n  Alpha selected: 2.807216\n\nTraining fold 2 (Ridge)...\n  Fold 2 RMSE: 8.899728\n  Alpha selected: 0.001000\n\nTraining fold 3 (Ridge)...\n  Fold 3 RMSE: 8.921972\n  Alpha selected: 2.807216\n\nTraining fold 4 (Ridge)...\n  Fold 4 RMSE: 8.849183\n  Alpha selected: 2.807216\n\nTraining fold 5 (Ridge)...\n  Fold 5 RMSE: 8.929922\n  Alpha selected: 0.001000\n\nTraining fold 6 (Ridge)...\n  Fold 6 RMSE: 8.846485\n  Alpha selected: 2.807216\n\nTraining fold 7 (Ridge)...\n  Fold 7 RMSE: 8.874272\n  Alpha selected: 2.807216\n\nTraining fold 8 (Ridge)...\n  Fold 8 RMSE: 8.925887\n  Alpha selected: 0.001000\n\nTraining fold 9 (Ridge)...\n  Fold 9 RMSE: 8.898398\n  Alpha selected: 0.001000\n\nTraining fold 10 (Ridge)...\n  Fold 10 RMSE: 8.848074\n  Alpha selected: 2.807216\n\n==================================================\nRIDGE REGRESSION RESULTS\n==================================================\nRidge OOF RMSE: 8.882806\nRidge Fold RMSE Mean: 8.882736 ± 0.035078\nMin fold RMSE: 8.833441\nMax fold RMSE: 8.929922\n\n==================================================\n ELASTICNET COMPARISON\n==================================================\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"FEATURE SELECTION\")\nprint(\"=\"*50)\n\n\nfeature_importance = pd.DataFrame({\n    'feature': X_train_encoded.columns,\n    'importance': np.abs(lr_models[0].coef_)\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 30 Features by Ridge importance:\")\nprint(feature_importance.head(30))\n\n\nn_features_to_keep = 95\ntop_features = feature_importance.head(n_features_to_keep)['feature'].tolist()\nprint(f\"\\nKeeping top {n_features_to_keep} features out of {len(feature_importance)}\")\n\n\nX = X[top_features]\nX_test = X_test[top_features]\nX_original = X_original[top_features]\n\nprint(f\"XGB feature shapes - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-04T13:48:44.803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing the Data with Categorical ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"PREPARING XGB DATA WITH CATEGORY MEAN ENCODING\")\nprint(\"=\"*50)\n\n\nX[\"ridge_pred\"] = oof_pred_lr\nX_test[\"ridge_pred\"] = test_preds_lr.mean(axis=1)\nX_original[\"ridge_pred\"] = orig_preds_lr\n\nprint(f\"Shapes before encoding - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")\n\n\ncat_cols = X.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\nprint(f\"Categorical columns to encode: {cat_cols}\")\n\ncat_transformer = CategoryMeanTransformer(cat_cols=cat_cols)\n\n\ncat_transformer.fit(X, y)\n\n\nX = cat_transformer.transform(X)\nX_test = cat_transformer.transform(X_test)\nX_original = cat_transformer.transform(X_original)\n\n\nX = X.astype(np.float32)\nX_test = X_test.astype(np.float32)\nX_original = X_original.astype(np.float32)\ny_float = y.values.astype(np.float32)\n\nprint(f\"Final XGB shapes with Ridge feature - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-04T13:48:44.803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### XGBoost Training ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 50)\nprint(\"TRAINING XGBOOST AND LIGHTGBM\")\nprint(\"=\" * 50)\n\n\ndtest = xgb.DMatrix(X_test)\n\nxgb_params = {\n    \"objective\": \"reg:squarederror\",\n    \"learning_rate\": 0.05,  \n    \"max_depth\": 8,        \n    \"subsample\": 0.85,     \n    \"colsample_bytree\": 0.75,  \n    \"colsample_bynode\": 0.8,  \n    \"min_child_weight\": 2,  \n    \"gamma\": 0.05,         \n    \"lambda\": 0.8,          \n    \"alpha\": 0.02,         \n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"hist\",\n    \"verbosity\": 0,\n    \"seed\": 42,\n}\n\n\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'max_depth': 8,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'min_child_samples': 20,\n    'lambda_l1': 0.1,\n    'lambda_l2': 0.1,\n    'verbose': -1,\n}\n\n\noof_predictions_xgb = np.zeros(len(X), dtype=np.float32)\noof_predictions_lgb = np.zeros(len(X), dtype=np.float32)\n\ntest_predictions_xgb = []\ntest_predictions_lgb = []\n\nfold_metrics = []\n\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y_bins), start=1):\n    print(f\"\\n{'='*50}\")\n    print(f\"Fold {fold}/5\")\n    print(f\"{'='*50}\")\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y_float[train_idx], y_float[val_idx]\n    \n\n    print(\"Training XGBoost...\")\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dval = xgb.DMatrix(X_val, label=y_val)\n    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n    \n    xgb_model = xgb.train(\n        params=xgb_params,\n        dtrain=dtrain,\n        num_boost_round=3000,\n        evals=evals,\n        early_stopping_rounds=50,\n        verbose_eval=False,\n    )\n    \n    xgb_val_preds = np.clip(xgb_model.predict(dval), 0, 100)\n    oof_predictions_xgb[val_idx] = xgb_val_preds\n    \n    xgb_test_pred = np.clip(xgb_model.predict(dtest), 0, 100)\n    test_predictions_xgb.append(xgb_test_pred)\n    \n    xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_preds))\n    xgb_mae = mean_absolute_error(y_val, xgb_val_preds)\n    \n    print(f\"  XGBoost RMSE: {xgb_rmse:.5f} | MAE: {xgb_mae:.5f}\")\n    print(f\"  Best iteration: {xgb_model.best_iteration}\")\n    \n  \n    print(\"Training LightGBM...\")\n    \n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n    \n \n    lgb_model = lgb.train(\n        params=lgb_params,\n        train_set=lgb_train,\n        num_boost_round=3000,\n        valid_sets=[lgb_train, lgb_val],\n        valid_names=['train', 'valid'],\n        callbacks=[\n            lgb.early_stopping(50),\n            lgb.log_evaluation(period=0)  \n        ]\n    )\n    \n    lgb_val_preds = np.clip(lgb_model.predict(X_val), 0, 100)\n    oof_predictions_lgb[val_idx] = lgb_val_preds\n    \n    lgb_test_pred = np.clip(lgb_model.predict(X_test), 0, 100)\n    test_predictions_lgb.append(lgb_test_pred)\n    \n    lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_val_preds))\n    lgb_mae = mean_absolute_error(y_val, lgb_val_preds)\n    \n    print(f\"  LightGBM RMSE: {lgb_rmse:.5f} | MAE: {lgb_mae:.5f}\")\n    print(f\"  Best iteration: {lgb_model.best_iteration}\")\n    \n \n    fold_metrics.append({\n        \"fold\": fold,\n        \"xgb_rmse\": xgb_rmse,\n        \"xgb_mae\": xgb_mae,\n        \"lgb_rmse\": lgb_rmse,\n        \"lgb_mae\": lgb_mae,\n    })\n\n\n\nprint(f\"\\n{'='*50}\")\nprint(\"AGGREGATING PREDICTIONS\")\nprint(f\"{'='*50}\\n\")\n\n\ntest_predictions_xgb = np.mean(test_predictions_xgb, axis=0)\ntest_predictions_lgb = np.mean(test_predictions_lgb, axis=0)\n\n\nxgb_oof_rmse = np.sqrt(mean_squared_error(y_float, oof_predictions_xgb))\nxgb_oof_mae = mean_absolute_error(y_float, oof_predictions_xgb)\n\nlgb_oof_rmse = np.sqrt(mean_squared_error(y_float, oof_predictions_lgb))\nlgb_oof_mae = mean_absolute_error(y_float, oof_predictions_lgb)\n\nprint(f\"XGBoost OOF RMSE: {xgb_oof_rmse:.5f} | MAE: {xgb_oof_mae:.5f}\")\nprint(f\"LightGBM OOF RMSE: {lgb_oof_rmse:.5f} | MAE: {lgb_oof_mae:.5f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-04T13:48:44.803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ensemble Blending ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"ENSEMBLE BLENDING WITH SOFT CLIPPING\")\nprint(\"=\"*50)\n\n\nprint(\"\\nTesting different ensemble weights...\\n\")\n\nbest_rmse = float('inf')\nbest_weights = None\n\n\nweight_combinations = [\n    (0.0, 0.5, 0.5),    \n    (0.0, 0.6, 0.4),    \n    (0.0, 0.4, 0.6),  \n    (0.05, 0.5, 0.45),  \n    (0.1, 0.45, 0.45),  \n    (0.0, 0.7, 0.3),   \n    (0.0, 0.3, 0.7),   \n]\n\nfor ridge_w, xgb_w, lgb_w in weight_combinations:\n    ensemble_oof = (ridge_w * oof_pred_lr + \n                    xgb_w * oof_predictions_xgb + \n                    lgb_w * oof_predictions_lgb)\n    \n    ensemble_oof = np.clip(ensemble_oof, 0, 100)\n    ensemble_rmse = np.sqrt(mean_squared_error(y, ensemble_oof))\n    \n    print(f\"Ridge: {ridge_w:.2f} | XGB: {xgb_w:.2f} | LGB: {lgb_w:.2f} → RMSE: {ensemble_rmse:.5f}\")\n    \n    if ensemble_rmse < best_rmse:\n        best_rmse = ensemble_rmse\n        best_weights = (ridge_w, xgb_w, lgb_w)\n\nridge_w, xgb_w, lgb_w = best_weights\n\nprint(f\"\\n  Best ensemble weights:\")\nprint(f\"   Ridge: {ridge_w:.2f} | XGB: {xgb_w:.2f} | LGB: {lgb_w:.2f}\")\nprint(f\"   Ensemble RMSE (before clipping): {best_rmse:.5f}\\n\")\n\n\nfinal_oof = (ridge_w * oof_pred_lr + \n             xgb_w * oof_predictions_xgb + \n             lgb_w * oof_predictions_lgb)\n\nfinal_test = (ridge_w * test_preds_lr.mean(axis=1) + \n              xgb_w * test_predictions_xgb + \n              lgb_w * test_predictions_lgb)\n\n\n\ndef soft_clip(pred, lower=0, upper=100):\n\n    scaled = lower + (upper - lower) / (1 + np.exp(-10 * (pred - 50) / 50))\n    return scaled\n\n\nlower_q = y.quantile(0.01)\nupper_q = y.quantile(0.99)\n\nprint(f\"Quantile bounds: [{lower_q:.2f}, {upper_q:.2f}]\")\n\n\nfinal_oof = np.clip(final_oof, 0, 100)\nfinal_test = np.clip(final_test, 0, 100)\n\n\nfinal_oof_rmse = np.sqrt(mean_squared_error(y, final_oof))\n\nprint(f\"\\n{'='*50}\")\nprint(\"FINAL MODEL PERFORMANCE\")\nprint(f\"{'='*50}\\n\")\n\nprint(f\"Individual Models:\")\nprint(f\"  Ridge OOF RMSE:    {ridge_oof_rmse:.5f}\")\nprint(f\"  XGBoost OOF RMSE:  {xgb_oof_rmse:.5f}\")\nprint(f\"  LightGBM OOF RMSE: {lgb_oof_rmse:.5f}\")\n\nprint(f\"\\nEnsemble (before clipping):\")\nprint(f\"  Ensemble RMSE: {best_rmse:.5f}\")\n\nprint(f\"\\nFinal (after clipping):\")\nprint(f\"  Final OOF RMSE: {final_oof_rmse:.5f}\")\nprint(f\"  Prediction range: [{final_test.min():.2f}, {final_test.max():.2f}]\")\n\nprint(f\"\\n{'='*50}\")\nprint(\"IMPROVEMENTS\")\nprint(f\"{'='*50}\")\nprint(f\"  vs Ridge:    {ridge_oof_rmse - final_oof_rmse:.5f}\")\nprint(f\"  vs XGBoost:  {xgb_oof_rmse - final_oof_rmse:.5f}\")\nprint(f\"  vs LightGBM: {lgb_oof_rmse - final_oof_rmse:.5f}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-04T13:48:44.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"SAVING RESULTS\")\nprint(\"=\"*50)\n\n\noof_df = pd.DataFrame({\n    \"id\": train_df[ID_COL], \n    TARGET: final_oof  \n})\n\noof_df.to_csv(\"oof_df.csv\", index=False)\n\n\nsubmission_df[TARGET] = final_test\nsubmission_df.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-04T13:48:44.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-04T13:48:44.804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Acknowledgement: [https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe](https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe)","metadata":{}}]}