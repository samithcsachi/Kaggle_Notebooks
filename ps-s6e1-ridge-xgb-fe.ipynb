{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/ps-s6e1-ridge-xgb-lgb-fe?scriptVersionId=290802410\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Acknowledgement: \n\n[https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe](https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe)\n[https://www.kaggle.com/code/act18l/s6e1-single-xgb-add-categorymean](https://www.kaggle.com/code/act18l/s6e1-single-xgb-add-categorymean)","metadata":{}},{"cell_type":"markdown","source":"### Importing Libraries and Loading the Data ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import root_mean_squared_error,mean_absolute_error\nfrom sklearn.preprocessing import TargetEncoder\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\ntrain_file = \"/kaggle/input/playground-series-s6e1/train.csv\"\ntest_file = \"/kaggle/input/playground-series-s6e1/test.csv\"\noriginal_file = \"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\noriginal_df = pd.read_csv(original_file)\n\nsubmission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n\nTARGET = \"exam_score\"\nID_COL = \"id\"\n\ntrain_df.shape, test_df.shape, original_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:28:19.911767Z","iopub.execute_input":"2026-01-08T15:28:19.912038Z","iopub.status.idle":"2026-01-08T15:28:30.971455Z","shell.execute_reply.started":"2026-01-08T15:28:19.911989Z","shell.execute_reply":"2026-01-08T15:28:30.970814Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"((630000, 13), (270000, 12), (20000, 13))"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"### Base features","metadata":{}},{"cell_type":"code","source":"base_features = [col for col in train_df.columns if col not in [TARGET, ID_COL]]\n\n\nCATS = train_df.select_dtypes(\"object\").columns.to_list()\nprint(\"CATS:\", CATS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:28:30.972668Z","iopub.execute_input":"2026-01-08T15:28:30.972904Z","iopub.status.idle":"2026-01-08T15:28:31.024915Z","shell.execute_reply.started":"2026-01-08T15:28:30.972884Z","shell.execute_reply":"2026-01-08T15:28:31.024239Z"}},"outputs":[{"name":"stdout","text":"CATS: ['gender', 'course', 'internet_access', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"\n\nclass CategoryMeanTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, cat_cols=None):\n        self.cat_cols = cat_cols\n        self.mappings_ = {}\n    \n    def fit(self, X, y):\n        X = X.copy()\n        if self.cat_cols is None:\n            self.cat_cols = X.select_dtypes(include=['category', 'object']).columns.tolist()\n        self.mappings_ = {}\n        for col in self.cat_cols:\n            df_temp = pd.DataFrame({col: X[col], 'y': y})\n            group_means = df_temp.groupby(col, dropna=False)['y'].mean()\n            sorted_categories = group_means.sort_values().index\n            self.mappings_[col] = {cat: i for i, cat in enumerate(sorted_categories)}\n        return self\n\n    def transform(self, X, y=None):\n        X = X.copy()\n        for col, mapping in self.mappings_.items():\n            if col in X.columns:\n                X[col] = X[col].map(mapping).astype(np.float32)\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:28:31.026205Z","iopub.execute_input":"2026-01-08T15:28:31.026479Z","iopub.status.idle":"2026-01-08T15:28:31.032934Z","shell.execute_reply.started":"2026-01-08T15:28:31.026458Z","shell.execute_reply":"2026-01-08T15:28:31.032411Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def preprocess(df):\n\n    df_temp = df.copy()\n    eps = 1e-5\n    \n\n    sh_pos = df_temp['study_hours'].clip(lower=0)\n    ca_pos = df_temp['class_attendance'].clip(lower=0)\n    sl_pos = df_temp['sleep_hours'].clip(lower=0)\n    ag_pos = df_temp['age'].clip(lower=0)\n    \n \n    df_temp['study_hours_squared'] = sh_pos ** 2\n    df_temp['study_hours_cubed'] = sh_pos ** 3\n    df_temp['study_hours_quartic'] = sh_pos ** 4\n    df_temp['study_hours_5th'] = sh_pos ** 5\n    \n    df_temp['class_attendance_squared'] = ca_pos ** 2\n    df_temp['class_attendance_cubed'] = ca_pos ** 3\n    df_temp['class_attendance_quartic'] = ca_pos ** 4\n    \n    df_temp['sleep_hours_squared'] = sl_pos ** 2\n    df_temp['sleep_hours_cubed'] = sl_pos ** 3\n    df_temp['sleep_hours_quartic'] = sl_pos ** 4\n    \n    df_temp['age_squared'] = ag_pos ** 2\n    df_temp['age_cubed'] = ag_pos ** 3\n    df_temp['age_quartic'] = ag_pos ** 4\n    \n \n    df_temp['log_study_hours'] = np.log1p(sh_pos)\n    df_temp['log_class_attendance'] = np.log1p(ca_pos)\n    df_temp['log_sleep_hours'] = np.log1p(sl_pos)\n    df_temp['log_age'] = np.log1p(ag_pos)\n    df_temp['log1p_study_hours'] = np.log1p(sh_pos + 1)\n    df_temp['log2_study_hours'] = np.log2(sh_pos + 1)\n    df_temp['log_study_hours_squared'] = (np.log1p(sh_pos)) ** 2\n    \n\n    df_temp['sqrt_study_hours'] = np.sqrt(sh_pos)\n    df_temp['sqrt_class_attendance'] = np.sqrt(ca_pos)\n    df_temp['sqrt_sleep_hours'] = np.sqrt(sl_pos)\n    df_temp['sqrt_age'] = np.sqrt(ag_pos)\n    \n    df_temp['cbrt_study_hours'] = np.cbrt(sh_pos)\n    df_temp['cbrt_class_attendance'] = np.cbrt(ca_pos)\n    df_temp['cbrt_sleep_hours'] = np.cbrt(sl_pos)\n    \n    df_temp['power_study_15'] = sh_pos ** 1.5\n    df_temp['power_attendance_15'] = ca_pos ** 1.5\n    df_temp['power_sleep_15'] = sl_pos ** 1.5\n    \n    df_temp['power_study_025'] = sh_pos ** 0.25\n    df_temp['power_attendance_025'] = ca_pos ** 0.25\n    \n\n    df_temp['inv_sleep'] = 1.0 / (sl_pos + 1.0)\n    df_temp['inv_study'] = 1.0 / (sh_pos + 1.0)\n    df_temp['inv_attendance'] = 1.0 / (ca_pos + 1.0)\n    df_temp['inv_age'] = 1.0 / (ag_pos + 1.0)\n    \n    df_temp['inv_sqrt_study'] = 1.0 / (np.sqrt(sh_pos) + eps)\n    df_temp['inv_sqrt_attendance'] = 1.0 / (np.sqrt(ca_pos) + eps)\n    \n\n    df_temp['study_tanh'] = np.tanh(sh_pos / 10.0)\n    df_temp['sleep_tanh'] = np.tanh(sl_pos / 10.0)\n    df_temp['attendance_tanh'] = np.tanh(ca_pos / 100.0)\n    df_temp['age_tanh'] = np.tanh(ag_pos / 25.0)\n    \n    df_temp['study_sigmoid'] = 1.0 / (1.0 + np.exp(-(sh_pos - 5.0)))\n    df_temp['sleep_sigmoid'] = 1.0 / (1.0 + np.exp(-(sl_pos - 7.0)))\n    df_temp['attendance_sigmoid'] = 1.0 / (1.0 + np.exp(-(ca_pos - 85.0) / 8.0))\n    df_temp['age_sigmoid'] = 1.0 / (1.0 + np.exp(-(ag_pos - 20.0) / 3.0))\n    \n    df_temp['study_elu'] = np.where(sh_pos > 0, sh_pos, 0.5 * (np.exp(sh_pos) - 1))\n    df_temp['sleep_elu'] = np.where(sl_pos > 0, sl_pos, 0.5 * (np.exp(sl_pos) - 1))\n    \n    df_temp['study_relu'] = np.maximum(0, sh_pos)\n    df_temp['sleep_relu'] = np.maximum(0, sl_pos)\n    df_temp['attendance_relu'] = np.maximum(0, ca_pos)\n    \n\n    df_temp['study_x_attendance'] = sh_pos * ca_pos\n    df_temp['study_x_sleep'] = sh_pos * sl_pos\n    df_temp['attendance_x_sleep'] = ca_pos * sl_pos\n    df_temp['study_x_age'] = sh_pos * ag_pos\n    df_temp['attendance_x_age'] = ca_pos * ag_pos\n    df_temp['sleep_x_age'] = sl_pos * ag_pos\n    \n\n    df_temp['study_sq_x_attendance'] = (sh_pos ** 2) * ca_pos\n    df_temp['study_x_attendance_sq'] = sh_pos * (ca_pos ** 2)\n    df_temp['study_sq_x_attendance_sq'] = (sh_pos ** 2) * (ca_pos ** 2)\n    \n    df_temp['sleep_sq_x_attendance'] = (sl_pos ** 2) * ca_pos\n    df_temp['study_sq_x_sleep'] = (sh_pos ** 2) * sl_pos\n    df_temp['study_x_sleep_sq'] = sh_pos * (sl_pos ** 2)\n    \n\n    df_temp['study_x_attendance_x_sleep'] = sh_pos * ca_pos * sl_pos\n    df_temp['study_sq_x_attendance_x_sleep'] = (sh_pos ** 2) * ca_pos * sl_pos\n    df_temp['study_x_attendance_sq_x_sleep'] = sh_pos * (ca_pos ** 2) * sl_pos\n    df_temp['study_x_attendance_x_sleep_sq'] = sh_pos * ca_pos * (sl_pos ** 2)\n    \n    df_temp['study_x_attendance_x_age'] = sh_pos * ca_pos * ag_pos\n    df_temp['study_x_sleep_x_age'] = sh_pos * sl_pos * ag_pos\n    df_temp['attendance_x_sleep_x_age'] = ca_pos * sl_pos * ag_pos\n    \n\n    study_mean = sh_pos.mean()\n    sleep_mean = sl_pos.mean()\n    att_mean = ca_pos.mean()\n    age_mean = ag_pos.mean()\n    \n    df_temp['study_center_5'] = sh_pos - 5.0\n    df_temp['sleep_center_7'] = sl_pos - 7.0\n    df_temp['att_center_85'] = ca_pos - 85.0\n    df_temp['age_center_mean'] = ag_pos - age_mean\n    \n    df_temp['study_center_sq'] = (sh_pos - 5.0) ** 2\n    df_temp['sleep_center_sq'] = (sl_pos - 7.0) ** 2\n    df_temp['att_center_sq'] = (ca_pos - 85.0) ** 2\n    \n    df_temp['study_center_cubed'] = (sh_pos - 5.0) ** 3\n    df_temp['sleep_center_cubed'] = (sl_pos - 7.0) ** 3\n    df_temp['att_center_cubed'] = (ca_pos - 85.0) ** 3\n    \n\n    df_temp['study_over_sleep'] = sh_pos / (sl_pos + eps)\n    df_temp['attendance_over_sleep'] = ca_pos / (sl_pos + eps)\n    df_temp['attendance_over_study'] = ca_pos / (sh_pos + eps)\n    df_temp['sleep_over_study'] = sl_pos / (sh_pos + eps)\n    df_temp['study_over_age'] = sh_pos / (ag_pos + eps)\n    df_temp['attendance_over_age'] = ca_pos / (ag_pos + eps)\n    \n\n    df_temp['log_study_sleep_ratio'] = np.log1p(sh_pos) / (np.log1p(sl_pos) + eps)\n    df_temp['log_att_study_ratio'] = np.log1p(ca_pos) / (np.log1p(sh_pos) + eps)\n    df_temp['sqrt_study_attendance_ratio'] = np.sqrt(sh_pos) / (np.sqrt(ca_pos) + eps)\n    \n    df_temp['study_efficiency'] = (sh_pos * ca_pos) / (sl_pos + 1)\n    df_temp['study_efficiency_log'] = np.log1p((sh_pos * ca_pos) / (sl_pos + 1))\n    \n    df_temp['balanced_effort'] = (sh_pos + ca_pos + sl_pos) / 3.0\n    df_temp['effort_variance'] = np.sqrt(((sh_pos - study_mean) ** 2 + \n                                          (ca_pos - att_mean) ** 2 + \n                                          (sl_pos - sleep_mean) ** 2) / 3.0)\n    \n\n    df_temp['study_hours_clip_12'] = sh_pos.clip(0, 12)\n    df_temp['sleep_hours_clip_12'] = sl_pos.clip(0, 12)\n    df_temp['attendance_clip_100'] = ca_pos.clip(0, 100)\n    df_temp['age_clip_30'] = ag_pos.clip(0, 30)\n    \n    df_temp['study_bounded'] = np.maximum(0, np.minimum(12, sh_pos))\n    df_temp['sleep_bounded'] = np.maximum(0, np.minimum(12, sl_pos))\n    \n\n    df_temp['sleep_gap_8'] = np.abs(sl_pos - 8.0)\n    df_temp['sleep_gap_7'] = np.abs(sl_pos - 7.0)\n    df_temp['sleep_gap_min'] = np.minimum(np.abs(sl_pos - 7.0), np.abs(sl_pos - 8.0))\n    \n    df_temp['attendance_gap_100'] = np.abs(ca_pos - 100.0)\n    df_temp['attendance_gap_90'] = np.abs(ca_pos - 90.0)\n    df_temp['attendance_gap_85'] = np.abs(ca_pos - 85.0)\n    df_temp['attendance_gap_min'] = np.minimum(np.minimum(np.abs(ca_pos - 100.0), np.abs(ca_pos - 90.0)), \n                                                np.abs(ca_pos - 85.0))\n    \n    df_temp['study_gap_6'] = np.abs(sh_pos - 6.0)\n    df_temp['study_gap_8'] = np.abs(sh_pos - 8.0)\n    df_temp['study_gap_min'] = np.minimum(np.abs(sh_pos - 6.0), np.abs(sh_pos - 8.0))\n    \n    df_temp['age_gap_20'] = np.abs(ag_pos - 20.0)\n    \n\n    df_temp['age_bin_num'] = pd.cut(df_temp['age'], bins=[0, 17, 19, 21, 23, 100], \n                                     labels=[0, 1, 2, 3, 4]).astype(float)\n    df_temp['study_bin_num'] = pd.cut(df_temp['study_hours'], bins=[-1, 2, 4, 6, 8, 100], \n                                       labels=[0, 1, 2, 3, 4]).astype(float)\n    df_temp['sleep_bin_num'] = pd.cut(df_temp['sleep_hours'], bins=[-1, 5, 6, 7, 8, 100], \n                                       labels=[0, 1, 2, 3, 4]).astype(float)\n    df_temp['attendance_bin_num'] = pd.cut(df_temp['class_attendance'], \n                                            bins=[-1, 60, 75, 85, 95, 101], \n                                            labels=[0, 1, 2, 3, 4]).astype(float)\n    \n\n    df_temp['study_bin_fine'] = pd.cut(df_temp['study_hours'], bins=[-1, 1, 2, 3, 4, 5, 6, 7, 8, 100], \n                                        labels=list(range(9))).astype(float)\n    df_temp['attendance_bin_fine'] = pd.cut(df_temp['class_attendance'], \n                                             bins=[-1, 50, 60, 70, 80, 85, 90, 95, 100, 101], \n                                             labels=list(range(9))).astype(float)\n    \n\n    sleep_quality_map = {'poor': 0, 'average': 1, 'good': 2}\n    facility_rating_map = {'low': 0, 'medium': 1, 'high': 2}\n    exam_difficulty_map = {'easy': 0, 'moderate': 1, 'hard': 2}\n    gender_map = {'male': 0, 'female': 1}\n    internet_access_map = {'no': 0, 'yes': 1}\n    \n    df_temp['sleep_quality_numeric'] = df_temp['sleep_quality'].map(sleep_quality_map).fillna(1).astype(int)\n    df_temp['facility_rating_numeric'] = df_temp['facility_rating'].map(facility_rating_map).fillna(1).astype(int)\n    df_temp['exam_difficulty_numeric'] = df_temp['exam_difficulty'].map(exam_difficulty_map).fillna(1).astype(int)\n    df_temp['gender_numeric'] = df_temp['gender'].map(gender_map).fillna(0).astype(int) if 'gender' in df_temp.columns else 0\n    df_temp['internet_access_numeric'] = df_temp['internet_access'].map(internet_access_map).fillna(0).astype(int) if 'internet_access' in df_temp.columns else 0\n    \n    if 'study_method' in df_temp.columns:\n        study_methods = df_temp['study_method'].unique()\n        study_method_map = {method: i for i, method in enumerate(sorted(study_methods))}\n        df_temp['study_method_numeric'] = df_temp['study_method'].map(study_method_map).fillna(0).astype(int)\n    else:\n        df_temp['study_method_numeric'] = 0\n    \n    if 'course' in df_temp.columns:\n        courses = df_temp['course'].unique()\n        course_map = {course: i for i, course in enumerate(sorted(courses))}\n        df_temp['course_numeric'] = df_temp['course'].map(course_map).fillna(0).astype(int)\n    else:\n        df_temp['course_numeric'] = 0\n    \n\n    df_temp['study_x_sleep_quality'] = sh_pos * df_temp['sleep_quality_numeric']\n    df_temp['attendance_x_facility'] = ca_pos * df_temp['facility_rating_numeric']\n    df_temp['sleep_x_difficulty'] = sl_pos * df_temp['exam_difficulty_numeric']\n    df_temp['study_x_internet'] = sh_pos * df_temp['internet_access_numeric']\n    df_temp['attendance_x_internet'] = ca_pos * df_temp['internet_access_numeric']\n    \n    df_temp['facility_x_sleepq'] = df_temp['facility_rating_numeric'] * df_temp['sleep_quality_numeric']\n    df_temp['difficulty_x_facility'] = df_temp['exam_difficulty_numeric'] * df_temp['facility_rating_numeric']\n    df_temp['difficulty_x_sleepq'] = df_temp['exam_difficulty_numeric'] * df_temp['sleep_quality_numeric']\n    \n    df_temp['study_method_x_internet'] = df_temp['study_method_numeric'] * df_temp['internet_access_numeric']\n    df_temp['course_x_difficulty'] = df_temp['course_numeric'] * df_temp['exam_difficulty_numeric']\n    \n\n    df_temp['high_att_low_sleep'] = ((ca_pos >= 90) & (sl_pos <= 6)).astype(int)\n    df_temp['high_att_high_study'] = ((ca_pos >= 90) & (sh_pos >= 6)).astype(int)\n    df_temp['low_att_high_study'] = ((ca_pos <= 60) & (sh_pos >= 7)).astype(int)\n    df_temp['ideal_sleep_flag'] = ((sl_pos >= 7) & (sl_pos <= 9)).astype(int)\n    df_temp['short_sleep_flag'] = (sl_pos <= 5.5).astype(int)\n    df_temp['long_study_flag'] = (sh_pos >= 7).astype(int)\n    df_temp['perfect_attendance'] = (ca_pos >= 95).astype(int)\n    df_temp['low_attendance'] = (ca_pos <= 60).astype(int)\n    \n    df_temp['good_facility'] = (df_temp['facility_rating_numeric'] >= 1).astype(int)\n    df_temp['hard_exam'] = (df_temp['exam_difficulty_numeric'] >= 2).astype(int)\n    df_temp['poor_sleep_quality'] = (df_temp['sleep_quality_numeric'] <= 1).astype(int)\n    df_temp['has_internet'] = (df_temp['internet_access_numeric'] == 1).astype(int)\n    \n\n    df_temp['efficiency'] = (sh_pos * ca_pos) / (sl_pos + 1)\n    df_temp['efficiency2'] = (df_temp['study_hours_clip_12'] * df_temp['attendance_clip_100']) / (df_temp['sleep_hours_clip_12'] + 1)\n    df_temp['efficiency_normalized'] = (sh_pos / 8.0) * (ca_pos / 100.0) * (sl_pos / 8.0)\n    \n    df_temp['weighted_effort'] = (0.06 * ca_pos + 2.0 * sh_pos + 1.2 * sl_pos)\n    df_temp['weighted_effort_x_difficulty'] = df_temp['weighted_effort'] * (1.0 + 0.2 * df_temp['exam_difficulty_numeric'])\n    \n\n    df_temp['harmonic_effort'] = 3 / ((1 / (sh_pos + eps)) + (1 / (ca_pos + eps)) + (1 / (sl_pos + eps)))\n    \n\n    df_temp['geo_effort'] = ((sh_pos + 1) * (ca_pos + 1) * (sl_pos + 1)) ** (1 / 3)\n    \n\n    df_temp['power_mean_effort'] = ((sh_pos ** 2 + ca_pos ** 2 + sl_pos ** 2) / 3.0) ** 0.5\n    \n\n    df_temp['study_rank'] = sh_pos.rank(pct=True)\n    df_temp['attendance_rank'] = ca_pos.rank(pct=True)\n    df_temp['sleep_rank'] = sl_pos.rank(pct=True)\n    df_temp['age_rank'] = ag_pos.rank(pct=True)\n    \n    df_temp['study_z'] = (sh_pos - study_mean) / (sh_pos.std() + eps)\n    df_temp['attendance_z'] = (ca_pos - att_mean) / (ca_pos.std() + eps)\n    df_temp['sleep_z'] = (sl_pos - sleep_mean) / (sl_pos.std() + eps)\n    df_temp['age_z'] = (ag_pos - age_mean) / (ag_pos.std() + eps)\n    \n \n    study_q75 = sh_pos.quantile(0.75)\n    att_q75 = ca_pos.quantile(0.75)\n    \n    df_temp['study_above_q75'] = (sh_pos >= study_q75).astype(int)\n    df_temp['attendance_above_q75'] = (ca_pos >= att_q75).astype(int)\n    \n \n    df_temp['study_above_6'] = np.maximum(0, sh_pos - 6)\n    df_temp['study_above_8'] = np.maximum(0, sh_pos - 8)\n    df_temp['sleep_below_6'] = np.maximum(0, 6 - sl_pos)\n    df_temp['sleep_below_5'] = np.maximum(0, 5 - sl_pos)\n    df_temp['attendance_below_75'] = np.maximum(0, 75 - ca_pos)\n    df_temp['attendance_below_85'] = np.maximum(0, 85 - ca_pos)\n    \n    df_temp['study_excess'] = np.maximum(0, sh_pos - 8)\n    df_temp['sleep_excess'] = np.maximum(0, sl_pos - 9)\n    \n\n    df_temp['high_study_good_facility'] = (sh_pos >= 6) & (df_temp['facility_rating_numeric'] >= 1)\n    df_temp['high_study_good_sleep'] = (sh_pos >= 6) & (df_temp['sleep_quality_numeric'] >= 1)\n    df_temp['high_att_good_sleep'] = (ca_pos >= 85) & (df_temp['sleep_quality_numeric'] >= 1)\n    \n    df_temp['low_study_easy_exam'] = (sh_pos <= 3) & (df_temp['exam_difficulty_numeric'] == 0)\n    df_temp['high_study_hard_exam'] = (sh_pos >= 7) & (df_temp['exam_difficulty_numeric'] == 2)\n    \n\n    df_temp['study_hours_log_log'] = np.log1p(np.log1p(sh_pos))\n    df_temp['attendance_log_log'] = np.log1p(np.log1p(ca_pos))\n    \n\n    ideal_study = 6.0\n    ideal_sleep = 8.0\n    ideal_attendance = 90.0\n    \n    df_temp['study_distance_from_ideal'] = np.abs(sh_pos - ideal_study)\n    df_temp['sleep_distance_from_ideal'] = np.abs(sl_pos - ideal_sleep)\n    df_temp['attendance_distance_from_ideal'] = np.abs(ca_pos - ideal_attendance)\n    \n    df_temp['total_distance_from_ideal'] = (\n        df_temp['study_distance_from_ideal'] + \n        df_temp['sleep_distance_from_ideal'] + \n        df_temp['attendance_distance_from_ideal'] / 10\n    )\n    \n    df_temp['euclidean_distance'] = np.sqrt(\n        (sh_pos - ideal_study) ** 2 + \n        (sl_pos - ideal_sleep) ** 2 + \n        ((ca_pos - ideal_attendance) / 10) ** 2\n    )\n    \n\n    numeric_features = [\n        \n        'study_hours_squared', 'study_hours_cubed', 'study_hours_quartic', 'study_hours_5th',\n        'class_attendance_squared', 'class_attendance_cubed', 'class_attendance_quartic',\n        'sleep_hours_squared', 'sleep_hours_cubed', 'sleep_hours_quartic',\n        'age_squared', 'age_cubed', 'age_quartic',\n        \n  \n        'log_study_hours', 'log_class_attendance', 'log_sleep_hours', 'log_age',\n        'log1p_study_hours', 'log2_study_hours', 'log_study_hours_squared',\n        \n       \n        'sqrt_study_hours', 'sqrt_class_attendance', 'sqrt_sleep_hours', 'sqrt_age',\n        'cbrt_study_hours', 'cbrt_class_attendance', 'cbrt_sleep_hours',\n        'power_study_15', 'power_attendance_15', 'power_sleep_15',\n        'power_study_025', 'power_attendance_025',\n        \n\n        'inv_sleep', 'inv_study', 'inv_attendance', 'inv_age',\n        'inv_sqrt_study', 'inv_sqrt_attendance',\n        \n  \n        'study_tanh', 'sleep_tanh', 'attendance_tanh', 'age_tanh',\n        'study_sigmoid', 'sleep_sigmoid', 'attendance_sigmoid', 'age_sigmoid',\n        'study_elu', 'sleep_elu',\n        'study_relu', 'sleep_relu', 'attendance_relu',\n        \n\n        'study_x_attendance', 'study_x_sleep', 'attendance_x_sleep',\n        'study_x_age', 'attendance_x_age', 'sleep_x_age',\n        'study_sq_x_attendance', 'study_x_attendance_sq', 'study_sq_x_attendance_sq',\n        'sleep_sq_x_attendance', 'study_sq_x_sleep', 'study_x_sleep_sq',\n        \n\n        'study_x_attendance_x_sleep', 'study_sq_x_attendance_x_sleep',\n        'study_x_attendance_sq_x_sleep', 'study_x_attendance_x_sleep_sq',\n        'study_x_attendance_x_age', 'study_x_sleep_x_age', 'attendance_x_sleep_x_age',\n        \n  \n        'study_center_5', 'sleep_center_7', 'att_center_85', 'age_center_mean',\n        'study_center_sq', 'sleep_center_sq', 'att_center_sq',\n        'study_center_cubed', 'sleep_center_cubed', 'att_center_cubed',\n        \n\n        'study_over_sleep', 'attendance_over_sleep', 'attendance_over_study',\n        'sleep_over_study', 'study_over_age', 'attendance_over_age',\n        'log_study_sleep_ratio', 'log_att_study_ratio', 'sqrt_study_attendance_ratio',\n        'study_efficiency', 'study_efficiency_log',\n        'balanced_effort', 'effort_variance',\n        \n    \n        'study_hours_clip_12', 'sleep_hours_clip_12', 'attendance_clip_100', 'age_clip_30',\n        'study_bounded', 'sleep_bounded',\n        \n       \n        'sleep_gap_8', 'sleep_gap_7', 'sleep_gap_min',\n        'attendance_gap_100', 'attendance_gap_90', 'attendance_gap_85', 'attendance_gap_min',\n        'study_gap_6', 'study_gap_8', 'study_gap_min', 'age_gap_20',\n        \n     \n        'age_bin_num', 'study_bin_num', 'sleep_bin_num', 'attendance_bin_num',\n        'study_bin_fine', 'attendance_bin_fine',\n        \n     \n        'sleep_quality_numeric', 'facility_rating_numeric', 'exam_difficulty_numeric',\n        'gender_numeric', 'internet_access_numeric', 'study_method_numeric', 'course_numeric',\n        \n   \n        'study_x_sleep_quality', 'attendance_x_facility', 'sleep_x_difficulty',\n        'study_x_internet', 'attendance_x_internet',\n        'facility_x_sleepq', 'difficulty_x_facility', 'difficulty_x_sleepq',\n        'study_method_x_internet', 'course_x_difficulty',\n        \n  \n        'high_att_low_sleep', 'high_att_high_study', 'low_att_high_study',\n        'ideal_sleep_flag', 'short_sleep_flag', 'long_study_flag',\n        'perfect_attendance', 'low_attendance',\n        'good_facility', 'hard_exam', 'poor_sleep_quality', 'has_internet',\n        \n\n        'efficiency', 'efficiency2', 'efficiency_normalized',\n        'weighted_effort', 'weighted_effort_x_difficulty',\n        'harmonic_effort', 'geo_effort', 'power_mean_effort',\n        \n \n        'study_rank', 'attendance_rank', 'sleep_rank', 'age_rank',\n        'study_z', 'attendance_z', 'sleep_z', 'age_z',\n        'study_above_q75', 'attendance_above_q75',\n        \n  \n        'study_above_6', 'study_above_8', 'sleep_below_6', 'sleep_below_5',\n        'attendance_below_75', 'attendance_below_85',\n        'study_excess', 'sleep_excess',\n        \n   \n        'high_study_good_facility', 'high_study_good_sleep', 'high_att_good_sleep',\n        'low_study_easy_exam', 'high_study_hard_exam',\n        \n      \n        'study_hours_log_log', 'attendance_log_log',\n        \n   \n        'study_distance_from_ideal', 'sleep_distance_from_ideal', 'attendance_distance_from_ideal',\n        'total_distance_from_ideal', 'euclidean_distance',\n    ]\n    \n    return df_temp[base_features + numeric_features], numeric_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:28:31.033995Z","iopub.execute_input":"2026-01-08T15:28:31.034492Z","iopub.status.idle":"2026-01-08T15:28:31.209933Z","shell.execute_reply.started":"2026-01-08T15:28:31.034471Z","shell.execute_reply":"2026-01-08T15:28:31.209241Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Preprocessing and Preparing the Data","metadata":{}},{"cell_type":"code","source":"X_raw, numeric_cols = preprocess(train_df)\ny = train_df[TARGET].reset_index(drop=True)\n\nX_test_raw, _ = preprocess(test_df)\nX_orig_raw, _ = preprocess(original_df)\ny_orig = original_df[TARGET].reset_index(drop=True)\n\n\ny = y.clip(0, 100)\ny_orig = y_orig.clip(0, 100)\n\nfull_data = pd.concat([X_raw, X_test_raw, X_orig_raw], axis=0, ignore_index=True)\n\n\nfor col in numeric_cols:\n    full_data[col] = full_data[col].astype(float)\n\n\nX = full_data.iloc[:len(train_df)].copy()\nX_test = full_data.iloc[len(train_df):len(train_df) + len(test_df)].copy()\nX_original = full_data.iloc[len(train_df) + len(test_df):].copy()\n\nprint(f\"Feature shapes - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:28:31.21164Z","iopub.execute_input":"2026-01-08T15:28:31.212092Z","iopub.status.idle":"2026-01-08T15:28:37.062072Z","shell.execute_reply.started":"2026-01-08T15:28:31.212066Z","shell.execute_reply":"2026-01-08T15:28:37.061305Z"}},"outputs":[{"name":"stdout","text":"Feature shapes - X: (630000, 194), X_test: (270000, 194), X_original: (20000, 194)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Ridge Regression ","metadata":{}},{"cell_type":"code","source":"FOLDS = 10\n\n\ny_bins = pd.qcut(y, q=10, labels=False, duplicates='drop').astype(int)\nkf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=1003)\n\nscalers_ridge = []\n\nN_SAMPLES_TRAIN = X.shape[0]\nN_SAMPLES_TEST = X_test.shape[0]\n\noof_pred_lr = np.zeros(N_SAMPLES_TRAIN)\ntest_preds_lr = np.zeros((N_SAMPLES_TEST, FOLDS))\norig_preds_lr = np.zeros(X_original.shape[0])\n\nfold_rmse_lr = []\nlr_models = []\ntarget_encoders = []\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"TRAINING RIDGE REGRESSION WITH SCALING\")\nprint(\"=\"*50)\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y_bins), start=1):\n    print(f\"Training fold {fold} (Ridge) ...\")\n    \n    X_train_fold, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n    \n    X_train_combined = pd.concat([X_train_fold, X_original], axis=0)\n    y_train_combined = pd.concat([y_train_fold, y_orig], axis=0)\n    \n\n    target_encoder = TargetEncoder(smooth='auto', target_type='continuous')\n    \n    X_train_encoded = X_train_combined.copy()\n    X_val_encoded = X_val.copy()\n    X_test_encoded = X_test.copy()\n    \n    X_train_encoded[CATS] = target_encoder.fit_transform(X_train_combined[CATS], y_train_combined)\n    X_val_encoded[CATS] = target_encoder.transform(X_val[CATS])\n    X_test_encoded[CATS] = target_encoder.transform(X_test[CATS])\n    \n    \n    scaler = RobustScaler()\n    \n    X_train_scaled = X_train_encoded.copy()\n    X_val_scaled = X_val_encoded.copy()\n    X_test_scaled = X_test_encoded.copy()\n    \n    X_train_scaled[:] = scaler.fit_transform(X_train_encoded)\n    X_val_scaled[:] = scaler.transform(X_val_encoded)\n    X_test_scaled[:] = scaler.transform(X_test_encoded)\n    \n    scalers_ridge.append(scaler)\n    \n    alphas = np.logspace(-1, 3, 60)\n    lr_model = RidgeCV(alphas=alphas, cv=10, scoring='neg_root_mean_squared_error')\n    lr_model.fit(X_train_scaled, y_train_combined.to_numpy().ravel())\n    lr_models.append(lr_model)\n    target_encoders.append(target_encoder)\n    \n    lr_val_pred = lr_model.predict(X_val_scaled)\n    lr_test_pred = lr_model.predict(X_test_scaled)\n    lr_orig_pred = lr_model.predict(X_train_scaled.iloc[-X_original.shape[0]:])\n    \n    lr_val_pred = np.clip(lr_val_pred, 0, 100)\n    lr_test_pred = np.clip(lr_test_pred, 0, 100)\n    lr_orig_pred = np.clip(lr_orig_pred, 0, 100)\n    \n    oof_pred_lr[val_index] = lr_val_pred\n    test_preds_lr[:, fold - 1] = lr_test_pred\n    orig_preds_lr += lr_orig_pred / FOLDS\n    \n    rmse_lr = root_mean_squared_error(y_val, lr_val_pred)\n    fold_rmse_lr.append(rmse_lr)\n    print(f\"Fold {fold} RMSE (Ridge): {rmse_lr:.6f}, Alpha: {lr_model.alpha_:.6f}\")\n\nridge_oof_rmse = root_mean_squared_error(y, oof_pred_lr)\nprint(f\"\\nRidge OOF RMSE: {ridge_oof_rmse:.6f}\")\nprint(f\"Ridge Fold RMSE Mean: {np.mean(fold_rmse_lr):.6f} ± {np.std(fold_rmse_lr):.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T15:54:28.566581Z","iopub.execute_input":"2026-01-08T15:54:28.567271Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTRAINING RIDGE REGRESSION WITH SCALING\n==================================================\nTraining fold 1 (Ridge) ...\nFold 1 RMSE (Ridge): 8.832211, Alpha: 32.245905\nTraining fold 2 (Ridge) ...\nFold 2 RMSE (Ridge): 8.897898, Alpha: 37.693910\nTraining fold 3 (Ridge) ...\nFold 3 RMSE (Ridge): 8.919101, Alpha: 32.245905\nTraining fold 4 (Ridge) ...\nFold 4 RMSE (Ridge): 8.846978, Alpha: 32.245905\nTraining fold 5 (Ridge) ...\nFold 5 RMSE (Ridge): 8.928423, Alpha: 32.245905\nTraining fold 6 (Ridge) ...\nFold 6 RMSE (Ridge): 8.844335, Alpha: 32.245905\nTraining fold 7 (Ridge) ...\nFold 7 RMSE (Ridge): 8.869446, Alpha: 27.585316\nTraining fold 8 (Ridge) ...\nFold 8 RMSE (Ridge): 8.923895, Alpha: 32.245905\nTraining fold 9 (Ridge) ...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"FEATURE SELECTION\")\nprint(\"=\"*50)\n\n\nfeature_importance = pd.DataFrame({\n    'feature': X_train_encoded.columns,\n    'importance': np.abs(lr_models[0].coef_)\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 30 Features by Ridge importance:\")\nprint(feature_importance.head(30))\n\n\nn_features_to_keep = 95\ntop_features = feature_importance.head(n_features_to_keep)['feature'].tolist()\nprint(f\"\\nKeeping top {n_features_to_keep} features out of {len(feature_importance)}\")\n\n\nX = X[top_features]\nX_test = X_test[top_features]\nX_original = X_original[top_features]\n\nprint(f\"XGB feature shapes - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing the Data with Categorical ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"PREPARING XGB DATA WITH CATEGORY MEAN ENCODING\")\nprint(\"=\"*50)\n\n\nX[\"ridge_pred\"] = oof_pred_lr\nX_test[\"ridge_pred\"] = test_preds_lr.mean(axis=1)\nX_original[\"ridge_pred\"] = orig_preds_lr\n\nprint(f\"Shapes before encoding - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")\n\n\ncat_cols = X.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\nprint(f\"Categorical columns to encode: {cat_cols}\")\n\ncat_transformer = CategoryMeanTransformer(cat_cols=cat_cols)\n\n\ncat_transformer.fit(X, y)\n\n\nX = cat_transformer.transform(X)\nX_test = cat_transformer.transform(X_test)\nX_original = cat_transformer.transform(X_original)\n\n\nX = X.astype(np.float32)\nX_test = X_test.astype(np.float32)\nX_original = X_original.astype(np.float32)\ny_float = y.values.astype(np.float32)\n\nprint(f\"Final XGB shapes with Ridge feature - X: {X.shape}, X_test: {X_test.shape}, X_original: {X_original.shape}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### XGBoost Training ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 50)\nprint(\"TRAINING XGBOOST AND LIGHTGBM\")\nprint(\"=\" * 50)\n\n\ndtest = xgb.DMatrix(X_test)\n\nxgb_params = {\n    \"objective\": \"reg:squarederror\",\n    \"learning_rate\": 0.05,  \n    \"max_depth\": 8,        \n    \"subsample\": 0.85,     \n    \"colsample_bytree\": 0.75,  \n    \"colsample_bynode\": 0.8,  \n    \"min_child_weight\": 2,  \n    \"gamma\": 0.05,         \n    \"lambda\": 0.8,          \n    \"alpha\": 0.02,         \n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"hist\",\n    \"verbosity\": 0,\n    \"seed\": 42,\n}\n\n\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'max_depth': 8,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'min_child_samples': 20,\n    'lambda_l1': 0.1,\n    'lambda_l2': 0.1,\n    'verbose': -1,\n}\n\n\noof_predictions_xgb = np.zeros(len(X), dtype=np.float32)\noof_predictions_lgb = np.zeros(len(X), dtype=np.float32)\n\ntest_predictions_xgb = []\ntest_predictions_lgb = []\n\nfold_metrics = []\n\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y_bins), start=1):\n    print(f\"\\n{'='*50}\")\n    print(f\"Fold {fold}/5\")\n    print(f\"{'='*50}\")\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y_float[train_idx], y_float[val_idx]\n    \n\n    print(\"Training XGBoost...\")\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dval = xgb.DMatrix(X_val, label=y_val)\n    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n    \n    xgb_model = xgb.train(\n        params=xgb_params,\n        dtrain=dtrain,\n        num_boost_round=3000,\n        evals=evals,\n        early_stopping_rounds=50,\n        verbose_eval=False,\n    )\n    \n    xgb_val_preds = np.clip(xgb_model.predict(dval), 0, 100)\n    oof_predictions_xgb[val_idx] = xgb_val_preds\n    \n    xgb_test_pred = np.clip(xgb_model.predict(dtest), 0, 100)\n    test_predictions_xgb.append(xgb_test_pred)\n    \n    xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_preds))\n    xgb_mae = mean_absolute_error(y_val, xgb_val_preds)\n    \n    print(f\"  XGBoost RMSE: {xgb_rmse:.5f} | MAE: {xgb_mae:.5f}\")\n    print(f\"  Best iteration: {xgb_model.best_iteration}\")\n    \n  \n    print(\"Training LightGBM...\")\n    \n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n    \n \n    lgb_model = lgb.train(\n        params=lgb_params,\n        train_set=lgb_train,\n        num_boost_round=3000,\n        valid_sets=[lgb_train, lgb_val],\n        valid_names=['train', 'valid'],\n        callbacks=[\n            lgb.early_stopping(50),\n            lgb.log_evaluation(period=0)  \n        ]\n    )\n    \n    lgb_val_preds = np.clip(lgb_model.predict(X_val), 0, 100)\n    oof_predictions_lgb[val_idx] = lgb_val_preds\n    \n    lgb_test_pred = np.clip(lgb_model.predict(X_test), 0, 100)\n    test_predictions_lgb.append(lgb_test_pred)\n    \n    lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_val_preds))\n    lgb_mae = mean_absolute_error(y_val, lgb_val_preds)\n    \n    print(f\"  LightGBM RMSE: {lgb_rmse:.5f} | MAE: {lgb_mae:.5f}\")\n    print(f\"  Best iteration: {lgb_model.best_iteration}\")\n    \n \n    fold_metrics.append({\n        \"fold\": fold,\n        \"xgb_rmse\": xgb_rmse,\n        \"xgb_mae\": xgb_mae,\n        \"lgb_rmse\": lgb_rmse,\n        \"lgb_mae\": lgb_mae,\n    })\n\n\n\nprint(f\"\\n{'='*50}\")\nprint(\"AGGREGATING PREDICTIONS\")\nprint(f\"{'='*50}\\n\")\n\n\ntest_predictions_xgb = np.mean(test_predictions_xgb, axis=0)\ntest_predictions_lgb = np.mean(test_predictions_lgb, axis=0)\n\n\nxgb_oof_rmse = np.sqrt(mean_squared_error(y_float, oof_predictions_xgb))\nxgb_oof_mae = mean_absolute_error(y_float, oof_predictions_xgb)\n\nlgb_oof_rmse = np.sqrt(mean_squared_error(y_float, oof_predictions_lgb))\nlgb_oof_mae = mean_absolute_error(y_float, oof_predictions_lgb)\n\nprint(f\"XGBoost OOF RMSE: {xgb_oof_rmse:.5f} | MAE: {xgb_oof_mae:.5f}\")\nprint(f\"LightGBM OOF RMSE: {lgb_oof_rmse:.5f} | MAE: {lgb_oof_mae:.5f}\")\n\n\n\nprint(f\"\\n{'='*50}\")\nprint(\"ENSEMBLE BLENDING (XGB + LGB + RIDGE)\")\nprint(f\"{'='*50}\\n\")\n\n\nprint(\"Testing different ensemble weights...\\n\")\n\nbest_rmse = float('inf')\nbest_weights = None\n\n\nweight_combinations = [\n    (0.0, 0.5, 0.5),    \n    (0.0, 0.6, 0.4),    \n    (0.0, 0.4, 0.6),   \n    (0.05, 0.5, 0.45),  \n    (0.1, 0.45, 0.45),  \n    (0.0, 0.7, 0.3),   \n    (0.0, 0.3, 0.7),    \n]\n\nfor ridge_w, xgb_w, lgb_w in weight_combinations:\n    ensemble_oof = (ridge_w * oof_pred_lr + \n                    xgb_w * oof_predictions_xgb + \n                    lgb_w * oof_predictions_lgb)\n    \n    ensemble_oof = np.clip(ensemble_oof, 0, 100)\n    ensemble_rmse = np.sqrt(mean_squared_error(y, ensemble_oof))\n    \n    print(f\"Ridge: {ridge_w:.2f} | XGB: {xgb_w:.2f} | LGB: {lgb_w:.2f} → RMSE: {ensemble_rmse:.5f}\")\n    \n    if ensemble_rmse < best_rmse:\n        best_rmse = ensemble_rmse\n        best_weights = (ridge_w, xgb_w, lgb_w)\n\nridge_w, xgb_w, lgb_w = best_weights\nprint(f\"\\n Best ensemble weights:\")\nprint(f\"   Ridge: {ridge_w:.2f} | XGB: {xgb_w:.2f} | LGB: {lgb_w:.2f}\")\nprint(f\"   Ensemble RMSE: {best_rmse:.5f}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ensemble Blending ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"ENSEMBLE BLENDING WITH SOFT CLIPPING\")\nprint(\"=\"*50)\n\n\nprint(\"\\nTesting different ensemble weights...\\n\")\n\nbest_rmse = float('inf')\nbest_weights = None\n\n\nweight_combinations = [\n    (0.0, 0.5, 0.5),    \n    (0.0, 0.6, 0.4),    \n    (0.0, 0.4, 0.6),  \n    (0.05, 0.5, 0.45),  \n    (0.1, 0.45, 0.45),  \n    (0.0, 0.7, 0.3),   \n    (0.0, 0.3, 0.7),   \n]\n\nfor ridge_w, xgb_w, lgb_w in weight_combinations:\n    ensemble_oof = (ridge_w * oof_pred_lr + \n                    xgb_w * oof_predictions_xgb + \n                    lgb_w * oof_predictions_lgb)\n    \n    ensemble_oof = np.clip(ensemble_oof, 0, 100)\n    ensemble_rmse = np.sqrt(mean_squared_error(y, ensemble_oof))\n    \n    print(f\"Ridge: {ridge_w:.2f} | XGB: {xgb_w:.2f} | LGB: {lgb_w:.2f} → RMSE: {ensemble_rmse:.5f}\")\n    \n    if ensemble_rmse < best_rmse:\n        best_rmse = ensemble_rmse\n        best_weights = (ridge_w, xgb_w, lgb_w)\n\nridge_w, xgb_w, lgb_w = best_weights\n\nprint(f\"\\n  Best ensemble weights:\")\nprint(f\"   Ridge: {ridge_w:.2f} | XGB: {xgb_w:.2f} | LGB: {lgb_w:.2f}\")\nprint(f\"   Ensemble RMSE (before clipping): {best_rmse:.5f}\\n\")\n\n\nfinal_oof = (ridge_w * oof_pred_lr + \n             xgb_w * oof_predictions_xgb + \n             lgb_w * oof_predictions_lgb)\n\nfinal_test = (ridge_w * test_preds_lr.mean(axis=1) + \n              xgb_w * test_predictions_xgb + \n              lgb_w * test_predictions_lgb)\n\n\n\ndef soft_clip(pred, lower=0, upper=100):\n\n    scaled = lower + (upper - lower) / (1 + np.exp(-10 * (pred - 50) / 50))\n    return scaled\n\n\nlower_q = y.quantile(0.01)\nupper_q = y.quantile(0.99)\n\nprint(f\"Quantile bounds: [{lower_q:.2f}, {upper_q:.2f}]\")\n\n\nfinal_oof = np.clip(final_oof, 0, 100)\nfinal_test = np.clip(final_test, 0, 100)\n\n\nfinal_oof_rmse = np.sqrt(mean_squared_error(y, final_oof))\n\nprint(f\"\\n{'='*50}\")\nprint(\"FINAL MODEL PERFORMANCE\")\nprint(f\"{'='*50}\\n\")\n\nprint(f\"Individual Models:\")\nprint(f\"  Ridge OOF RMSE:    {ridge_oof_rmse:.5f}\")\nprint(f\"  XGBoost OOF RMSE:  {xgb_oof_rmse:.5f}\")\nprint(f\"  LightGBM OOF RMSE: {lgb_oof_rmse:.5f}\")\n\nprint(f\"\\nEnsemble (before clipping):\")\nprint(f\"  Ensemble RMSE: {best_rmse:.5f}\")\n\nprint(f\"\\nFinal (after clipping):\")\nprint(f\"  Final OOF RMSE: {final_oof_rmse:.5f}\")\nprint(f\"  Prediction range: [{final_test.min():.2f}, {final_test.max():.2f}]\")\n\nprint(f\"\\n{'='*50}\")\nprint(\"IMPROVEMENTS\")\nprint(f\"{'='*50}\")\nprint(f\"  vs Ridge:    {ridge_oof_rmse - final_oof_rmse:.5f}\")\nprint(f\"  vs XGBoost:  {xgb_oof_rmse - final_oof_rmse:.5f}\")\nprint(f\"  vs LightGBM: {lgb_oof_rmse - final_oof_rmse:.5f}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"SAVING RESULTS\")\nprint(\"=\"*50)\n\n\noof_df = pd.DataFrame({\n    \"id\": train_df[ID_COL], \n    TARGET: final_oof  \n})\n\noof_df.to_csv(\"oof_df.csv\", index=False)\n\n\nsubmission_df[TARGET] = final_test\nsubmission_df.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Acknowledgement: [https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe](https://www.kaggle.com/code/mdevian/ps-s6e1-clean-strong-baseline-ridge-xgb-fe)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}