{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/attention-is-all-you-need-implementation?scriptVersionId=247835585\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"005d6bf5","metadata":{"papermill":{"duration":0.003691,"end_time":"2025-06-28T12:39:49.438338","exception":false,"start_time":"2025-06-28T12:39:49.434647","status":"completed"},"tags":[]},"source":["## Understanding “Attention Is All You Need”"]},{"cell_type":"markdown","id":"61edb985","metadata":{"papermill":{"duration":0.002651,"end_time":"2025-06-28T12:39:49.444326","exception":false,"start_time":"2025-06-28T12:39:49.441675","status":"completed"},"tags":[]},"source":["\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal generative AI.\n"]},{"cell_type":"code","execution_count":1,"id":"d42bbf85","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:49.451612Z","iopub.status.busy":"2025-06-28T12:39:49.451209Z","iopub.status.idle":"2025-06-28T12:39:54.558433Z","shell.execute_reply":"2025-06-28T12:39:54.557691Z"},"papermill":{"duration":5.112921,"end_time":"2025-06-28T12:39:54.560138","exception":false,"start_time":"2025-06-28T12:39:49.447217","status":"completed"},"tags":[]},"outputs":[],"source":["import torch \n","import torch.nn as nn "]},{"cell_type":"markdown","id":"b7c32203","metadata":{"papermill":{"duration":0.002923,"end_time":"2025-06-28T12:39:54.566503","exception":false,"start_time":"2025-06-28T12:39:54.56358","status":"completed"},"tags":[]},"source":["#### Multi-Head Self-Attention mechanism implementation.\n","\n","This is the core component of the Transformer architecture that allows the model to attend to different positions in the input sequence."]},{"cell_type":"code","execution_count":2,"id":"b96c9f36","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.573797Z","iopub.status.busy":"2025-06-28T12:39:54.573388Z","iopub.status.idle":"2025-06-28T12:39:54.582469Z","shell.execute_reply":"2025-06-28T12:39:54.581661Z"},"papermill":{"duration":0.014493,"end_time":"2025-06-28T12:39:54.58401","exception":false,"start_time":"2025-06-28T12:39:54.569517","status":"completed"},"tags":[]},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    \n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size \n","        self.heads = heads \n","        self.head_dim = embed_size // heads \n","        \n","        \n","        assert (self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n","\n","        \n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","\n","        \n","        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        \n","        N =query.shape[0]\n","\n","        \n","        value_len, key_len, query_len =  values.shape[1], keys.shape[1], query.shape[1]\n","\n","        \n","        \n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len,self.heads, self.head_dim )\n","        queries = query.reshape(N, query_len,self.heads, self.head_dim) \n","\n","           \n","        values = self.values(values)    \n","        keys = self.keys(keys)          \n","        queries = self.queries(queries) \n","        \n","\n","       \n","        \n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        \n","        \n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask ==0, float(\"-1e20\"))\n","\n","        \n","       \n","        attention = torch.softmax(energy / (self.embed_size **(1/2)), dim=3)\n","\n","        \n","        out = torch.einsum(\"nhql,nlhd->nqhd\",[attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n","       \n","\n","        \n","        out = self.fc_out(out)\n","        return out \n"]},{"cell_type":"markdown","id":"3f757694","metadata":{"papermill":{"duration":0.002837,"end_time":"2025-06-28T12:39:54.590057","exception":false,"start_time":"2025-06-28T12:39:54.58722","status":"completed"},"tags":[]},"source":["A single Transformer encoder block consisting of:\n","\n","    1. Multi-head self-attention\n","    2. Feed-forward network\n","    \n","Both with residual connections and layer normalization"]},{"cell_type":"code","execution_count":3,"id":"1adcc819","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.5972Z","iopub.status.busy":"2025-06-28T12:39:54.596843Z","iopub.status.idle":"2025-06-28T12:39:54.60363Z","shell.execute_reply":"2025-06-28T12:39:54.602687Z"},"papermill":{"duration":0.011978,"end_time":"2025-06-28T12:39:54.60499","exception":false,"start_time":"2025-06-28T12:39:54.593012","status":"completed"},"tags":[]},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","\n","        \n","        self.attention = SelfAttention(embed_size,heads)\n","\n","       \n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        \n","        \n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion*embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion*embed_size, embed_size)\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    \n","    def forward(self, value, key, query, mask):\n","\n","        \n","        attention = self.attention(value, key, query, mask)\n","\n","       \n","        x = self.dropout(self.norm1(attention + query))\n","\n","       \n","        forward = self.feed_forward(x)\n","\n","        \n","        out = self.dropout(self.norm2(forward + x ))\n","        return out "]},{"cell_type":"markdown","id":"e936c71a","metadata":{"papermill":{"duration":0.002813,"end_time":"2025-06-28T12:39:54.611013","exception":false,"start_time":"2025-06-28T12:39:54.6082","status":"completed"},"tags":[]},"source":["Transformer Encoder consisting of:\n","\n","    1. Input embeddings (word + positional)\n","    2. Stack of Transformer blocks"]},{"cell_type":"code","execution_count":4,"id":"79f7d7a8","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.618427Z","iopub.status.busy":"2025-06-28T12:39:54.618097Z","iopub.status.idle":"2025-06-28T12:39:54.625253Z","shell.execute_reply":"2025-06-28T12:39:54.624437Z"},"papermill":{"duration":0.012624,"end_time":"2025-06-28T12:39:54.626781","exception":false,"start_time":"2025-06-28T12:39:54.614157","status":"completed"},"tags":[]},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(\n","        self, \n","        src_vocab_size,    \n","        embed_size,       \n","        num_layers,       \n","        heads,             \n","        device,           \n","        forward_expansion, \n","        dropout,           \n","        max_length,        \n","    ):\n","        super(Encoder,self).__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","\n","        \n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","\n","        \n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        \n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerBlock(\n","                    embed_size,\n","                    heads,\n","                    dropout=dropout,\n","                    forward_expansion=forward_expansion,\n","                    \n","                )\n","                               \n","            for _ in range(num_layers)        ]\n","            \n","        )\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        \n","        \n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","\n","        \n","        out = self.dropout(\n","            self.word_embedding(x) + self.position_embedding(positions)\n","        )\n","\n","        \n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","\n","        return out \n","        "]},{"cell_type":"markdown","id":"39784e76","metadata":{"papermill":{"duration":0.002777,"end_time":"2025-06-28T12:39:54.632698","exception":false,"start_time":"2025-06-28T12:39:54.629921","status":"completed"},"tags":[]},"source":["A single Transformer decoder block consisting of:\n","\n","    1. Masked self-attention (on target sequence)\n","    2. Cross-attention (attending to encoder output)\n","    3. Feed-forward network"]},{"cell_type":"code","execution_count":5,"id":"32671ab0","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.639818Z","iopub.status.busy":"2025-06-28T12:39:54.639517Z","iopub.status.idle":"2025-06-28T12:39:54.645755Z","shell.execute_reply":"2025-06-28T12:39:54.645029Z"},"papermill":{"duration":0.011584,"end_time":"2025-06-28T12:39:54.647219","exception":false,"start_time":"2025-06-28T12:39:54.635635","status":"completed"},"tags":[]},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","        \n","        \n","        self.attention = SelfAttention(embed_size, heads)\n","        \n","        \n","        self.norm = nn.LayerNorm(embed_size)\n","        \n","        \n","        self.transformer_block = TransformerBlock(\n","            embed_size, heads, dropout, forward_expansion\n","        )\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","        \n","        attention = self.attention(x, x, x, trg_mask)\n","        \n","        \n","        query = self.dropout(self.norm(attention + x))\n","        \n","        \n","        out = self.transformer_block(value, key, query, src_mask)\n","        \n","        return out\n","        "]},{"cell_type":"markdown","id":"568de2af","metadata":{"papermill":{"duration":0.002952,"end_time":"2025-06-28T12:39:54.653442","exception":false,"start_time":"2025-06-28T12:39:54.65049","status":"completed"},"tags":[]},"source":["Transformer Decoder consisting of:\n","\n","    1. Target embeddings (word + positional)\n","    2. Stack of Decoder blocks\n","    3. Output projection to vocabulary"]},{"cell_type":"code","execution_count":6,"id":"eaa776dc","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.661034Z","iopub.status.busy":"2025-06-28T12:39:54.660689Z","iopub.status.idle":"2025-06-28T12:39:54.668227Z","shell.execute_reply":"2025-06-28T12:39:54.667054Z"},"papermill":{"duration":0.013192,"end_time":"2025-06-28T12:39:54.669986","exception":false,"start_time":"2025-06-28T12:39:54.656794","status":"completed"},"tags":[]},"outputs":[],"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self,\n","                 trg_vocab_size,    \n","                 embed_size,        \n","                 num_layers,        \n","                 heads,             \n","                 forward_expansion, \n","                 dropout,           \n","                 device,            \n","                 max_length):       \n","        super(Decoder, self).__init__()\n","        self.device = device\n","\n","        \n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        \n","        \n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        \n","        self.layers = nn.ModuleList([\n","            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","            for _ in range(num_layers)\n","        ])\n","\n","        \n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        \n","        \n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","\n","       \n","        x = self.dropout(\n","            self.word_embedding(x) + self.position_embedding(positions)\n","        )\n","\n","       \n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","        \n","        out = self.fc_out(x)\n","        return out\n"]},{"cell_type":"markdown","id":"beca5bc0","metadata":{"papermill":{"duration":0.002896,"end_time":"2025-06-28T12:39:54.676215","exception":false,"start_time":"2025-06-28T12:39:54.673319","status":"completed"},"tags":[]},"source":["Complete Transformer model for sequence-to-sequence tasks.\n","    \n","    Architecture:\n","    Input -> Encoder -> Decoder -> Output\n","    \n","    The encoder processes the source sequence and the decoder generates\n","    the target sequence while attending to the encoder output."]},{"cell_type":"code","execution_count":7,"id":"231a04d0","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.683581Z","iopub.status.busy":"2025-06-28T12:39:54.683263Z","iopub.status.idle":"2025-06-28T12:39:54.691348Z","shell.execute_reply":"2025-06-28T12:39:54.690421Z"},"papermill":{"duration":0.013355,"end_time":"2025-06-28T12:39:54.69264","exception":false,"start_time":"2025-06-28T12:39:54.679285","status":"completed"},"tags":[]},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(\n","        self, \n","        src_vocab_size,     \n","        trg_vocab_size,     \n","        src_pad_idx,        \n","        trg_pad_idx,        \n","        embed_size=256,     \n","        num_layers=6,       \n","        forward_expansion=4, \n","        heads=8,           \n","        dropout=0,          \n","        device=\"cpu\",      \n","        max_length=100      \n","    ):\n","        super(Transformer, self).__init__()\n","        \n","        \n","        self.encoder = Encoder(\n","            src_vocab_size,\n","            embed_size, \n","            num_layers,\n","            heads, \n","            device,\n","            forward_expansion,\n","            dropout,\n","            max_length\n","        )\n","\n","      \n","        self.decoder = Decoder(\n","            trg_vocab_size,\n","            embed_size, \n","            num_layers,\n","            heads, \n","            forward_expansion,\n","            dropout,\n","            device,\n","            max_length\n","        )\n","\n","        \n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device \n","\n","    def make_src_mask(self, src):\n","    \n","        \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","       \n","        N, trg_len = trg.shape\n","        \n","        \n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len\n","        )\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        \n","        \n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        \n","        \n","        enc_src = self.encoder(src, src_mask)\n","        \n","        \n","        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n","        \n","        return out\n"]},{"cell_type":"code","execution_count":8,"id":"2566d584","metadata":{"execution":{"iopub.execute_input":"2025-06-28T12:39:54.701036Z","iopub.status.busy":"2025-06-28T12:39:54.700138Z","iopub.status.idle":"2025-06-28T12:39:55.059917Z","shell.execute_reply":"2025-06-28T12:39:55.058937Z"},"papermill":{"duration":0.36566,"end_time":"2025-06-28T12:39:55.06164","exception":false,"start_time":"2025-06-28T12:39:54.69598","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Output shape: torch.Size([2, 8, 10])\n"]}],"source":["\n","if __name__ == \"__main__\":\n","    device = torch.device(\"cpu\")\n","\n","    \n","    x = torch.tensor([\n","        [1, 5, 6, 4, 3, 9, 5, 2, 0],  \n","        [1, 8, 7, 3, 4, 5, 6, 7, 2]  \n","    ], dtype=torch.long).to(device)\n","\n","    \n","    trg = torch.tensor([\n","        [1, 7, 4, 3, 5, 9, 2, 0, 0],  \n","        [1, 5, 6, 2, 4, 7, 6, 2, 0]  \n","    ], dtype=torch.long).to(device)\n","\n","    \n","    src_pad_idx = 0      \n","    trg_pad_idx = 0      \n","    src_vocab_size = 10 \n","    trg_vocab_size = 10  \n","\n","    \n","    model = Transformer(\n","        src_vocab_size, \n","        trg_vocab_size, \n","        src_pad_idx, \n","        trg_pad_idx\n","    ).to(device)\n","\n","    \n","    out = model(x, trg[:, :-1]) \n","    \n","    print(\"Output shape:\", out.shape) \n","    "]},{"cell_type":"markdown","id":"aaec895c","metadata":{"papermill":{"duration":0.003227,"end_time":"2025-06-28T12:39:55.068436","exception":false,"start_time":"2025-06-28T12:39:55.065209","status":"completed"},"tags":[]},"source":["### Acknowledgement : [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":13.18183,"end_time":"2025-06-28T12:39:57.750928","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-28T12:39:44.569098","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}