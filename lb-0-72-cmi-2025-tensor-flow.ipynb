{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":242954653,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/lb-0-72-cmi-2025-tensor-flow?scriptVersionId=244173452\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## CMI - Detect Behavior with Sensor Data","metadata":{}},{"cell_type":"markdown","source":"The goal of this competition is to develop a predictive model that distinguishes BFRB-like and non-BFRB-like activity using data from a variety of sensors collected via a wrist-worn device. Successfully disentangling these behaviors will improve the design and accuracy of wearable BFRB-detection devices, which are relevant to a wide range of mental illnesses, ultimately strengthening the tools available to support their treatment.","metadata":{}},{"cell_type":"markdown","source":"### Importing the necessary Libraries","metadata":{}},{"cell_type":"code","source":"# Two‑Branch Human‑Activity‑Recognition Pipeline (IMU + Thermopile/TOF  + SE‑CNN + BiLSTM + Attention)\nimport os, json, joblib, numpy as np, pandas as pd\nfrom pathlib import Path\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n    Lambda, Concatenate\n)\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport polars as pl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:47:28.135132Z","iopub.execute_input":"2025-06-07T05:47:28.135379Z","iopub.status.idle":"2025-06-07T05:47:42.128839Z","shell.execute_reply.started":"2025-06-07T05:47:28.135348Z","shell.execute_reply":"2025-06-07T05:47:42.128293Z"}},"outputs":[{"name":"stderr","text":"2025-06-07 05:47:30.286510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749275250.460327      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749275250.511650      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"markdown","source":"This sets the hyperparameters and paths. Can play around the these values to get higher LB score. \n\nTraining hyperparameters:\n\nBATCH_SIZE: Minibatch size.\n\nPAD_PERCENTILE: Used for sequence padding.\n\nLR_INIT: Learning rate.\n\nWD: Weight decay (L2).\n\nMIXUP_ALPHA: If using mixup augmentation.\n\nEPOCHS: Max training epochs.\n\nPATIENCE: Early stopping patience.","metadata":{}},{"cell_type":"code","source":"# (Competition metric will only be imported when TRAINing)\nTRAIN = True                     # ← set to True when you want to train\nRAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nPRETRAINED_DIR = Path(\"/kaggle/input/pretrained-model\")  # used when TRAIN=False\nEXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\nBATCH_SIZE = 64\nPAD_PERCENTILE = 95\nLR_INIT = 5e-4\nWD = 3e-4\nMIXUP_ALPHA = 0.4\nEPOCHS = 160\nPATIENCE = 40\n\n\nprint(\"▶ imports ready · tensorflow\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:47:42.130491Z","iopub.execute_input":"2025-06-07T05:47:42.130953Z","iopub.status.idle":"2025-06-07T05:47:42.136032Z","shell.execute_reply.started":"2025-06-07T05:47:42.130932Z","shell.execute_reply":"2025-06-07T05:47:42.13473Z"}},"outputs":[{"name":"stdout","text":"▶ imports ready · tensorflow 2.18.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Utility Functions","metadata":{}},{"cell_type":"markdown","source":"Utility Fucntions are having modular helper fucntions which simplify the architecture design in TensorFlow/Keras models.","metadata":{}},{"cell_type":"code","source":"#Tensor Manipulations\ndef time_sum(x):\n    return K.sum(x, axis=1)\n\ndef squeeze_last_axis(x):\n    return tf.squeeze(x, axis=-1)\n\ndef expand_last_axis(x):\n    return tf.expand_dims(x, axis=-1)\n\ndef se_block(x, reduction=8):\n    ch = x.shape[-1]\n    se = GlobalAveragePooling1D()(x)\n    se = Dense(ch // reduction, activation='relu')(se)\n    se = Dense(ch, activation='sigmoid')(se)\n    se = Reshape((1, ch))(se)\n    return Multiply()([x, se])\n\n\n# Residual CNN Block with SE\ndef residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n    shortcut = x\n    for _ in range(2):\n        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n                   kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n    x = se_block(x)\n    if shortcut.shape[-1] != filters:\n        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n                          kernel_regularizer=l2(wd))(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n    x = add([x, shortcut])\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size)(x)\n    x = Dropout(drop)(x)\n    return x\n\ndef attention_layer(inputs):\n    score = Dense(1, activation='tanh')(inputs)\n    score = Lambda(squeeze_last_axis)(score)\n    weights = Activation('softmax')(score)\n    weights = Lambda(expand_last_axis)(weights)\n    context = Multiply()([inputs, weights])\n    context = Lambda(time_sum)(context)\n    return context\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:47:42.137085Z","iopub.execute_input":"2025-06-07T05:47:42.137355Z","iopub.status.idle":"2025-06-07T05:47:42.31615Z","shell.execute_reply.started":"2025-06-07T05:47:42.137332Z","shell.execute_reply":"2025-06-07T05:47:42.315506Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Data Helpers","metadata":{}},{"cell_type":"code","source":"# Normalizes and cleans the time series sequence. \n\ndef preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n    return scaler.transform(mat).astype('float32')\n\n# MixUp the data argumentation in order to regularize the neural network. \n\nclass MixupGenerator(Sequence):\n    def __init__(self, X, y, batch_size, alpha=0.2):\n        self.X, self.y = X, y\n        self.batch = batch_size\n        self.alpha = alpha\n        self.indices = np.arange(len(X))\n    def __len__(self):\n        return int(np.ceil(len(self.X) / self.batch))\n    def __getitem__(self, i):\n        idx = self.indices[i*self.batch:(i+1)*self.batch]\n        Xb, yb = self.X[idx], self.y[idx]\n        lam = np.random.beta(self.alpha, self.alpha)\n        perm = np.random.permutation(len(Xb))\n        X_mix = lam * Xb + (1-lam) * Xb[perm]\n        y_mix = lam * yb + (1-lam) * yb[perm]\n        return X_mix, y_mix\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:47:42.316958Z","iopub.execute_input":"2025-06-07T05:47:42.317206Z","iopub.status.idle":"2025-06-07T05:47:42.331897Z","shell.execute_reply.started":"2025-06-07T05:47:42.317185Z","shell.execute_reply":"2025-06-07T05:47:42.331337Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Model Definition - Two Branch Architecture","metadata":{}},{"cell_type":"code","source":"\n\ndef build_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n\n    # IMU deep branch\n    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.3, wd=wd)\n    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.3, wd=wd)\n\n    # TOF/Thermal lighter branch\n    x2 = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.3)(x2)\n    x2 = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2)\n    x2 = BatchNormalization()(x2); x2 = Activation('relu')(x2)\n    x2 = MaxPooling1D(2)(x2); x2 = Dropout(0.3)(x2)\n\n    merged = Concatenate()([x1, x2])\n\n    x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    x = Dropout(0.4)(x)\n    x = attention_layer(x)\n\n    for units, drop in [(256, 0.5), (128, 0.3)]:\n        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x); x = Activation('relu')(x)\n        x = Dropout(drop)(x)\n\n    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n    return Model(inp, out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:47:42.332628Z","iopub.execute_input":"2025-06-07T05:47:42.332795Z","iopub.status.idle":"2025-06-07T05:47:42.34883Z","shell.execute_reply.started":"2025-06-07T05:47:42.332782Z","shell.execute_reply":"2025-06-07T05:47:42.348304Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Training / Inference Pipeline","metadata":{}},{"cell_type":"code","source":"if TRAIN:\n    print(\"▶ TRAIN MODE – loading dataset …\")\n    df = pd.read_csv(RAW_DIR / \"train.csv\")\n\n    # label encoding\n    le = LabelEncoder(); df['gesture_int'] = le.fit_transform(df['gesture'])\n    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n\n    # feature list\n    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n                 'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n    feature_cols = [c for c in df.columns if c not in meta_cols]\n\n    imu_cols  = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n    tof_cols  = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n    print(f\"  IMU {len(imu_cols)} | TOF/THM {len(tof_cols)} | total {len(feature_cols)} features\")\n\n    # global scaler\n    scaler = StandardScaler().fit(df[feature_cols].ffill().bfill().fillna(0).values)\n    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n\n    # build sequences\n    seq_gp = df.groupby('sequence_id')\n    X_list, y_list, lens = [], [], []\n    for seq_id, seq in seq_gp:\n        mat = preprocess_sequence(seq, feature_cols, scaler)\n        X_list.append(mat)\n        y_list.append(seq['gesture_int'].iloc[0])\n        lens.append(len(mat))\n    pad_len = int(np.percentile(lens, PAD_PERCENTILE))\n    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(feature_cols))\n\n    X = pad_sequences(X_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    y = to_categorical(y_list, num_classes=len(le.classes_))\n\n    # split\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_list)\n\n    # class weights\n    cw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_list)\n    class_weight = dict(enumerate(cw_vals))\n\n    # model\n    model = build_two_branch_model(pad_len, len(imu_cols), len(tof_cols), len(le.classes_), wd=WD)\n    steps = len(X_tr)//BATCH_SIZE\n    lr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(LR_INIT, first_decay_steps=5*steps)\n    model.compile(optimizer=Adam(lr_sched),\n                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n                  metrics=['accuracy'])\n\n    train_gen = MixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\n    cb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1)\n    model.fit(train_gen, epochs=EPOCHS, validation_data=(X_val, y_val),\n              class_weight=class_weight, callbacks=[cb], verbose=1)\n\n    model.save(EXPORT_DIR / \"gesture_two_branch_mixup.h5\")\n    print(\"✔ Training done – artefacts saved in\", EXPORT_DIR)\n\n    # quick metric\n    from cmi_2025_metric_copy_for_import import CompetitionMetric\n    preds = model.predict(X_val).argmax(1)\n    true  = y_val.argmax(1)\n    h_f1 = CompetitionMetric().calculate_hierarchical_f1(\n        pd.DataFrame({'gesture': le.classes_[true]}),\n        pd.DataFrame({'gesture': le.classes_[preds]}))\n    print(\"Hold‑out H‑F1 =\", round(h_f1, 4))\n\nelse:\n    print(\"▶ INFERENCE MODE – loading artefacts from\", PRETRAINED_DIR)\n    feature_cols   = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n    pad_len        = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n    scaler         = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n\n    imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n    tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n\n    custom_objs = {\n        'time_sum': time_sum,\n        'squeeze_last_axis': squeeze_last_axis,\n        'expand_last_axis': expand_last_axis,\n        'se_block': se_block,\n        'residual_se_cnn_block': residual_se_cnn_block,\n        'attention_layer': attention_layer,\n    }\n    model = load_model(PRETRAINED_DIR / \"gesture_two_branch_mixup.h5\",\n                       compile=False, custom_objects=custom_objs)\n    print(\"  model, scaler, pads loaded – ready for evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:47:42.349599Z","iopub.execute_input":"2025-06-07T05:47:42.350343Z","iopub.status.idle":"2025-06-07T05:58:15.075277Z","shell.execute_reply.started":"2025-06-07T05:47:42.350322Z","shell.execute_reply":"2025-06-07T05:58:15.074513Z"}},"outputs":[{"name":"stdout","text":"▶ TRAIN MODE – loading dataset …\n  IMU 7 | TOF/THM 325 | total 332 features\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1749275321.716853      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/160\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1749275336.566808      95 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.1176 - loss: 3.5727 - val_accuracy: 0.2428 - val_loss: 3.1160\nEpoch 2/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.2455 - loss: 2.9499 - val_accuracy: 0.3170 - val_loss: 2.7769\nEpoch 3/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.2781 - loss: 2.8672 - val_accuracy: 0.3906 - val_loss: 2.5269\nEpoch 4/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3175 - loss: 2.7706 - val_accuracy: 0.4329 - val_loss: 2.4346\nEpoch 5/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3367 - loss: 2.7165 - val_accuracy: 0.4402 - val_loss: 2.3787\nEpoch 6/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3348 - loss: 2.7374 - val_accuracy: 0.4568 - val_loss: 2.3056\nEpoch 7/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3509 - loss: 2.6789 - val_accuracy: 0.4727 - val_loss: 2.2739\nEpoch 8/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.3911 - loss: 2.6558 - val_accuracy: 0.4746 - val_loss: 2.2287\nEpoch 9/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4063 - loss: 2.5503 - val_accuracy: 0.5083 - val_loss: 2.1721\nEpoch 10/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4202 - loss: 2.5674 - val_accuracy: 0.5230 - val_loss: 2.1137\nEpoch 11/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4511 - loss: 2.4285 - val_accuracy: 0.5334 - val_loss: 2.0676\nEpoch 12/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4507 - loss: 2.4914 - val_accuracy: 0.5457 - val_loss: 2.0504\nEpoch 13/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.4906 - loss: 2.3429 - val_accuracy: 0.5469 - val_loss: 2.0448\nEpoch 14/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4714 - loss: 2.3784 - val_accuracy: 0.5494 - val_loss: 2.0433\nEpoch 15/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.4732 - loss: 2.4356 - val_accuracy: 0.5402 - val_loss: 2.0555\nEpoch 16/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4448 - loss: 2.4667 - val_accuracy: 0.5353 - val_loss: 2.0631\nEpoch 17/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.4641 - loss: 2.5011 - val_accuracy: 0.5346 - val_loss: 2.0777\nEpoch 18/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.4796 - loss: 2.3473 - val_accuracy: 0.5297 - val_loss: 2.0212\nEpoch 19/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5096 - loss: 2.2745 - val_accuracy: 0.5671 - val_loss: 1.9240\nEpoch 20/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5274 - loss: 2.2667 - val_accuracy: 0.5635 - val_loss: 1.9254\nEpoch 21/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5451 - loss: 2.1776 - val_accuracy: 0.5788 - val_loss: 1.8894\nEpoch 22/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.5393 - loss: 2.2448 - val_accuracy: 0.5843 - val_loss: 1.8599\nEpoch 23/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5446 - loss: 2.2226 - val_accuracy: 0.6009 - val_loss: 1.8259\nEpoch 24/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5775 - loss: 2.0852 - val_accuracy: 0.6015 - val_loss: 1.8176\nEpoch 25/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5899 - loss: 2.0230 - val_accuracy: 0.6094 - val_loss: 1.7781\nEpoch 26/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5959 - loss: 2.0501 - val_accuracy: 0.6205 - val_loss: 1.7586\nEpoch 27/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6139 - loss: 2.0563 - val_accuracy: 0.6309 - val_loss: 1.7315\nEpoch 28/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5677 - loss: 2.1939 - val_accuracy: 0.6266 - val_loss: 1.7354\nEpoch 29/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6220 - loss: 2.0596 - val_accuracy: 0.6327 - val_loss: 1.7180\nEpoch 30/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6490 - loss: 1.9510 - val_accuracy: 0.6462 - val_loss: 1.7085\nEpoch 31/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6454 - loss: 2.0201 - val_accuracy: 0.6370 - val_loss: 1.7024\nEpoch 32/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.6276 - loss: 1.9540 - val_accuracy: 0.6395 - val_loss: 1.6958\nEpoch 33/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6496 - loss: 1.8984 - val_accuracy: 0.6401 - val_loss: 1.6932\nEpoch 34/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6476 - loss: 1.9497 - val_accuracy: 0.6383 - val_loss: 1.6929\nEpoch 35/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6440 - loss: 1.9393 - val_accuracy: 0.5843 - val_loss: 1.8312\nEpoch 36/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.5931 - loss: 2.1133 - val_accuracy: 0.5935 - val_loss: 1.8015\nEpoch 37/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6148 - loss: 1.9405 - val_accuracy: 0.6009 - val_loss: 1.7647\nEpoch 38/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6175 - loss: 2.0258 - val_accuracy: 0.6186 - val_loss: 1.7169\nEpoch 39/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6186 - loss: 2.0131 - val_accuracy: 0.6235 - val_loss: 1.7351\nEpoch 40/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6320 - loss: 1.9732 - val_accuracy: 0.6180 - val_loss: 1.7195\nEpoch 41/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.6441 - loss: 1.9454 - val_accuracy: 0.6205 - val_loss: 1.6945\nEpoch 42/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6212 - loss: 2.0006 - val_accuracy: 0.6272 - val_loss: 1.6758\nEpoch 43/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6585 - loss: 1.9224 - val_accuracy: 0.6143 - val_loss: 1.7325\nEpoch 44/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6326 - loss: 1.9755 - val_accuracy: 0.6487 - val_loss: 1.6371\nEpoch 45/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6512 - loss: 1.9397 - val_accuracy: 0.6481 - val_loss: 1.6073\nEpoch 46/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6786 - loss: 1.8582 - val_accuracy: 0.6475 - val_loss: 1.6044\nEpoch 47/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6796 - loss: 1.9154 - val_accuracy: 0.6517 - val_loss: 1.6142\nEpoch 48/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6812 - loss: 1.8135 - val_accuracy: 0.6462 - val_loss: 1.6097\nEpoch 49/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6635 - loss: 1.9238 - val_accuracy: 0.6475 - val_loss: 1.6150\nEpoch 50/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.6694 - loss: 1.8566 - val_accuracy: 0.6530 - val_loss: 1.5945\nEpoch 51/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.6733 - loss: 1.8844 - val_accuracy: 0.6560 - val_loss: 1.6030\nEpoch 52/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7164 - loss: 1.7207 - val_accuracy: 0.6634 - val_loss: 1.5708\nEpoch 53/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7096 - loss: 1.7868 - val_accuracy: 0.6554 - val_loss: 1.5800\nEpoch 54/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7016 - loss: 1.7859 - val_accuracy: 0.6671 - val_loss: 1.5457\nEpoch 55/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7137 - loss: 1.7990 - val_accuracy: 0.6560 - val_loss: 1.5520\nEpoch 56/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7620 - loss: 1.6853 - val_accuracy: 0.6665 - val_loss: 1.5462\nEpoch 57/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7434 - loss: 1.6556 - val_accuracy: 0.6597 - val_loss: 1.5303\nEpoch 58/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7124 - loss: 1.7338 - val_accuracy: 0.6726 - val_loss: 1.5430\nEpoch 59/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7299 - loss: 1.7246 - val_accuracy: 0.6769 - val_loss: 1.5415\nEpoch 60/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7447 - loss: 1.7028 - val_accuracy: 0.6959 - val_loss: 1.5128\nEpoch 61/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7512 - loss: 1.7358 - val_accuracy: 0.6849 - val_loss: 1.5033\nEpoch 62/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7617 - loss: 1.6363 - val_accuracy: 0.6726 - val_loss: 1.5174\nEpoch 63/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7741 - loss: 1.6361 - val_accuracy: 0.6922 - val_loss: 1.5020\nEpoch 64/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7461 - loss: 1.6413 - val_accuracy: 0.6842 - val_loss: 1.5098\nEpoch 65/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8032 - loss: 1.5240 - val_accuracy: 0.6965 - val_loss: 1.4989\nEpoch 66/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7762 - loss: 1.6325 - val_accuracy: 0.6910 - val_loss: 1.4978\nEpoch 67/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7628 - loss: 1.6225 - val_accuracy: 0.6959 - val_loss: 1.4921\nEpoch 68/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7884 - loss: 1.6142 - val_accuracy: 0.6910 - val_loss: 1.4945\nEpoch 69/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.7656 - loss: 1.6118 - val_accuracy: 0.6910 - val_loss: 1.5013\nEpoch 70/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8042 - loss: 1.5386 - val_accuracy: 0.6910 - val_loss: 1.4944\nEpoch 71/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7717 - loss: 1.6618 - val_accuracy: 0.6879 - val_loss: 1.4912\nEpoch 72/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7711 - loss: 1.6201 - val_accuracy: 0.6910 - val_loss: 1.4895\nEpoch 73/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7845 - loss: 1.6172 - val_accuracy: 0.6904 - val_loss: 1.4896\nEpoch 74/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8038 - loss: 1.5718 - val_accuracy: 0.6885 - val_loss: 1.4893\nEpoch 75/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7599 - loss: 1.7376 - val_accuracy: 0.6413 - val_loss: 1.6094\nEpoch 76/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7328 - loss: 1.6533 - val_accuracy: 0.6407 - val_loss: 1.5955\nEpoch 77/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7066 - loss: 1.7358 - val_accuracy: 0.6554 - val_loss: 1.5773\nEpoch 78/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.7206 - loss: 1.7090 - val_accuracy: 0.6603 - val_loss: 1.5725\nEpoch 79/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6967 - loss: 1.7374 - val_accuracy: 0.6573 - val_loss: 1.5833\nEpoch 80/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7169 - loss: 1.7536 - val_accuracy: 0.6585 - val_loss: 1.5720\nEpoch 81/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7031 - loss: 1.8053 - val_accuracy: 0.6738 - val_loss: 1.5412\nEpoch 82/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7099 - loss: 1.6791 - val_accuracy: 0.6800 - val_loss: 1.5198\nEpoch 83/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.7539 - loss: 1.6223 - val_accuracy: 0.6763 - val_loss: 1.5110\nEpoch 84/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7235 - loss: 1.7416 - val_accuracy: 0.6634 - val_loss: 1.5459\nEpoch 85/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7512 - loss: 1.6246 - val_accuracy: 0.6836 - val_loss: 1.5125\nEpoch 86/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7459 - loss: 1.7386 - val_accuracy: 0.6910 - val_loss: 1.5222\nEpoch 87/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.7652 - loss: 1.5964 - val_accuracy: 0.6824 - val_loss: 1.5367\nEpoch 88/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.7448 - loss: 1.6300 - val_accuracy: 0.6855 - val_loss: 1.5037\nEpoch 89/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7453 - loss: 1.7215 - val_accuracy: 0.6757 - val_loss: 1.5360\nEpoch 90/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7794 - loss: 1.6190 - val_accuracy: 0.6885 - val_loss: 1.5097\nEpoch 91/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7630 - loss: 1.6147 - val_accuracy: 0.6757 - val_loss: 1.5285\nEpoch 92/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7720 - loss: 1.5973 - val_accuracy: 0.6806 - val_loss: 1.5151\nEpoch 93/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7518 - loss: 1.6957 - val_accuracy: 0.6652 - val_loss: 1.5290\nEpoch 94/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7567 - loss: 1.7080 - val_accuracy: 0.6904 - val_loss: 1.4992\nEpoch 95/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.7920 - loss: 1.5531 - val_accuracy: 0.6781 - val_loss: 1.5086\nEpoch 96/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7890 - loss: 1.5130 - val_accuracy: 0.6855 - val_loss: 1.5214\nEpoch 97/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.7746 - loss: 1.5896 - val_accuracy: 0.6769 - val_loss: 1.5136\nEpoch 98/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.7982 - loss: 1.5520 - val_accuracy: 0.6787 - val_loss: 1.5167\nEpoch 99/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8040 - loss: 1.4995 - val_accuracy: 0.6934 - val_loss: 1.5167\nEpoch 100/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7978 - loss: 1.5607 - val_accuracy: 0.6726 - val_loss: 1.5417\nEpoch 101/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8070 - loss: 1.5216 - val_accuracy: 0.6977 - val_loss: 1.4879\nEpoch 102/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8137 - loss: 1.5313 - val_accuracy: 0.6916 - val_loss: 1.4868\nEpoch 103/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8234 - loss: 1.4634 - val_accuracy: 0.6904 - val_loss: 1.5006\nEpoch 104/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8057 - loss: 1.5630 - val_accuracy: 0.6941 - val_loss: 1.4784\nEpoch 105/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7986 - loss: 1.5491 - val_accuracy: 0.6867 - val_loss: 1.5258\nEpoch 106/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8028 - loss: 1.5280 - val_accuracy: 0.6965 - val_loss: 1.4991\nEpoch 107/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7995 - loss: 1.5275 - val_accuracy: 0.6953 - val_loss: 1.4785\nEpoch 108/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8233 - loss: 1.4716 - val_accuracy: 0.6916 - val_loss: 1.4864\nEpoch 109/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8068 - loss: 1.5451 - val_accuracy: 0.7008 - val_loss: 1.4639\nEpoch 110/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8371 - loss: 1.4822 - val_accuracy: 0.7063 - val_loss: 1.4656\nEpoch 111/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8434 - loss: 1.4626 - val_accuracy: 0.7075 - val_loss: 1.4470\nEpoch 112/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8374 - loss: 1.4310 - val_accuracy: 0.7057 - val_loss: 1.4642\nEpoch 113/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8353 - loss: 1.4230 - val_accuracy: 0.7155 - val_loss: 1.4417\nEpoch 114/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.8565 - loss: 1.4028 - val_accuracy: 0.7155 - val_loss: 1.4631\nEpoch 115/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.7976 - loss: 1.5338 - val_accuracy: 0.7057 - val_loss: 1.4502\nEpoch 116/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8288 - loss: 1.4985 - val_accuracy: 0.7124 - val_loss: 1.4561\nEpoch 117/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8118 - loss: 1.6043 - val_accuracy: 0.7088 - val_loss: 1.4606\nEpoch 118/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8319 - loss: 1.5631 - val_accuracy: 0.7112 - val_loss: 1.4511\nEpoch 119/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8658 - loss: 1.3344 - val_accuracy: 0.6947 - val_loss: 1.4907\nEpoch 120/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8507 - loss: 1.5010 - val_accuracy: 0.7198 - val_loss: 1.4502\nEpoch 121/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8676 - loss: 1.3685 - val_accuracy: 0.6977 - val_loss: 1.4803\nEpoch 122/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8576 - loss: 1.3970 - val_accuracy: 0.7155 - val_loss: 1.4358\nEpoch 123/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8369 - loss: 1.5434 - val_accuracy: 0.7112 - val_loss: 1.4398\nEpoch 124/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.8571 - loss: 1.4078 - val_accuracy: 0.7174 - val_loss: 1.4370\nEpoch 125/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8700 - loss: 1.4760 - val_accuracy: 0.7167 - val_loss: 1.4411\nEpoch 126/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8842 - loss: 1.3704 - val_accuracy: 0.7253 - val_loss: 1.4303\nEpoch 127/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8520 - loss: 1.4716 - val_accuracy: 0.7241 - val_loss: 1.4255\nEpoch 128/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8599 - loss: 1.4694 - val_accuracy: 0.7284 - val_loss: 1.4135\nEpoch 129/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8520 - loss: 1.4347 - val_accuracy: 0.7327 - val_loss: 1.4205\nEpoch 130/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.8849 - loss: 1.3700 - val_accuracy: 0.7265 - val_loss: 1.4181\nEpoch 131/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8556 - loss: 1.4694 - val_accuracy: 0.7198 - val_loss: 1.4299\nEpoch 132/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8553 - loss: 1.4066 - val_accuracy: 0.7351 - val_loss: 1.4073\nEpoch 133/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8420 - loss: 1.4475 - val_accuracy: 0.7376 - val_loss: 1.4209\nEpoch 134/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8840 - loss: 1.3284 - val_accuracy: 0.7308 - val_loss: 1.4166\nEpoch 135/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8534 - loss: 1.4464 - val_accuracy: 0.7223 - val_loss: 1.4348\nEpoch 136/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8855 - loss: 1.2957 - val_accuracy: 0.7272 - val_loss: 1.4325\nEpoch 137/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9013 - loss: 1.3243 - val_accuracy: 0.7351 - val_loss: 1.4014\nEpoch 138/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8583 - loss: 1.4336 - val_accuracy: 0.7413 - val_loss: 1.4052\nEpoch 139/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8889 - loss: 1.3896 - val_accuracy: 0.7364 - val_loss: 1.4085\nEpoch 140/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9159 - loss: 1.2722 - val_accuracy: 0.7376 - val_loss: 1.4212\nEpoch 141/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8596 - loss: 1.4230 - val_accuracy: 0.7327 - val_loss: 1.4139\nEpoch 142/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8915 - loss: 1.3626 - val_accuracy: 0.7376 - val_loss: 1.4200\nEpoch 143/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8980 - loss: 1.3228 - val_accuracy: 0.7400 - val_loss: 1.4119\nEpoch 144/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9089 - loss: 1.2818 - val_accuracy: 0.7419 - val_loss: 1.4065\nEpoch 145/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8756 - loss: 1.3225 - val_accuracy: 0.7376 - val_loss: 1.4115\nEpoch 146/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.8739 - loss: 1.4018 - val_accuracy: 0.7419 - val_loss: 1.4069\nEpoch 147/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8846 - loss: 1.4040 - val_accuracy: 0.7425 - val_loss: 1.4078\nEpoch 148/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8938 - loss: 1.2787 - val_accuracy: 0.7400 - val_loss: 1.4092\nEpoch 149/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8987 - loss: 1.3026 - val_accuracy: 0.7437 - val_loss: 1.4057\nEpoch 150/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8817 - loss: 1.3651 - val_accuracy: 0.7431 - val_loss: 1.4064\nEpoch 151/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8894 - loss: 1.3921 - val_accuracy: 0.7425 - val_loss: 1.4061\nEpoch 152/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9063 - loss: 1.3422 - val_accuracy: 0.7431 - val_loss: 1.4051\nEpoch 153/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.9107 - loss: 1.3811 - val_accuracy: 0.7437 - val_loss: 1.4048\nEpoch 154/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.8845 - loss: 1.3040 - val_accuracy: 0.6468 - val_loss: 1.5992\nEpoch 155/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7785 - loss: 1.5382 - val_accuracy: 0.6413 - val_loss: 1.5831\nEpoch 156/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7989 - loss: 1.5016 - val_accuracy: 0.6628 - val_loss: 1.5314\nEpoch 157/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8164 - loss: 1.4548 - val_accuracy: 0.6769 - val_loss: 1.4888\nEpoch 158/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8197 - loss: 1.5044 - val_accuracy: 0.6983 - val_loss: 1.4870\nEpoch 159/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8285 - loss: 1.4565 - val_accuracy: 0.6873 - val_loss: 1.5348\nEpoch 160/160\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.8037 - loss: 1.5421 - val_accuracy: 0.6836 - val_loss: 1.4685\nRestoring model weights from the end of the best epoch: 137.\n✔ Training done – artefacts saved in .\n\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\nHold‑out H‑F1 = 0.839\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Predicting","metadata":{}},{"cell_type":"code","source":"# make sure gesture_classes exists in both modes\nif TRAIN:\n    gesture_classes = le.classes_\n\n\ndef predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    global gesture_classes\n    if gesture_classes is None:\n        gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n\n    df_seq = sequence.to_pandas()\n    mat = preprocess_sequence(df_seq, feature_cols, scaler)\n    pad = pad_sequences([mat], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    idx = int(model.predict(pad, verbose=0).argmax(1)[0])\n    return str(gesture_classes[idx])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:58:15.082381Z","iopub.execute_input":"2025-06-07T05:58:15.082608Z","iopub.status.idle":"2025-06-07T05:58:15.087506Z","shell.execute_reply.started":"2025-06-07T05:58:15.08259Z","shell.execute_reply":"2025-06-07T05:58:15.086937Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Integration with Kaggle Evaluation","metadata":{}},{"cell_type":"code","source":"import kaggle_evaluation.cmi_inference_server\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T05:58:15.088222Z","iopub.execute_input":"2025-06-07T05:58:15.088523Z","iopub.status.idle":"2025-06-07T05:58:16.673203Z","shell.execute_reply.started":"2025-06-07T05:58:15.088494Z","shell.execute_reply":"2025-06-07T05:58:16.672487Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Acknowledgement:\n  [https://www.kaggle.com/code/vonmainstein/imu-tof](https://www.kaggle.com/code/vonmainstein/imu-tof)\n\n  All creadits goes to this notebook. Have tried some played with some parametes to get better score. Hope this is useful. ","metadata":{}}]}