{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":5549164,"sourceType":"datasetVersion","datasetId":3197305},{"sourceId":5792099,"sourceType":"datasetVersion","datasetId":3327296},{"sourceId":6247561,"sourceType":"datasetVersion","datasetId":3590060}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/cafa-6-protein-function-prediction-tf?scriptVersionId=273529044\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nimport os\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:02.801109Z","iopub.execute_input":"2025-11-04T20:14:02.801409Z","iopub.status.idle":"2025-11-04T20:14:16.728749Z","shell.execute_reply.started":"2025-11-04T20:14:02.801387Z","shell.execute_reply":"2025-11-04T20:14:16.728128Z"}},"outputs":[{"name":"stderr","text":"2025-11-04 20:14:04.462481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762287244.638837      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762287244.701750      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:16.729977Z","iopub.execute_input":"2025-11-04T20:14:16.730566Z","iopub.status.idle":"2025-11-04T20:14:17.236361Z","shell.execute_reply.started":"2025-11-04T20:14:16.730538Z","shell.execute_reply":"2025-11-04T20:14:17.235331Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.18.0\nGPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### SETUP & CONFIGURATION","metadata":{}},{"cell_type":"code","source":"class config:\n    MAIN_DIR = \"/kaggle/input/cafa-6-protein-function-prediction\"\n    \n    num_labels = 500\n    n_epochs = 20  \n    batch_size = 64 #128 \n    lr = 0.0005 #5e-4  \n    \n  \n    weight_decay = 1e-5\n    \n    \n    use_mixed_precision = True\n    \n    device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n    \n    \nprint(f\"Using device: {config.device}\")\n\n\nembeds_map = {\n    \"T5\": \"t5embeds\",\n    \"ProtBERT\": \"protbert-embeddings-for-cafa5\",\n    \"EMS2\": \"cafa-5-ems-2-embeddings-numpy\"\n}\nembeds_dim = {\n    \"T5\": 1024,\n    \"ProtBERT\": 1024,\n    \"EMS2\": 1280\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.237357Z","iopub.execute_input":"2025-11-04T20:14:17.237653Z","iopub.status.idle":"2025-11-04T20:14:17.269602Z","shell.execute_reply.started":"2025-11-04T20:14:17.237629Z","shell.execute_reply":"2025-11-04T20:14:17.26869Z"}},"outputs":[{"name":"stdout","text":"Using device: /GPU:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Loading the Data","metadata":{}},{"cell_type":"code","source":"def load_protein_data(datatype, embeddings_source):\n \n    base_path = f\"/kaggle/input/{embeds_map[embeddings_source]}/\"\n    \n  \n    embeds_path = os.path.join(base_path, f\"{datatype}_embeddings.npy\")\n    ids_path = os.path.join(base_path, f\"{datatype}_ids.npy\")\n    \n\n    if embeddings_source == \"T5\":\n        embeds_path = os.path.join(base_path, f\"{datatype}_embeds.npy\")\n\n    embeds = np.load(embeds_path)\n    ids = np.load(ids_path)\n    \n    if datatype == \"train\":\n        labels_path = f\"/kaggle/input/train-targets-top{config.num_labels}/train_targets_top{config.num_labels}.npy\"\n        labels = np.load(labels_path)\n        return embeds, labels, ids\n    else:\n        return embeds, ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.270475Z","iopub.execute_input":"2025-11-04T20:14:17.270746Z","iopub.status.idle":"2025-11-04T20:14:17.283905Z","shell.execute_reply.started":"2025-11-04T20:14:17.270722Z","shell.execute_reply":"2025-11-04T20:14:17.2832Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### MODEL ARCHITECTURE: 1D CNN ","metadata":{}},{"cell_type":"markdown","source":"we are building a 1D Convolutional Neural Network (CNN) for multi-label classification. Staring with input layer that reshapes the data so that is it fitted as per the NN requirements then we are applying 32 filters to get the baic features then 3 more Conv1D are applied to get the advances features. We are using GlobalAveragePooling layer so that the features are reduces to a compact form. Followed by dense layer and drop out to reduce overfitting. ","metadata":{}},{"cell_type":"code","source":"def build_cnn_model(input_dim, num_classes):\n\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Reshape((input_dim, 1))(inputs)\n    \n    \n    x = layers.Conv1D(64, 7, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.SpatialDropout1D(0.1)(x)\n    \n\n    residual = layers.Conv1D(128, 1, strides=2, padding='same')(x)\n    residual = layers.BatchNormalization()(residual)\n    \n    x = layers.Conv1D(128, 5, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.SpatialDropout1D(0.1)(x)\n    x = layers.MaxPooling1D(2)(x)\n    x = layers.Add()([x, residual])\n    x = layers.Activation('relu')(x)\n    \n  \n    residual = layers.Conv1D(256, 1, strides=2, padding='same')(x)\n    residual = layers.BatchNormalization()(residual)\n    \n    x = layers.Conv1D(256, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.SpatialDropout1D(0.2)(x)\n    x = layers.MaxPooling1D(2)(x)\n    x = layers.Add()([x, residual])\n    x = layers.Activation('relu')(x)\n    \n  \n    residual = layers.Conv1D(512, 1, padding='same')(x)\n    residual = layers.BatchNormalization()(residual)\n    \n    x = layers.Conv1D(512, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.SpatialDropout1D(0.2)(x)\n    x = layers.Add()([x, residual])\n    x = layers.Activation('relu')(x)\n    \n\n    x = layers.GlobalAveragePooling1D()(x)\n    \n\n    x = layers.Dense(256, kernel_regularizer=regularizers.l2(0.001))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Dropout(0.5)(x)\n    \n    x = layers.Dense(128, kernel_regularizer=regularizers.l2(0.001))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Dropout(0.4)(x)\n    \n \n    \n    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs)\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.286176Z","iopub.execute_input":"2025-11-04T20:14:17.286472Z","iopub.status.idle":"2025-11-04T20:14:17.30044Z","shell.execute_reply.started":"2025-11-04T20:14:17.286454Z","shell.execute_reply":"2025-11-04T20:14:17.299636Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### CUSTOM METRICS","metadata":{}},{"cell_type":"markdown","source":"F1-score is calculatedby tracking true positives, false positives, and false negatives during training. Predictions are converted to binary values using a defined threshold (default 0.5). From these values, precision and recall are computed, and the F1-score is derived.  The metric is reset after each epoch in order to track the metric correctly. The metric has configuration methods to ensure it is fully serializable and can be saved and loaded with the model. Keras will automatically serialize and deserialize this metric when the model is trained and reloaded, because the class is decorated with the @keras.utils.register_keras_serializable decorator.","metadata":{}},{"cell_type":"code","source":"@keras.utils.register_keras_serializable(package=\"Custom\", name=\"MultilabelF1Score\")\nclass MultilabelF1Score(keras.metrics.Metric):\n    \"\"\"\n    F1 Score metric with LOWER threshold for focal loss compatibility\n    \"\"\"\n    \n    def __init__(self, num_labels=500, threshold=0.1, name='f1_score', **kwargs):\n        # CRITICAL: Changed default threshold from 0.5 to 0.1\n        super().__init__(name=name, **kwargs)\n        self.num_labels = num_labels\n        self.threshold = threshold\n        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n    \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n        y_true = tf.cast(y_true, tf.float32)\n        \n        tp = tf.reduce_sum(y_true * y_pred)\n        fp = tf.reduce_sum((1 - y_true) * y_pred)\n        fn = tf.reduce_sum(y_true * (1 - y_pred))\n        \n        self.true_positives.assign_add(tp)\n        self.false_positives.assign_add(fp)\n        self.false_negatives.assign_add(fn)\n    \n    def result(self):\n        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n        return f1\n    \n    def reset_state(self):\n        self.true_positives.assign(0)\n        self.false_positives.assign(0)\n        self.false_negatives.assign(0)\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            'num_labels': self.num_labels,\n            'threshold': self.threshold\n        })\n        return config\n    \n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.301105Z","iopub.execute_input":"2025-11-04T20:14:17.301377Z","iopub.status.idle":"2025-11-04T20:14:17.313717Z","shell.execute_reply.started":"2025-11-04T20:14:17.301357Z","shell.execute_reply":"2025-11-04T20:14:17.312966Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### THRESHOLD FINDING FUNCTION","metadata":{}},{"cell_type":"markdown","source":"A function is created for the optimal prediction threshold that gives the highest F1-score","metadata":{}},{"cell_type":"code","source":"def find_best_threshold(model, X_val, y_val, thresholds=np.arange(0.05, 0.55, 0.05)):\n    \n    predictions = model.predict(X_val, batch_size=config.batch_size, verbose=0)\n    \n    best_f1 = 0\n    best_thresh = 0.5\n    threshold_scores = []\n    \n    for thresh in thresholds:\n        y_pred_binary = (predictions > thresh).astype(np.float32)\n        \n        tp = np.sum(y_val * y_pred_binary)\n        fp = np.sum((1 - y_val) * y_pred_binary)\n        fn = np.sum(y_val * (1 - y_pred_binary))\n        \n        precision = tp / (tp + fp + 1e-7)\n        recall = tp / (tp + fn + 1e-7)\n        f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n        \n        threshold_scores.append({\n            'threshold': thresh,\n            'f1': f1,\n            'precision': precision,\n            'recall': recall\n        })\n        \n        if f1 > best_f1:\n            best_f1 = f1\n            best_thresh = thresh\n    \n  \n    df_scores = pd.DataFrame(threshold_scores)\n    print(\"\\nThreshold Analysis:\")\n    print(df_scores.to_string())\n    \n    return best_f1, best_thresh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.314592Z","iopub.execute_input":"2025-11-04T20:14:17.314854Z","iopub.status.idle":"2025-11-04T20:14:17.332137Z","shell.execute_reply.started":"2025-11-04T20:14:17.31483Z","shell.execute_reply":"2025-11-04T20:14:17.331347Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def augment_embeddings(embeddings, labels, augment_factor=0.2):\n    \n    n_augment = int(len(embeddings) * augment_factor)\n    \n   \n    indices = np.random.choice(len(embeddings), n_augment, replace=True)\n    \n    augmented_embeddings = embeddings[indices].copy()\n    augmented_labels = labels[indices].copy()\n    \n \n    noise = np.random.normal(0, 0.01, augmented_embeddings.shape)\n    augmented_embeddings += noise\n    \n   \n    X_combined = np.vstack([embeddings, augmented_embeddings])\n    y_combined = np.vstack([labels, augmented_labels])\n    \n    return X_combined, y_combined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.332975Z","iopub.execute_input":"2025-11-04T20:14:17.333264Z","iopub.status.idle":"2025-11-04T20:14:17.350404Z","shell.execute_reply.started":"2025-11-04T20:14:17.333237Z","shell.execute_reply":"2025-11-04T20:14:17.34962Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### TRAINING FUNCTION","metadata":{}},{"cell_type":"markdown","source":"The function trains a convolutional neural network using the specified embedding source. It monitors performance on a validation set, calculates F1-score across multiple thresholds, and saves the model with the best F1 automatically. Finally, it returns the best model and optimal threshold for predictions.","metadata":{}},{"cell_type":"code","source":"def focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n        alpha_factor = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n        focal_weight = alpha_factor * tf.pow(1 - pt, gamma)\n        loss = -focal_weight * tf.math.log(pt)\n        return tf.reduce_mean(loss)\n    return loss\n\n\n\ndef train_model(embeddings_source, model_type=\"convolutional\", train_size=0.9, use_augmentation=True):\n    print(\"Loading training data...\")\n    X_train_full, y_train_full, ids = load_protein_data(\"train\", embeddings_source)\n    \n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_full, y_train_full, \n        train_size=train_size, \n        random_state=42\n    )\n    \n  \n    if use_augmentation:\n        print(\"Applying data augmentation...\")\n        X_train, y_train = augment_embeddings(X_train, y_train, augment_factor=0.2)\n        print(f\"Training samples after augmentation: {len(X_train)}\")\n    \n    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n    \n  \n    model = build_cnn_model(\n        input_dim=embeds_dim[embeddings_source], \n        num_classes=config.num_labels\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n        loss=focal_loss(gamma=2.0, alpha=0.25),\n        metrics=[\n            MultilabelF1Score(num_labels=config.num_labels, threshold=0.1),\n            keras.metrics.AUC(name='auc', multi_label=True)  # Add this\n        ]\n    )  \n\n    checkpoint = ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_auc',  \n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_auc',  \n        patience=5,\n        mode='max',\n        restore_best_weights=True,\n        verbose=1\n    )\n  \n    history = model.fit(\n        X_train, y_train,\n        batch_size=config.batch_size,\n        epochs=config.n_epochs,\n        validation_data=(X_val, y_val),\n        callbacks=[checkpoint, early_stop],\n        verbose=1\n    )\n    \n   \n    \n    \n  \n    best_val_f1, best_threshold = find_best_threshold(model, X_val, y_val)\n    print(f\"\\nBest Validation F1: {best_val_f1:.4f} at threshold {best_threshold:.2f}\")\n    \n    return model, best_threshold, history\n\n\nems2_model, best_threshold, history = train_model(\n    embeddings_source=\"EMS2\", \n    model_type=\"convolutional\",\n    use_augmentation=True  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:17.351224Z","iopub.execute_input":"2025-11-04T20:14:17.351506Z","iopub.status.idle":"2025-11-04T20:24:58.34981Z","shell.execute_reply.started":"2025-11-04T20:14:17.351478Z","shell.execute_reply":"2025-11-04T20:24:58.348996Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\nApplying data augmentation...\nTraining samples after augmentation: 153625\nTraining samples: 153625, Validation samples: 14225\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1762287271.586349      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1762287286.661035      97 service.cc:148] XLA service 0x7e599000d2f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1762287286.661725      97 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1762287287.719628      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   5/2401\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 32ms/step - auc: 0.4668 - f1_score: 0.0985 - loss: 0.6691  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1762287296.686723      97 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - auc: 0.5010 - f1_score: 0.1082 - loss: 0.0909\nEpoch 1: val_auc improved from -inf to 0.50035, saving model to best_model.keras\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 35ms/step - auc: 0.5010 - f1_score: 0.1082 - loss: 0.0909 - val_auc: 0.5003 - val_f1_score: 0.0986 - val_loss: 0.0172\nEpoch 2/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5015 - f1_score: 0.1078 - loss: 0.0174\nEpoch 2: val_auc did not improve from 0.50035\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 30ms/step - auc: 0.5015 - f1_score: 0.1078 - loss: 0.0174 - val_auc: 0.4989 - val_f1_score: 0.1034 - val_loss: 0.0170\nEpoch 3/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4997 - f1_score: 0.1052 - loss: 0.0171\nEpoch 3: val_auc improved from 0.50035 to 0.50545, saving model to best_model.keras\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 31ms/step - auc: 0.4997 - f1_score: 0.1052 - loss: 0.0171 - val_auc: 0.5055 - val_f1_score: 0.1202 - val_loss: 0.0169\nEpoch 4/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5002 - f1_score: 0.1038 - loss: 0.0169\nEpoch 4: val_auc did not improve from 0.50545\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 30ms/step - auc: 0.5002 - f1_score: 0.1038 - loss: 0.0169 - val_auc: 0.4989 - val_f1_score: 0.0971 - val_loss: 0.0166\nEpoch 5/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5002 - f1_score: 0.1027 - loss: 0.0167\nEpoch 5: val_auc did not improve from 0.50545\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 30ms/step - auc: 0.5002 - f1_score: 0.1027 - loss: 0.0167 - val_auc: 0.4996 - val_f1_score: 0.1100 - val_loss: 0.0165\nEpoch 6/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5012 - f1_score: 0.1011 - loss: 0.0165\nEpoch 6: val_auc did not improve from 0.50545\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 30ms/step - auc: 0.5012 - f1_score: 0.1011 - loss: 0.0165 - val_auc: 0.4993 - val_f1_score: 0.0976 - val_loss: 0.0162\nEpoch 7/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4969 - f1_score: 0.0998 - loss: 0.0163\nEpoch 7: val_auc did not improve from 0.50545\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 30ms/step - auc: 0.4969 - f1_score: 0.0998 - loss: 0.0163 - val_auc: 0.5008 - val_f1_score: 0.0982 - val_loss: 0.0161\nEpoch 8/20\n\u001b[1m2399/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4981 - f1_score: 0.0988 - loss: 0.0162\nEpoch 8: val_auc did not improve from 0.50545\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 30ms/step - auc: 0.4981 - f1_score: 0.0988 - loss: 0.0162 - val_auc: 0.4994 - val_f1_score: 0.0968 - val_loss: 0.0160\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.096333   0.050604  0.999958\n1       0.10  0.120154   0.064281  0.918649\n2       0.15  0.207492   0.121703  0.703124\n3       0.20  0.292994   0.204977  0.513484\n4       0.25  0.343348   0.320422  0.369808\n5       0.30  0.331479   0.439689  0.266012\n6       0.35  0.275963   0.505841  0.189737\n7       0.40  0.144683   0.633965  0.081660\n8       0.45  0.097110   0.644128  0.052513\n9       0.50  0.000000   0.000000  0.000000\n\nBest Validation F1: 0.3433 at threshold 0.25\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def predict_with_tta(model, X_test, threshold, n_tta=5):\n    \n    print(f\"Generating predictions with {n_tta} TTA iterations...\")\n    all_predictions = []\n    \n    for i in range(n_tta):\n        if i == 0:\n           \n            preds = model.predict(X_test, batch_size=config.batch_size, verbose=0)\n        else:\n           \n            X_noisy = X_test + np.random.normal(0, 0.005, X_test.shape)\n            preds = model.predict(X_noisy, batch_size=config.batch_size, verbose=0)\n        \n        all_predictions.append(preds)\n        print(f\"TTA iteration {i+1}/{n_tta} complete\")\n    \n    \n    final_predictions = np.mean(all_predictions, axis=0)\n    print(\"TTA averaging complete\")\n    \n    return final_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:24:58.350736Z","iopub.execute_input":"2025-11-04T20:24:58.351423Z","iopub.status.idle":"2025-11-04T20:24:58.357113Z","shell.execute_reply.started":"2025-11-04T20:24:58.351395Z","shell.execute_reply":"2025-11-04T20:24:58.356391Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def predict_ensemble(models, embeddings_source, thresholds, use_tta=False):\n    \n    print(\"\\n=== ENSEMBLE PREDICTION ===\")\n    print(f\"Number of models in ensemble: {len(models)}\")\n    \n    \n    print(\"\\nLoading test data...\")\n    X_test, test_ids = load_protein_data(\"test\", embeddings_source)\n    \n \n    labels_df = pd.read_csv(os.path.join(config.MAIN_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n    top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n    labels_names = top_terms.head(config.num_labels).index.values\n    \n    \n    all_predictions = []\n    for idx, (model, threshold) in enumerate(zip(models, thresholds)):\n        print(f\"\\nModel {idx+1}/{len(models)} - Threshold: {threshold:.2f}\")\n        \n        if use_tta:\n            predictions = predict_with_tta(model, X_test, threshold, n_tta=3)\n        else:\n            predictions = model.predict(X_test, batch_size=config.batch_size, verbose=1)\n        \n        all_predictions.append(predictions)\n    \n  \n    print(\"\\nAveraging ensemble predictions...\")\n    ensemble_predictions = np.mean(all_predictions, axis=0)\n    \n    \n    ensemble_threshold = np.mean(thresholds)\n    print(f\"Using ensemble threshold: {ensemble_threshold:.2f}\")\n    \n   \n    results = []\n    for i, protein_id in enumerate(tqdm(test_ids, desc=\"Processing ensemble predictions\")):\n        protein_probs = ensemble_predictions[i]\n        go_indices = np.where(protein_probs > ensemble_threshold)[0]\n        for idx in go_indices:\n            results.append({\n                \"Id\": protein_id,\n                \"GO term\": labels_names[idx],\n                \"Confidence\": float(protein_probs[idx])\n            })\n    \n    submission_df = pd.DataFrame(results)\n    print(f\"ENSEMBLE PREDICTIONS COMPLETE. Generated {len(submission_df)} predictions.\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:24:58.357945Z","iopub.execute_input":"2025-11-04T20:24:58.358212Z","iopub.status.idle":"2025-11-04T20:24:58.373922Z","shell.execute_reply.started":"2025-11-04T20:24:58.358189Z","shell.execute_reply":"2025-11-04T20:24:58.373129Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\ndef predict_in_batches(model, embeddings_source, threshold, batch_size=5000):\n\n    print(\"\\n=== BATCH PREDICTION (Memory-Efficient) ===\")\n    \n   \n    print(\"Loading test data...\")\n    X_test, test_ids = load_protein_data(\"test\", embeddings_source)\n    \n\n    labels_df = pd.read_csv(os.path.join(config.MAIN_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n    top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n    labels_names = top_terms.head(config.num_labels).index.values\n    \n    results = []\n    n_batches = (len(X_test) + batch_size - 1) // batch_size\n    \n    print(f\"Processing {len(X_test)} samples in {n_batches} batches...\")\n    \n    for i in tqdm(range(n_batches), desc=\"Batch prediction\"):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(X_test))\n        \n        batch_X = X_test[start_idx:end_idx]\n        batch_ids = test_ids[start_idx:end_idx]\n        \n    \n        predictions = model.predict(batch_X, batch_size=128, verbose=0)\n        \n        \n        for j, protein_id in enumerate(batch_ids):\n            protein_probs = predictions[j]\n            go_indices = np.where(protein_probs > threshold)[0]\n            \n            for idx in go_indices:\n                results.append({\n                    \"Id\": protein_id,\n                    \"GO term\": labels_names[idx],\n                    \"Confidence\": float(protein_probs[idx])\n                })\n        \n     \n        if i % 10 == 0:\n            del predictions\n            import gc\n            gc.collect()\n            tf.keras.backend.clear_session()\n    \n    submission_df = pd.DataFrame(results)\n    print(f\"BATCH PREDICTIONS COMPLETE. Generated {len(submission_df)} predictions.\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:24:58.374869Z","iopub.execute_input":"2025-11-04T20:24:58.375358Z","iopub.status.idle":"2025-11-04T20:24:58.391911Z","shell.execute_reply.started":"2025-11-04T20:24:58.375331Z","shell.execute_reply":"2025-11-04T20:24:58.391239Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ndef train_with_kfold(embeddings_source, n_folds=5, use_augmentation=True):\n  \n    from sklearn.model_selection import KFold\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"TRAINING WITH {n_folds}-FOLD CROSS-VALIDATION\")\n    print(\"=\"*70)\n    \n   \n    print(\"\\nLoading training data...\")\n    X_full, y_full, ids = load_protein_data(\"train\", embeddings_source)\n    print(f\"Total samples: {len(X_full)}\")\n    \n    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    models = []\n    thresholds = []\n    histories = []\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_full)):\n        print(\"\\n\" + \"=\"*70)\n        print(f\"FOLD {fold + 1}/{n_folds}\")\n        print(\"=\"*70)\n        \n      \n        X_train, X_val = X_full[train_idx], X_full[val_idx]\n        y_train, y_val = y_full[train_idx], y_full[val_idx]\n        \n      \n        if use_augmentation:\n            print(\"Applying data augmentation...\")\n            X_train, y_train = augment_embeddings(X_train, y_train, augment_factor=0.2)\n            print(f\"Augmented training samples: {len(X_train)}\")\n        \n        print(f\"Training: {len(X_train)}, Validation: {len(X_val)}\")\n        \n      \n        model = build_cnn_model(\n            input_dim=embeds_dim[embeddings_source],\n            num_classes=config.num_labels\n        )\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n            loss=focal_loss(gamma=2.0, alpha=0.25),\n            metrics=[\n                MultilabelF1Score(num_labels=config.num_labels, threshold=0.1),\n                keras.metrics.AUC(name='auc', multi_label=True)  \n            ]\n        )  \n    \n        checkpoint = ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_auc',  \n            save_best_only=True,\n            mode='max',\n            verbose=1\n        )\n        \n        early_stop = EarlyStopping(\n            monitor='val_auc',  \n            patience=5,\n            mode='max',\n            restore_best_weights=True,\n            verbose=1\n        )\n        \n       \n        history = model.fit(\n            X_train, y_train,\n            batch_size=config.batch_size,\n            epochs=config.n_epochs,\n            validation_data=(X_val, y_val),\n            callbacks=[checkpoint, early_stop],\n            verbose=1\n        )\n        \n\n        \n        \n       \n        best_f1, best_thresh = find_best_threshold(model, X_val, y_val)\n        \n        print(f\"\\nFold {fold+1} Results:\")\n        print(f\"  Best F1: {best_f1:.4f}\")\n        print(f\"  Best Threshold: {best_thresh:.2f}\")\n        \n        models.append(model)\n        thresholds.append(best_thresh)\n        histories.append(history)\n        fold_scores.append(best_f1)\n        \n     \n        tf.keras.backend.clear_session()\n    \n   \n    print(\"\\n\" + \"=\"*70)\n    print(\"CROSS-VALIDATION SUMMARY\")\n    print(\"=\"*70)\n    for fold, (score, thresh) in enumerate(zip(fold_scores, thresholds)):\n        print(f\"Fold {fold+1}: F1={score:.4f}, Threshold={thresh:.2f}\")\n    print(f\"\\nMean F1: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")\n    print(f\"Mean Threshold: {np.mean(thresholds):.2f} ± {np.std(thresholds):.4f}\")\n    print(\"=\"*70)\n    \n    return models, thresholds, histories","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:24:58.392758Z","iopub.execute_input":"2025-11-04T20:24:58.39312Z","iopub.status.idle":"2025-11-04T20:24:58.409418Z","shell.execute_reply.started":"2025-11-04T20:24:58.3931Z","shell.execute_reply":"2025-11-04T20:24:58.408539Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_single_fold_for_ensemble(embeddings_source, fold_num, train_size=0.9, seed=None):\n   \n    if seed is None:\n        seed = fold_num * 42\n    \n    print(f\"\\n=== Training Ensemble Model {fold_num} (seed={seed}) ===\")\n    \n   \n    X_train_full, y_train_full, ids = load_protein_data(\"train\", embeddings_source)\n    \n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_full, y_train_full,\n        train_size=train_size,\n        random_state=seed\n    )\n    \n   \n    print(\"Applying data augmentation...\")\n    X_train, y_train = augment_embeddings(X_train, y_train, augment_factor=0.2)\n    print(f\"Training samples: {len(X_train)}, Validation: {len(X_val)}\")\n    \n  \n    model = build_cnn_model(\n        input_dim=embeds_dim[embeddings_source],\n        num_classes=config.num_labels\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n        loss=focal_loss(gamma=2.0, alpha=0.25),\n        metrics=[\n            MultilabelF1Score(num_labels=config.num_labels, threshold=0.1),\n            keras.metrics.AUC(name='auc', multi_label=True)  # Add this\n        ]\n    )  \n\n    checkpoint = ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_auc',  \n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_auc',  \n        patience=5,\n        mode='max',\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    history = model.fit(\n        X_train, y_train,\n        batch_size=config.batch_size,\n        epochs=config.n_epochs,\n        validation_data=(X_val, y_val),\n        callbacks=[checkpoint, early_stop],\n        verbose=1\n    )\n    \n  \n    model = keras.models.load_model(f'ensemble_model_{fold_num}.keras')\n    best_f1, best_thresh = find_best_threshold(model, X_val, y_val)\n    \n    print(f\"Model {fold_num} - F1: {best_f1:.4f}, Threshold: {best_thresh:.2f}\")\n    \n    tf.keras.backend.clear_session()\n    \n    return model, best_thresh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:24:58.411968Z","iopub.execute_input":"2025-11-04T20:24:58.412634Z","iopub.status.idle":"2025-11-04T20:24:58.425629Z","shell.execute_reply.started":"2025-11-04T20:24:58.412607Z","shell.execute_reply":"2025-11-04T20:24:58.424844Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### GENERATING PREDICTIONS ","metadata":{}},{"cell_type":"code","source":"\nprint(\"Starting K-Fold Cross-Validation Training...\")\nkfold_models, kfold_thresholds, kfold_histories = train_with_kfold(\n    embeddings_source=\"EMS2\",\n    n_folds=5,\n    use_augmentation=True\n)\n\n\nsubmission_df = predict_ensemble(\n    models=kfold_models,\n    embeddings_source=\"EMS2\",\n    thresholds=kfold_thresholds,\n    use_tta=True  \n)\n\nprint(\"K-FOLD ENSEMBLE PREDICTION COMPLETE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:24:58.426353Z","iopub.execute_input":"2025-11-04T20:24:58.426601Z","iopub.status.idle":"2025-11-04T21:21:18.069647Z","shell.execute_reply.started":"2025-11-04T20:24:58.426583Z","shell.execute_reply":"2025-11-04T21:21:18.068921Z"}},"outputs":[{"name":"stdout","text":"Starting K-Fold Cross-Validation Training...\n\n======================================================================\nTRAINING WITH 5-FOLD CROSS-VALIDATION\n======================================================================\n\nLoading training data...\nTotal samples: 142246\n\n======================================================================\nFOLD 1/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136555\nTraining: 136555, Validation: 28450\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - auc: 0.4988 - f1_score: 0.1081 - loss: 0.0979\nEpoch 1: val_auc improved from -inf to 0.49798, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 37ms/step - auc: 0.4988 - f1_score: 0.1081 - loss: 0.0979 - val_auc: 0.4980 - val_f1_score: 0.0963 - val_loss: 0.0186\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4996 - f1_score: 0.1082 - loss: 0.0175\nEpoch 2: val_auc did not improve from 0.49798\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4996 - f1_score: 0.1082 - loss: 0.0175 - val_auc: 0.4979 - val_f1_score: 0.0972 - val_loss: 0.0169\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4990 - f1_score: 0.1055 - loss: 0.0171\nEpoch 3: val_auc improved from 0.49798 to 0.50184, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4990 - f1_score: 0.1055 - loss: 0.0171 - val_auc: 0.5018 - val_f1_score: 0.1038 - val_loss: 0.0168\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5004 - f1_score: 0.1038 - loss: 0.0169\nEpoch 4: val_auc did not improve from 0.50184\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 31ms/step - auc: 0.5004 - f1_score: 0.1038 - loss: 0.0169 - val_auc: 0.4996 - val_f1_score: 0.1002 - val_loss: 0.0166\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5013 - f1_score: 0.1028 - loss: 0.0167\nEpoch 5: val_auc did not improve from 0.50184\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 31ms/step - auc: 0.5013 - f1_score: 0.1028 - loss: 0.0167 - val_auc: 0.4995 - val_f1_score: 0.0989 - val_loss: 0.0166\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5026 - f1_score: 0.1019 - loss: 0.0166\nEpoch 6: val_auc did not improve from 0.50184\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 31ms/step - auc: 0.5026 - f1_score: 0.1019 - loss: 0.0166 - val_auc: 0.5006 - val_f1_score: 0.0968 - val_loss: 0.0166\nEpoch 7/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5006 - f1_score: 0.1010 - loss: 0.0165\nEpoch 7: val_auc did not improve from 0.50184\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.5006 - f1_score: 0.1010 - loss: 0.0165 - val_auc: 0.5003 - val_f1_score: 0.1061 - val_loss: 0.0164\nEpoch 8/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4989 - f1_score: 0.0997 - loss: 0.0163\nEpoch 8: val_auc did not improve from 0.50184\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4989 - f1_score: 0.0997 - loss: 0.0163 - val_auc: 0.4999 - val_f1_score: 0.0974 - val_loss: 0.0161\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.096282   0.050576  1.000000\n1       0.10  0.103850   0.054858  0.971218\n2       0.15  0.155309   0.086080  0.793388\n3       0.20  0.245196   0.154264  0.597250\n4       0.25  0.324445   0.268814  0.409110\n5       0.30  0.336662   0.408771  0.286179\n6       0.35  0.298918   0.489110  0.215226\n7       0.40  0.178784   0.611884  0.104686\n8       0.45  0.113079   0.644856  0.061973\n9       0.50  0.000000   0.000000  0.000000\n\nFold 1 Results:\n  Best F1: 0.3367\n  Best Threshold: 0.30\n\n======================================================================\nFOLD 2/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - auc: 0.5005 - f1_score: 0.1078 - loss: 0.0973\nEpoch 1: val_auc improved from -inf to 0.49980, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 37ms/step - auc: 0.5005 - f1_score: 0.1078 - loss: 0.0973 - val_auc: 0.4998 - val_f1_score: 0.0973 - val_loss: 0.0203\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4991 - f1_score: 0.1085 - loss: 0.0176\nEpoch 2: val_auc improved from 0.49980 to 0.50094, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4991 - f1_score: 0.1085 - loss: 0.0176 - val_auc: 0.5009 - val_f1_score: 0.1044 - val_loss: 0.0171\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4999 - f1_score: 0.1054 - loss: 0.0171\nEpoch 3: val_auc improved from 0.50094 to 0.50241, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4999 - f1_score: 0.1054 - loss: 0.0171 - val_auc: 0.5024 - val_f1_score: 0.0990 - val_loss: 0.0170\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4993 - f1_score: 0.1039 - loss: 0.0169\nEpoch 4: val_auc did not improve from 0.50241\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4994 - f1_score: 0.1039 - loss: 0.0169 - val_auc: 0.4967 - val_f1_score: 0.0987 - val_loss: 0.0167\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5025 - f1_score: 0.1030 - loss: 0.0168\nEpoch 5: val_auc did not improve from 0.50241\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.5025 - f1_score: 0.1030 - loss: 0.0168 - val_auc: 0.4979 - val_f1_score: 0.1026 - val_loss: 0.0165\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4989 - f1_score: 0.1015 - loss: 0.0166\nEpoch 6: val_auc did not improve from 0.50241\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4989 - f1_score: 0.1015 - loss: 0.0166 - val_auc: 0.4982 - val_f1_score: 0.1096 - val_loss: 0.0166\nEpoch 7/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4971 - f1_score: 0.1012 - loss: 0.0165\nEpoch 7: val_auc did not improve from 0.50241\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4971 - f1_score: 0.1012 - loss: 0.0165 - val_auc: 0.4983 - val_f1_score: 0.0989 - val_loss: 0.0164\nEpoch 8/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4992 - f1_score: 0.1003 - loss: 0.0164\nEpoch 8: val_auc did not improve from 0.50241\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4992 - f1_score: 0.1003 - loss: 0.0164 - val_auc: 0.4974 - val_f1_score: 0.0975 - val_loss: 0.0162\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.097166   0.051065  0.999756\n1       0.10  0.098999   0.052104  0.990297\n2       0.15  0.128682   0.069443  0.875747\n3       0.20  0.216139   0.128985  0.666460\n4       0.25  0.312099   0.238198  0.452482\n5       0.30  0.339085   0.393756  0.297745\n6       0.35  0.310717   0.482561  0.229124\n7       0.40  0.195407   0.601179  0.116664\n8       0.45  0.103327   0.646893  0.056147\n9       0.50  0.000000   0.000000  0.000000\n\nFold 2 Results:\n  Best F1: 0.3391\n  Best Threshold: 0.30\n\n======================================================================\nFOLD 3/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - auc: 0.5006 - f1_score: 0.1081 - loss: 0.0975\nEpoch 1: val_auc improved from -inf to 0.50293, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 37ms/step - auc: 0.5006 - f1_score: 0.1081 - loss: 0.0975 - val_auc: 0.5029 - val_f1_score: 0.0971 - val_loss: 0.0202\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4996 - f1_score: 0.1077 - loss: 0.0175\nEpoch 2: val_auc did not improve from 0.50293\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4996 - f1_score: 0.1077 - loss: 0.0175 - val_auc: 0.4982 - val_f1_score: 0.0966 - val_loss: 0.0166\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5010 - f1_score: 0.1049 - loss: 0.0170\nEpoch 3: val_auc did not improve from 0.50293\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5010 - f1_score: 0.1049 - loss: 0.0170 - val_auc: 0.4982 - val_f1_score: 0.1286 - val_loss: 0.0171\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5000 - f1_score: 0.1036 - loss: 0.0169\nEpoch 4: val_auc did not improve from 0.50293\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5000 - f1_score: 0.1036 - loss: 0.0169 - val_auc: 0.4995 - val_f1_score: 0.1008 - val_loss: 0.0164\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4995 - f1_score: 0.1014 - loss: 0.0165\nEpoch 5: val_auc did not improve from 0.50293\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4995 - f1_score: 0.1014 - loss: 0.0165 - val_auc: 0.5004 - val_f1_score: 0.0965 - val_loss: 0.0162\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4990 - f1_score: 0.1005 - loss: 0.0164\nEpoch 6: val_auc did not improve from 0.50293\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4990 - f1_score: 0.1005 - loss: 0.0164 - val_auc: 0.4993 - val_f1_score: 0.0966 - val_loss: 0.0161\nEpoch 6: early stopping\nRestoring model weights from the end of the best epoch: 1.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.096482   0.050686  1.000000\n1       0.10  0.097062   0.051012  0.997842\n2       0.15  0.112404   0.059822  0.928727\n3       0.20  0.139924   0.076706  0.795768\n4       0.25  0.175337   0.101958  0.625538\n5       0.30  0.260017   0.185439  0.434932\n6       0.35  0.320675   0.367621  0.284361\n7       0.40  0.239391   0.532260  0.154423\n8       0.45  0.119007   0.645997  0.065541\n9       0.50  0.000000   0.000000  0.000000\n\nFold 3 Results:\n  Best F1: 0.3207\n  Best Threshold: 0.35\n\n======================================================================\nFOLD 4/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - auc: 0.4998 - f1_score: 0.1090 - loss: 0.0984\nEpoch 1: val_auc improved from -inf to 0.49824, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 37ms/step - auc: 0.4998 - f1_score: 0.1090 - loss: 0.0984 - val_auc: 0.4982 - val_f1_score: 0.0970 - val_loss: 0.0358\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4991 - f1_score: 0.1086 - loss: 0.0176\nEpoch 2: val_auc improved from 0.49824 to 0.50083, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4991 - f1_score: 0.1086 - loss: 0.0176 - val_auc: 0.5008 - val_f1_score: 0.0974 - val_loss: 0.0243\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4988 - f1_score: 0.1061 - loss: 0.0172\nEpoch 3: val_auc did not improve from 0.50083\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4988 - f1_score: 0.1061 - loss: 0.0172 - val_auc: 0.4981 - val_f1_score: 0.1005 - val_loss: 0.0265\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4987 - f1_score: 0.1043 - loss: 0.0170\nEpoch 4: val_auc did not improve from 0.50083\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4987 - f1_score: 0.1043 - loss: 0.0170 - val_auc: 0.4998 - val_f1_score: 0.0969 - val_loss: 0.0188\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5006 - f1_score: 0.1031 - loss: 0.0167\nEpoch 5: val_auc improved from 0.50083 to 0.50360, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5006 - f1_score: 0.1031 - loss: 0.0167 - val_auc: 0.5036 - val_f1_score: 0.1019 - val_loss: 0.0171\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4990 - f1_score: 0.1028 - loss: 0.0167\nEpoch 6: val_auc did not improve from 0.50360\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4990 - f1_score: 0.1028 - loss: 0.0167 - val_auc: 0.4969 - val_f1_score: 0.0966 - val_loss: 0.0163\nEpoch 7/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4988 - f1_score: 0.1002 - loss: 0.0163\nEpoch 7: val_auc did not improve from 0.50360\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4988 - f1_score: 0.1002 - loss: 0.0163 - val_auc: 0.4988 - val_f1_score: 0.0961 - val_loss: 0.0161\nEpoch 8/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4981 - f1_score: 0.0994 - loss: 0.0163\nEpoch 8: val_auc did not improve from 0.50360\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4981 - f1_score: 0.0994 - loss: 0.0163 - val_auc: 0.4999 - val_f1_score: 0.0963 - val_loss: 0.0160\nEpoch 9/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4992 - f1_score: 0.0989 - loss: 0.0162\nEpoch 9: val_auc did not improve from 0.50360\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4992 - f1_score: 0.0989 - loss: 0.0162 - val_auc: 0.5002 - val_f1_score: 0.0961 - val_loss: 0.0159\nEpoch 10/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4986 - f1_score: 0.0980 - loss: 0.0161\nEpoch 10: val_auc did not improve from 0.50360\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4986 - f1_score: 0.0980 - loss: 0.0161 - val_auc: 0.5002 - val_f1_score: 0.0961 - val_loss: 0.0159\nEpoch 10: early stopping\nRestoring model weights from the end of the best epoch: 5.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.096341   0.050610  0.999218\n1       0.10  0.101858   0.053746  0.971779\n2       0.15  0.121710   0.065394  0.876768\n3       0.20  0.198547   0.116417  0.674165\n4       0.25  0.314035   0.242042  0.446988\n5       0.30  0.339463   0.400695  0.294464\n6       0.35  0.313125   0.482245  0.231826\n7       0.40  0.187248   0.609964  0.110600\n8       0.45  0.090305   0.650835  0.048519\n9       0.50  0.000000   0.000000  0.000000\n\nFold 4 Results:\n  Best F1: 0.3395\n  Best Threshold: 0.30\n\n======================================================================\nFOLD 5/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - auc: 0.4988 - f1_score: 0.1080 - loss: 0.0977\nEpoch 1: val_auc improved from -inf to 0.50032, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 37ms/step - auc: 0.4988 - f1_score: 0.1080 - loss: 0.0977 - val_auc: 0.5003 - val_f1_score: 0.0998 - val_loss: 0.0182\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5003 - f1_score: 0.1086 - loss: 0.0176\nEpoch 2: val_auc improved from 0.50032 to 0.50173, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5003 - f1_score: 0.1086 - loss: 0.0176 - val_auc: 0.5017 - val_f1_score: 0.1025 - val_loss: 0.0170\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4995 - f1_score: 0.1059 - loss: 0.0172\nEpoch 3: val_auc did not improve from 0.50173\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4995 - f1_score: 0.1059 - loss: 0.0172 - val_auc: 0.5008 - val_f1_score: 0.0963 - val_loss: 0.0183\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5003 - f1_score: 0.1042 - loss: 0.0169\nEpoch 4: val_auc improved from 0.50173 to 0.50258, saving model to best_model.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5003 - f1_score: 0.1042 - loss: 0.0169 - val_auc: 0.5026 - val_f1_score: 0.0973 - val_loss: 0.0169\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4991 - f1_score: 0.1032 - loss: 0.0168\nEpoch 5: val_auc did not improve from 0.50258\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 32ms/step - auc: 0.4991 - f1_score: 0.1032 - loss: 0.0168 - val_auc: 0.5006 - val_f1_score: 0.1034 - val_loss: 0.0168\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4967 - f1_score: 0.1019 - loss: 0.0166\nEpoch 6: val_auc did not improve from 0.50258\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4967 - f1_score: 0.1019 - loss: 0.0166 - val_auc: 0.5025 - val_f1_score: 0.1027 - val_loss: 0.0165\nEpoch 7/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5003 - f1_score: 0.1010 - loss: 0.0165\nEpoch 7: val_auc did not improve from 0.50258\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5003 - f1_score: 0.1010 - loss: 0.0165 - val_auc: 0.5002 - val_f1_score: 0.0975 - val_loss: 0.0162\nEpoch 8/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.4991 - f1_score: 0.0995 - loss: 0.0163\nEpoch 8: val_auc did not improve from 0.50258\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.4991 - f1_score: 0.0995 - loss: 0.0163 - val_auc: 0.4993 - val_f1_score: 0.0968 - val_loss: 0.0161\nEpoch 9/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - auc: 0.5005 - f1_score: 0.0986 - loss: 0.0162\nEpoch 9: val_auc did not improve from 0.50258\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 32ms/step - auc: 0.5005 - f1_score: 0.0986 - loss: 0.0162 - val_auc: 0.5007 - val_f1_score: 0.0968 - val_loss: 0.0160\nEpoch 9: early stopping\nRestoring model weights from the end of the best epoch: 4.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.096288   0.050579  0.999983\n1       0.10  0.097266   0.051131  0.995265\n2       0.15  0.120736   0.064672  0.907105\n3       0.20  0.207566   0.122136  0.690652\n4       0.25  0.310404   0.232563  0.466571\n5       0.30  0.343350   0.379574  0.313437\n6       0.35  0.313923   0.480426  0.233127\n7       0.40  0.196316   0.601895  0.117285\n8       0.45  0.138071   0.650931  0.077226\n9       0.50  0.000000   0.000000  0.000000\n\nFold 5 Results:\n  Best F1: 0.3433\n  Best Threshold: 0.30\n\n======================================================================\nCROSS-VALIDATION SUMMARY\n======================================================================\nFold 1: F1=0.3367, Threshold=0.30\nFold 2: F1=0.3391, Threshold=0.30\nFold 3: F1=0.3207, Threshold=0.35\nFold 4: F1=0.3395, Threshold=0.30\nFold 5: F1=0.3433, Threshold=0.30\n\nMean F1: 0.3358 ± 0.0079\nMean Threshold: 0.31 ± 0.0200\n======================================================================\n\n=== ENSEMBLE PREDICTION ===\nNumber of models in ensemble: 5\n\nLoading test data...\n\nModel 1/5 - Threshold: 0.30\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 2/5 - Threshold: 0.30\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 3/5 - Threshold: 0.35\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 4/5 - Threshold: 0.30\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 5/5 - Threshold: 0.30\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nAveraging ensemble predictions...\nUsing ensemble threshold: 0.31\n","output_type":"stream"},{"name":"stderr","text":"Processing ensemble predictions: 100%|██████████| 141864/141864 [00:02<00:00, 64734.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"ENSEMBLE PREDICTIONS COMPLETE. Generated 2536781 predictions.\nK-FOLD ENSEMBLE PREDICTION COMPLETE!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### SUBMISSION FILE GENERATION ","metadata":{}},{"cell_type":"code","source":"print(\"\\nMerging submission files...\")\n\n\nsubmission2 = pd.read_csv('/kaggle/input/blast-quick-sprof-zero-pred/submission.tsv',\n                          sep='\\t', header=None, names=['Id', 'GO term', 'Confidence2'])\n\n\nsubs = pd.merge(submission_df, submission2, on=['Id', 'GO term'], how='outer')\n\n\nsubs['Confidence_combined'] = subs['Confidence2'].fillna(subs['Confidence'])\n\n\nfinal_submission = subs[['Id', 'GO term', 'Confidence_combined']]\nfinal_submission.to_csv('submission.tsv', sep='\\t', header=False, index=False)\n\nprint(\"Submission file 'submission.tsv' created successfully!\")\nprint(f\"It contains {len(final_submission)} predictions in total.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T21:21:18.070449Z","iopub.execute_input":"2025-11-04T21:21:18.070687Z","iopub.status.idle":"2025-11-04T21:22:07.072687Z","shell.execute_reply.started":"2025-11-04T21:21:18.07067Z","shell.execute_reply":"2025-11-04T21:22:07.071832Z"}},"outputs":[{"name":"stdout","text":"\nMerging submission files...\nSubmission file 'submission.tsv' created successfully!\nIt contains 13699025 predictions in total.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Acknowledgement: - [https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn](https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}