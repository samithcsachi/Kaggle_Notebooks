{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":5549164,"sourceType":"datasetVersion","datasetId":3197305},{"sourceId":5792099,"sourceType":"datasetVersion","datasetId":3327296},{"sourceId":6247561,"sourceType":"datasetVersion","datasetId":3590060}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/cafa-6-protein-function-prediction-tf?scriptVersionId=268977502\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nimport os\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:09.863644Z","iopub.execute_input":"2025-10-18T15:22:09.86395Z","iopub.status.idle":"2025-10-18T15:22:30.930252Z","shell.execute_reply.started":"2025-10-18T15:22:09.863923Z","shell.execute_reply":"2025-10-18T15:22:30.929364Z"}},"outputs":[{"name":"stderr","text":"2025-10-18 15:22:14.165587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760800934.571161      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760800934.695201      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:30.931161Z","iopub.execute_input":"2025-10-18T15:22:30.931608Z","iopub.status.idle":"2025-10-18T15:22:32.366832Z","shell.execute_reply.started":"2025-10-18T15:22:30.931587Z","shell.execute_reply":"2025-10-18T15:22:32.36594Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.18.0\nGPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### SETUP & CONFIGURATION","metadata":{}},{"cell_type":"code","source":"class config:\n    MAIN_DIR = \"/kaggle/input/cafa-6-protein-function-prediction\"\n    \n    num_labels = 500\n    n_epochs = 20  \n    batch_size = 64 #128 \n    lr = 0.001 #5e-4  \n    \n  \n    weight_decay = 1e-5\n    \n    \n    use_mixed_precision = True\n    \n    device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n    \n    \nprint(f\"Using device: {config.device}\")\n\n\nembeds_map = {\n    \"T5\": \"t5embeds\",\n    \"ProtBERT\": \"protbert-embeddings-for-cafa5\",\n    \"EMS2\": \"cafa-5-ems-2-embeddings-numpy\"\n}\nembeds_dim = {\n    \"T5\": 1024,\n    \"ProtBERT\": 1024,\n    \"EMS2\": 1280\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.368441Z","iopub.execute_input":"2025-10-18T15:22:32.368667Z","iopub.status.idle":"2025-10-18T15:22:32.390979Z","shell.execute_reply.started":"2025-10-18T15:22:32.368649Z","shell.execute_reply":"2025-10-18T15:22:32.390361Z"}},"outputs":[{"name":"stdout","text":"Using device: /GPU:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Loading the Data","metadata":{}},{"cell_type":"code","source":"def load_protein_data(datatype, embeddings_source):\n \n    base_path = f\"/kaggle/input/{embeds_map[embeddings_source]}/\"\n    \n  \n    embeds_path = os.path.join(base_path, f\"{datatype}_embeddings.npy\")\n    ids_path = os.path.join(base_path, f\"{datatype}_ids.npy\")\n    \n\n    if embeddings_source == \"T5\":\n        embeds_path = os.path.join(base_path, f\"{datatype}_embeds.npy\")\n\n    embeds = np.load(embeds_path)\n    ids = np.load(ids_path)\n    \n    if datatype == \"train\":\n        labels_path = f\"/kaggle/input/train-targets-top{config.num_labels}/train_targets_top{config.num_labels}.npy\"\n        labels = np.load(labels_path)\n        return embeds, labels, ids\n    else:\n        return embeds, ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.391549Z","iopub.execute_input":"2025-10-18T15:22:32.391729Z","iopub.status.idle":"2025-10-18T15:22:32.407568Z","shell.execute_reply.started":"2025-10-18T15:22:32.391714Z","shell.execute_reply":"2025-10-18T15:22:32.407014Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### MODEL ARCHITECTURE: 1D CNN ","metadata":{}},{"cell_type":"markdown","source":"we are building a 1D Convolutional Neural Network (CNN) for multi-label classification. Staring with input layer that reshapes the data so that is it fitted as per the NN requirements then we are applying 32 filters to get the baic features then 3 more Conv1D are applied to get the advances features. We are using GlobalAveragePooling layer so that the features are reduces to a compact form. Followed by dense layer and drop out to reduce overfitting. ","metadata":{}},{"cell_type":"code","source":"def build_cnn_model(input_dim, num_classes):\n    model = models.Sequential([\n        layers.Input(shape=(input_dim,)),\n        layers.Reshape((input_dim, 1)),\n        \n        \n        layers.Conv1D(64, kernel_size=7, padding='same'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.SpatialDropout1D(0.1), \n        layers.MaxPooling1D(pool_size=2),\n        \n        \n        layers.Conv1D(128, kernel_size=5, padding='same'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.SpatialDropout1D(0.1),\n        layers.MaxPooling1D(pool_size=2),\n        \n        layers.Conv1D(256, kernel_size=3, padding='same'),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.SpatialDropout1D(0.2),\n        layers.MaxPooling1D(pool_size=2),\n        \n        \n        layers.GlobalAveragePooling1D(),\n        \n       \n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.4),\n        layers.Dense(num_classes, activation='sigmoid')\n    ])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.408218Z","iopub.execute_input":"2025-10-18T15:22:32.408489Z","iopub.status.idle":"2025-10-18T15:22:32.428168Z","shell.execute_reply.started":"2025-10-18T15:22:32.408474Z","shell.execute_reply":"2025-10-18T15:22:32.427612Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### CUSTOM METRICS","metadata":{}},{"cell_type":"markdown","source":"F1-score is calculatedby tracking true positives, false positives, and false negatives during training. Predictions are converted to binary values using a defined threshold (default 0.5). From these values, precision and recall are computed, and the F1-score is derived.  The metric is reset after each epoch in order to track the metric correctly. The metric has configuration methods to ensure it is fully serializable and can be saved and loaded with the model. Keras will automatically serialize and deserialize this metric when the model is trained and reloaded, because the class is decorated with the @keras.utils.register_keras_serializable decorator.","metadata":{}},{"cell_type":"code","source":"@keras.utils.register_keras_serializable(package=\"Custom\", name=\"MultilabelF1Score\")\nclass MultilabelF1Score(keras.metrics.Metric):\n    \n    \n    def __init__(self, num_labels=500, threshold=0.5, name='f1_score', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.num_labels = num_labels\n        self.threshold = threshold\n        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n    \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n        y_true = tf.cast(y_true, tf.float32)\n        \n        tp = tf.reduce_sum(y_true * y_pred)\n        fp = tf.reduce_sum((1 - y_true) * y_pred)\n        fn = tf.reduce_sum(y_true * (1 - y_pred))\n        \n        self.true_positives.assign_add(tp)\n        self.false_positives.assign_add(fp)\n        self.false_negatives.assign_add(fn)\n    \n    def result(self):\n        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n        return f1\n    \n    def reset_state(self):\n        self.true_positives.assign(0)\n        self.false_positives.assign(0)\n        self.false_negatives.assign(0)\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            'num_labels': self.num_labels,\n            'threshold': self.threshold\n        })\n        return config\n    \n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.428839Z","iopub.execute_input":"2025-10-18T15:22:32.429106Z","iopub.status.idle":"2025-10-18T15:22:32.45354Z","shell.execute_reply.started":"2025-10-18T15:22:32.429089Z","shell.execute_reply":"2025-10-18T15:22:32.453017Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### THRESHOLD FINDING FUNCTION","metadata":{}},{"cell_type":"markdown","source":"A function is created for the optimal prediction threshold that gives the highest F1-score","metadata":{}},{"cell_type":"code","source":"# def find_best_threshold(model, X_val, y_val, thresholds=np.arange(0.1, 0.51, 0.05)):\n    \n#     predictions = model.predict(X_val, batch_size=config.batch_size, verbose=0)\n    \n#     best_f1 = 0\n#     best_thresh = 0.5\n    \n#     for thresh in thresholds:\n#         y_pred_binary = (predictions > thresh).astype(np.float32)\n        \n    \n#         tp = np.sum(y_val * y_pred_binary)\n#         fp = np.sum((1 - y_val) * y_pred_binary)\n#         fn = np.sum(y_val * (1 - y_pred_binary))\n        \n#         precision = tp / (tp + fp + 1e-7)\n#         recall = tp / (tp + fn + 1e-7)\n#         f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n        \n#         if f1 > best_f1:\n#             best_f1 = f1\n#             best_thresh = thresh\n    \n#     return best_f1, best_thresh\n\n\ndef find_best_threshold(model, X_val, y_val, thresholds=np.arange(0.05, 0.55, 0.05)):\n    \n    predictions = model.predict(X_val, batch_size=config.batch_size, verbose=0)\n    \n    best_f1 = 0\n    best_thresh = 0.5\n    threshold_scores = []\n    \n    for thresh in thresholds:\n        y_pred_binary = (predictions > thresh).astype(np.float32)\n        \n        tp = np.sum(y_val * y_pred_binary)\n        fp = np.sum((1 - y_val) * y_pred_binary)\n        fn = np.sum(y_val * (1 - y_pred_binary))\n        \n        precision = tp / (tp + fp + 1e-7)\n        recall = tp / (tp + fn + 1e-7)\n        f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n        \n        threshold_scores.append({\n            'threshold': thresh,\n            'f1': f1,\n            'precision': precision,\n            'recall': recall\n        })\n        \n        if f1 > best_f1:\n            best_f1 = f1\n            best_thresh = thresh\n    \n  \n    df_scores = pd.DataFrame(threshold_scores)\n    print(\"\\nThreshold Analysis:\")\n    print(df_scores.to_string())\n    \n    return best_f1, best_thresh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.454181Z","iopub.execute_input":"2025-10-18T15:22:32.454344Z","iopub.status.idle":"2025-10-18T15:22:32.477624Z","shell.execute_reply.started":"2025-10-18T15:22:32.454331Z","shell.execute_reply":"2025-10-18T15:22:32.477013Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def augment_embeddings(embeddings, labels, augment_factor=0.2):\n    \n    n_augment = int(len(embeddings) * augment_factor)\n    \n   \n    indices = np.random.choice(len(embeddings), n_augment, replace=True)\n    \n    augmented_embeddings = embeddings[indices].copy()\n    augmented_labels = labels[indices].copy()\n    \n \n    noise = np.random.normal(0, 0.01, augmented_embeddings.shape)\n    augmented_embeddings += noise\n    \n   \n    X_combined = np.vstack([embeddings, augmented_embeddings])\n    y_combined = np.vstack([labels, augmented_labels])\n    \n    return X_combined, y_combined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.478264Z","iopub.execute_input":"2025-10-18T15:22:32.478566Z","iopub.status.idle":"2025-10-18T15:22:32.497566Z","shell.execute_reply.started":"2025-10-18T15:22:32.47854Z","shell.execute_reply":"2025-10-18T15:22:32.497039Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### TRAINING FUNCTION","metadata":{}},{"cell_type":"markdown","source":"The function trains a convolutional neural network using the specified embedding source. It monitors performance on a validation set, calculates F1-score across multiple thresholds, and saves the model with the best F1 automatically. Finally, it returns the best model and optimal threshold for predictions.","metadata":{}},{"cell_type":"code","source":"def train_model(embeddings_source, model_type=\"convolutional\", train_size=0.9, use_augmentation=True):\n    print(\"Loading training data...\")\n    X_train_full, y_train_full, ids = load_protein_data(\"train\", embeddings_source)\n    \n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_full, y_train_full, \n        train_size=train_size, \n        random_state=42\n    )\n    \n  \n    if use_augmentation:\n        print(\"Applying data augmentation...\")\n        X_train, y_train = augment_embeddings(X_train, y_train, augment_factor=0.2)\n        print(f\"Training samples after augmentation: {len(X_train)}\")\n    \n    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n    \n  \n    model = build_cnn_model(\n        input_dim=embeds_dim[embeddings_source], \n        num_classes=config.num_labels\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=config.lr),\n        loss='binary_crossentropy',\n        metrics=[MultilabelF1Score(num_labels=config.num_labels)]\n    )\n    \n\n    checkpoint = ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_f1_score',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_f1_score',\n        patience=5,\n        mode='max',\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n  \n    history = model.fit(\n        X_train, y_train,\n        batch_size=config.batch_size,\n        epochs=config.n_epochs,\n        validation_data=(X_val, y_val),\n        callbacks=[checkpoint, early_stop],\n        verbose=1\n    )\n    \n   \n    model = keras.models.load_model('best_model.keras')\n    \n  \n    best_val_f1, best_threshold = find_best_threshold(model, X_val, y_val)\n    print(f\"\\nBest Validation F1: {best_val_f1:.4f} at threshold {best_threshold:.2f}\")\n    \n    return model, best_threshold, history\n\n\nems2_model, best_threshold, history = train_model(\n    embeddings_source=\"EMS2\", \n    model_type=\"convolutional\",\n    use_augmentation=True  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:22:32.499791Z","iopub.execute_input":"2025-10-18T15:22:32.500051Z","iopub.status.idle":"2025-10-18T15:29:09.84827Z","shell.execute_reply.started":"2025-10-18T15:22:32.500036Z","shell.execute_reply":"2025-10-18T15:29:09.847398Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\nApplying data augmentation...\nTraining samples after augmentation: 153625\nTraining samples: 153625, Validation samples: 14225\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1760800962.273375      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1760800962.274121      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1760800971.652891     100 service.cc:148] XLA service 0x2ba2ad50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1760800971.654704     100 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1760800971.654725     100 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1760800972.285906     100 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   7/2401\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 20ms/step - f1_score: 0.0969 - loss: 0.6658 ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1760800977.918479     100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1803 - loss: 0.1799\nEpoch 1: val_f1_score improved from -inf to 0.16896, saving model to best_model.keras\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 15ms/step - f1_score: 0.1803 - loss: 0.1799 - val_f1_score: 0.1690 - val_loss: 0.1643\nEpoch 2/20\n\u001b[1m2398/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1817 - loss: 0.1644\nEpoch 2: val_f1_score did not improve from 0.16896\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - f1_score: 0.1817 - loss: 0.1644 - val_f1_score: 0.1690 - val_loss: 0.1643\nEpoch 3/20\n\u001b[1m2398/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1824 - loss: 0.1644\nEpoch 3: val_f1_score did not improve from 0.16896\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1644 - val_f1_score: 0.1690 - val_loss: 0.1645\nEpoch 4/20\n\u001b[1m2397/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1651\nEpoch 4: val_f1_score did not improve from 0.16896\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1651 - val_f1_score: 0.1690 - val_loss: 0.1643\nEpoch 5/20\n\u001b[1m2397/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1822 - loss: 0.1649\nEpoch 5: val_f1_score did not improve from 0.16896\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - f1_score: 0.1822 - loss: 0.1649 - val_f1_score: 0.1690 - val_loss: 0.1644\nEpoch 6/20\n\u001b[1m2397/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1822 - loss: 0.1648\nEpoch 6: val_f1_score improved from 0.16896 to 0.19604, saving model to best_model.keras\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1822 - loss: 0.1648 - val_f1_score: 0.1960 - val_loss: 0.1642\nEpoch 7/20\n\u001b[1m2397/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1837 - loss: 0.1643\nEpoch 7: val_f1_score did not improve from 0.19604\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1837 - loss: 0.1643 - val_f1_score: 0.1690 - val_loss: 0.1643\nEpoch 8/20\n\u001b[1m2397/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1839 - loss: 0.1648\nEpoch 8: val_f1_score did not improve from 0.19604\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1839 - loss: 0.1648 - val_f1_score: 0.1690 - val_loss: 0.1643\nEpoch 9/20\n\u001b[1m2397/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1830 - loss: 0.1645\nEpoch 9: val_f1_score did not improve from 0.19604\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1830 - loss: 0.1645 - val_f1_score: 0.1960 - val_loss: 0.1643\nEpoch 10/20\n\u001b[1m2398/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1835 - loss: 0.1646\nEpoch 10: val_f1_score did not improve from 0.19604\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1835 - loss: 0.1646 - val_f1_score: 0.1690 - val_loss: 0.1642\nEpoch 11/20\n\u001b[1m2398/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1832 - loss: 0.1646\nEpoch 11: val_f1_score did not improve from 0.19604\n\u001b[1m2401/2401\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - f1_score: 0.1832 - loss: 0.1646 - val_f1_score: 0.1960 - val_loss: 0.1643\nEpoch 11: early stopping\nRestoring model weights from the end of the best epoch: 6.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.230172   0.138953  0.670018\n1       0.10  0.304906   0.220126  0.495899\n2       0.15  0.344774   0.322787  0.369975\n3       0.20  0.344056   0.369885  0.321599\n4       0.25  0.330507   0.443993  0.263226\n5       0.30  0.318171   0.468704  0.240825\n6       0.35  0.308454   0.479402  0.227375\n7       0.40  0.270168   0.514837  0.183136\n8       0.45  0.196041   0.594025  0.117391\n9       0.50  0.196041   0.594025  0.117391\n\nBest Validation F1: 0.3448 at threshold 0.15\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# def train_model(embeddings_source, model_type=\"convolutional\", train_size=0.9):\n    \n    \n#     print(\"Loading training data...\")\n#     X_train_full, y_train_full, ids = load_protein_data(\"train\", embeddings_source)\n    \n    \n#     X_train, X_val, y_train, y_val = train_test_split(\n#         X_train_full, y_train_full, \n#         train_size=train_size, \n#         random_state=42\n#     )\n    \n#     print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n    \n   \n#     if model_type == \"convolutional\":\n#         model = build_cnn_model(\n#             input_dim=embeds_dim[embeddings_source], \n#             num_classes=config.num_labels\n#         )\n#     else:\n#         raise ValueError(\"Unsupported model type\")\n    \n \n#     model.compile(\n#         optimizer=keras.optimizers.Adam(learning_rate=config.lr),\n#         loss='binary_crossentropy',\n#         metrics=[MultilabelF1Score(num_labels=config.num_labels)]\n#     )\n    \n#     print(model.summary())\n    \n   \n#     checkpoint = ModelCheckpoint(\n#         'best_model.keras',\n#         monitor='val_loss',\n#         save_best_only=True,\n#         mode='min',\n#         verbose=1\n#     )\n    \n#     reduce_lr = ReduceLROnPlateau(\n#         monitor='val_f1_score',\n#         factor=0.1,\n#         patience=1,\n#         mode='max',\n#         verbose=1\n#     )\n    \n#     print(\"STARTING TRAINING...\")\n    \n    \n#     best_val_f1 = 0.0\n#     best_threshold = 0.5\n    \n#     for epoch in range(config.n_epochs):\n#         print(f\"\\nEPOCH {epoch+1}/{config.n_epochs}\")\n        \n   \n#         history = model.fit(\n#             X_train, y_train,\n#             batch_size=config.batch_size,\n#             epochs=1,\n#             verbose=1,\n#             validation_data=(X_val, y_val)\n#         )\n        \n     \n#         val_f1, val_threshold = find_best_threshold(model, X_val, y_val)\n        \n#         print(f\"Validation F1-Score: {val_f1:.4f} (at threshold: {val_threshold:.2f})\")\n        \n      \n#         if val_f1 > best_val_f1:\n#             best_val_f1 = val_f1\n#             best_threshold = val_threshold\n#             model.save('best_model.keras')\n#             print(f\"New best model saved! F1: {best_val_f1:.4f}\")\n        \n        \n#         current_lr = float(model.optimizer.learning_rate.numpy())\n#         if epoch > 0 and val_f1 < best_val_f1:\n#             new_lr = current_lr * 0.1\n#             model.optimizer.learning_rate.assign(new_lr)\n#             print(f\"Reducing learning rate to {new_lr}\")\n    \n#     print(\"\\nTRAINING FINISHED\")\n#     print(f\"Highest Validation F1-Score: {best_val_f1:.4f}\")\n#     print(f\"Best threshold for this score: {best_threshold:.2f}\")\n    \n\n#     model = keras.models.load_model('best_model.keras')\n    \n#     return model, best_threshold\n\n\n# ems2_model, best_threshold = train_model(embeddings_source=\"EMS2\", model_type=\"convolutional\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.849212Z","iopub.execute_input":"2025-10-18T15:29:09.849619Z","iopub.status.idle":"2025-10-18T15:29:09.854491Z","shell.execute_reply.started":"2025-10-18T15:29:09.849591Z","shell.execute_reply":"2025-10-18T15:29:09.853924Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def predict_with_tta(model, X_test, threshold, n_tta=5):\n    \n    print(f\"Generating predictions with {n_tta} TTA iterations...\")\n    all_predictions = []\n    \n    for i in range(n_tta):\n        if i == 0:\n           \n            preds = model.predict(X_test, batch_size=config.batch_size, verbose=0)\n        else:\n           \n            X_noisy = X_test + np.random.normal(0, 0.005, X_test.shape)\n            preds = model.predict(X_noisy, batch_size=config.batch_size, verbose=0)\n        \n        all_predictions.append(preds)\n        print(f\"TTA iteration {i+1}/{n_tta} complete\")\n    \n    \n    final_predictions = np.mean(all_predictions, axis=0)\n    print(\"TTA averaging complete\")\n    \n    return final_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.855196Z","iopub.execute_input":"2025-10-18T15:29:09.855403Z","iopub.status.idle":"2025-10-18T15:29:09.882561Z","shell.execute_reply.started":"2025-10-18T15:29:09.855388Z","shell.execute_reply":"2025-10-18T15:29:09.882034Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def predict_ensemble(models, embeddings_source, thresholds, use_tta=False):\n    \n    print(\"\\n=== ENSEMBLE PREDICTION ===\")\n    print(f\"Number of models in ensemble: {len(models)}\")\n    \n    \n    print(\"\\nLoading test data...\")\n    X_test, test_ids = load_protein_data(\"test\", embeddings_source)\n    \n \n    labels_df = pd.read_csv(os.path.join(config.MAIN_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n    top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n    labels_names = top_terms.head(config.num_labels).index.values\n    \n    \n    all_predictions = []\n    for idx, (model, threshold) in enumerate(zip(models, thresholds)):\n        print(f\"\\nModel {idx+1}/{len(models)} - Threshold: {threshold:.2f}\")\n        \n        if use_tta:\n            predictions = predict_with_tta(model, X_test, threshold, n_tta=3)\n        else:\n            predictions = model.predict(X_test, batch_size=config.batch_size, verbose=1)\n        \n        all_predictions.append(predictions)\n    \n  \n    print(\"\\nAveraging ensemble predictions...\")\n    ensemble_predictions = np.mean(all_predictions, axis=0)\n    \n    \n    ensemble_threshold = np.mean(thresholds)\n    print(f\"Using ensemble threshold: {ensemble_threshold:.2f}\")\n    \n   \n    results = []\n    for i, protein_id in enumerate(tqdm(test_ids, desc=\"Processing ensemble predictions\")):\n        protein_probs = ensemble_predictions[i]\n        go_indices = np.where(protein_probs > ensemble_threshold)[0]\n        for idx in go_indices:\n            results.append({\n                \"Id\": protein_id,\n                \"GO term\": labels_names[idx],\n                \"Confidence\": float(protein_probs[idx])\n            })\n    \n    submission_df = pd.DataFrame(results)\n    print(f\"ENSEMBLE PREDICTIONS COMPLETE. Generated {len(submission_df)} predictions.\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.883277Z","iopub.execute_input":"2025-10-18T15:29:09.883608Z","iopub.status.idle":"2025-10-18T15:29:09.906646Z","shell.execute_reply.started":"2025-10-18T15:29:09.883586Z","shell.execute_reply":"2025-10-18T15:29:09.905811Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ndef predict_in_batches(model, embeddings_source, threshold, batch_size=5000):\n\n    print(\"\\n=== BATCH PREDICTION (Memory-Efficient) ===\")\n    \n   \n    print(\"Loading test data...\")\n    X_test, test_ids = load_protein_data(\"test\", embeddings_source)\n    \n\n    labels_df = pd.read_csv(os.path.join(config.MAIN_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n    top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n    labels_names = top_terms.head(config.num_labels).index.values\n    \n    results = []\n    n_batches = (len(X_test) + batch_size - 1) // batch_size\n    \n    print(f\"Processing {len(X_test)} samples in {n_batches} batches...\")\n    \n    for i in tqdm(range(n_batches), desc=\"Batch prediction\"):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(X_test))\n        \n        batch_X = X_test[start_idx:end_idx]\n        batch_ids = test_ids[start_idx:end_idx]\n        \n    \n        predictions = model.predict(batch_X, batch_size=128, verbose=0)\n        \n        \n        for j, protein_id in enumerate(batch_ids):\n            protein_probs = predictions[j]\n            go_indices = np.where(protein_probs > threshold)[0]\n            \n            for idx in go_indices:\n                results.append({\n                    \"Id\": protein_id,\n                    \"GO term\": labels_names[idx],\n                    \"Confidence\": float(protein_probs[idx])\n                })\n        \n     \n        if i % 10 == 0:\n            del predictions\n            import gc\n            gc.collect()\n            tf.keras.backend.clear_session()\n    \n    submission_df = pd.DataFrame(results)\n    print(f\"BATCH PREDICTIONS COMPLETE. Generated {len(submission_df)} predictions.\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.907454Z","iopub.execute_input":"2025-10-18T15:29:09.907684Z","iopub.status.idle":"2025-10-18T15:29:09.930023Z","shell.execute_reply.started":"2025-10-18T15:29:09.907668Z","shell.execute_reply":"2025-10-18T15:29:09.929421Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\ndef train_with_kfold(embeddings_source, n_folds=5, use_augmentation=True):\n  \n    from sklearn.model_selection import KFold\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"TRAINING WITH {n_folds}-FOLD CROSS-VALIDATION\")\n    print(\"=\"*70)\n    \n   \n    print(\"\\nLoading training data...\")\n    X_full, y_full, ids = load_protein_data(\"train\", embeddings_source)\n    print(f\"Total samples: {len(X_full)}\")\n    \n    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    models = []\n    thresholds = []\n    histories = []\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_full)):\n        print(\"\\n\" + \"=\"*70)\n        print(f\"FOLD {fold + 1}/{n_folds}\")\n        print(\"=\"*70)\n        \n      \n        X_train, X_val = X_full[train_idx], X_full[val_idx]\n        y_train, y_val = y_full[train_idx], y_full[val_idx]\n        \n      \n        if use_augmentation:\n            print(\"Applying data augmentation...\")\n            X_train, y_train = augment_embeddings(X_train, y_train, augment_factor=0.2)\n            print(f\"Augmented training samples: {len(X_train)}\")\n        \n        print(f\"Training: {len(X_train)}, Validation: {len(X_val)}\")\n        \n      \n        model = build_cnn_model(\n            input_dim=embeds_dim[embeddings_source],\n            num_classes=config.num_labels\n        )\n        \n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=config.lr),\n            loss='binary_crossentropy',\n            metrics=[MultilabelF1Score(num_labels=config.num_labels)]\n        )\n        \n      \n        checkpoint = ModelCheckpoint(\n            f'best_model_fold{fold+1}.keras',\n            monitor='val_f1_score',\n            save_best_only=True,\n            mode='max',\n            verbose=1\n        )\n        \n        early_stop = EarlyStopping(\n            monitor='val_f1_score',\n            patience=5,\n            mode='max',\n            restore_best_weights=True,\n            verbose=1\n        )\n        \n       \n        history = model.fit(\n            X_train, y_train,\n            batch_size=config.batch_size,\n            epochs=config.n_epochs,\n            validation_data=(X_val, y_val),\n            callbacks=[checkpoint, early_stop],\n            verbose=1\n        )\n        \n\n        model = keras.models.load_model(f'best_model_fold{fold+1}.keras')\n        \n       \n        best_f1, best_thresh = find_best_threshold(model, X_val, y_val)\n        \n        print(f\"\\nFold {fold+1} Results:\")\n        print(f\"  Best F1: {best_f1:.4f}\")\n        print(f\"  Best Threshold: {best_thresh:.2f}\")\n        \n        models.append(model)\n        thresholds.append(best_thresh)\n        histories.append(history)\n        fold_scores.append(best_f1)\n        \n     \n        tf.keras.backend.clear_session()\n    \n   \n    print(\"\\n\" + \"=\"*70)\n    print(\"CROSS-VALIDATION SUMMARY\")\n    print(\"=\"*70)\n    for fold, (score, thresh) in enumerate(zip(fold_scores, thresholds)):\n        print(f\"Fold {fold+1}: F1={score:.4f}, Threshold={thresh:.2f}\")\n    print(f\"\\nMean F1: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")\n    print(f\"Mean Threshold: {np.mean(thresholds):.2f} ± {np.std(thresholds):.4f}\")\n    print(\"=\"*70)\n    \n    return models, thresholds, histories","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.93071Z","iopub.execute_input":"2025-10-18T15:29:09.930898Z","iopub.status.idle":"2025-10-18T15:29:09.953657Z","shell.execute_reply.started":"2025-10-18T15:29:09.930863Z","shell.execute_reply":"2025-10-18T15:29:09.95291Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_single_fold_for_ensemble(embeddings_source, fold_num, train_size=0.9, seed=None):\n   \n    if seed is None:\n        seed = fold_num * 42\n    \n    print(f\"\\n=== Training Ensemble Model {fold_num} (seed={seed}) ===\")\n    \n   \n    X_train_full, y_train_full, ids = load_protein_data(\"train\", embeddings_source)\n    \n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_full, y_train_full,\n        train_size=train_size,\n        random_state=seed\n    )\n    \n   \n    print(\"Applying data augmentation...\")\n    X_train, y_train = augment_embeddings(X_train, y_train, augment_factor=0.2)\n    print(f\"Training samples: {len(X_train)}, Validation: {len(X_val)}\")\n    \n  \n    model = build_cnn_model(\n        input_dim=embeds_dim[embeddings_source],\n        num_classes=config.num_labels\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=config.lr),\n        loss='binary_crossentropy',\n        metrics=[MultilabelF1Score(num_labels=config.num_labels)]\n    )\n    \n    checkpoint = ModelCheckpoint(\n        f'ensemble_model_{fold_num}.keras',\n        monitor='val_f1_score',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n    \n    early_stop = EarlyStopping(\n        monitor='val_f1_score',\n        patience=5,\n        mode='max',\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    history = model.fit(\n        X_train, y_train,\n        batch_size=config.batch_size,\n        epochs=config.n_epochs,\n        validation_data=(X_val, y_val),\n        callbacks=[checkpoint, early_stop],\n        verbose=1\n    )\n    \n  \n    model = keras.models.load_model(f'ensemble_model_{fold_num}.keras')\n    best_f1, best_thresh = find_best_threshold(model, X_val, y_val)\n    \n    print(f\"Model {fold_num} - F1: {best_f1:.4f}, Threshold: {best_thresh:.2f}\")\n    \n    tf.keras.backend.clear_session()\n    \n    return model, best_thresh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.954415Z","iopub.execute_input":"2025-10-18T15:29:09.955125Z","iopub.status.idle":"2025-10-18T15:29:09.978945Z","shell.execute_reply.started":"2025-10-18T15:29:09.955102Z","shell.execute_reply":"2025-10-18T15:29:09.978339Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### GENERATING PREDICTIONS ","metadata":{}},{"cell_type":"code","source":"\nprint(\"Starting K-Fold Cross-Validation Training...\")\nkfold_models, kfold_thresholds, kfold_histories = train_with_kfold(\n    embeddings_source=\"EMS2\",\n    n_folds=5,\n    use_augmentation=True\n)\n\n\nsubmission_df = predict_ensemble(\n    models=kfold_models,\n    embeddings_source=\"EMS2\",\n    thresholds=kfold_thresholds,\n    use_tta=True  \n)\n\nprint(\"K-FOLD ENSEMBLE PREDICTION COMPLETE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:29:09.979544Z","iopub.execute_input":"2025-10-18T15:29:09.979766Z","iopub.status.idle":"2025-10-18T15:55:05.060971Z","shell.execute_reply.started":"2025-10-18T15:29:09.97975Z","shell.execute_reply":"2025-10-18T15:55:05.060265Z"}},"outputs":[{"name":"stdout","text":"Starting K-Fold Cross-Validation Training...\n\n======================================================================\nTRAINING WITH 5-FOLD CROSS-VALIDATION\n======================================================================\n\nLoading training data...\nTotal samples: 142246\n\n======================================================================\nFOLD 1/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136555\nTraining: 136555, Validation: 28450\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - f1_score: 0.1810 - loss: 0.1814\nEpoch 1: val_f1_score improved from -inf to 0.16967, saving model to best_model_fold1.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 18ms/step - f1_score: 0.1810 - loss: 0.1813 - val_f1_score: 0.1697 - val_loss: 0.1642\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1837 - loss: 0.1652\nEpoch 2: val_f1_score did not improve from 0.16967\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1837 - loss: 0.1652 - val_f1_score: 0.1697 - val_loss: 0.1642\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1829 - loss: 0.1645\nEpoch 3: val_f1_score did not improve from 0.16967\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1829 - loss: 0.1645 - val_f1_score: 0.1697 - val_loss: 0.1642\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1828 - loss: 0.1652\nEpoch 4: val_f1_score did not improve from 0.16967\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1828 - loss: 0.1652 - val_f1_score: 0.1697 - val_loss: 0.1642\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1831 - loss: 0.1644\nEpoch 5: val_f1_score did not improve from 0.16967\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1831 - loss: 0.1644 - val_f1_score: 0.1697 - val_loss: 0.1642\nEpoch 6/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1837 - loss: 0.1646\nEpoch 6: val_f1_score improved from 0.16967 to 0.19666, saving model to best_model_fold1.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1837 - loss: 0.1646 - val_f1_score: 0.1967 - val_loss: 0.1642\nEpoch 7/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1830 - loss: 0.1651\nEpoch 7: val_f1_score did not improve from 0.19666\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1830 - loss: 0.1651 - val_f1_score: 0.1697 - val_loss: 0.1643\nEpoch 8/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1854 - loss: 0.1649\nEpoch 8: val_f1_score did not improve from 0.19666\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1854 - loss: 0.1649 - val_f1_score: 0.1967 - val_loss: 0.1641\nEpoch 9/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1837 - loss: 0.1642\nEpoch 9: val_f1_score did not improve from 0.19666\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 14ms/step - f1_score: 0.1837 - loss: 0.1642 - val_f1_score: 0.1967 - val_loss: 0.1642\nEpoch 10/20\n\u001b[1m2130/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1831 - loss: 0.1642\nEpoch 10: val_f1_score did not improve from 0.19666\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 14ms/step - f1_score: 0.1831 - loss: 0.1642 - val_f1_score: 0.1967 - val_loss: 0.1641\nEpoch 11/20\n\u001b[1m2130/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1845 - loss: 0.1644\nEpoch 11: val_f1_score did not improve from 0.19666\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 14ms/step - f1_score: 0.1845 - loss: 0.1644 - val_f1_score: 0.1697 - val_loss: 0.1641\nEpoch 11: early stopping\nRestoring model weights from the end of the best epoch: 6.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.231749   0.140296  0.665671\n1       0.10  0.309097   0.226962  0.484394\n2       0.15  0.345452   0.328739  0.363954\n3       0.20  0.340484   0.396833  0.298148\n4       0.25  0.331174   0.444743  0.263808\n5       0.30  0.319015   0.469785  0.241508\n6       0.35  0.309385   0.480680  0.228100\n7       0.40  0.285213   0.503227  0.198999\n8       0.45  0.196660   0.595641  0.117772\n9       0.50  0.196660   0.595641  0.117772\n\nFold 1 Results:\n  Best F1: 0.3455\n  Best Threshold: 0.15\n\n======================================================================\nFOLD 2/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - f1_score: 0.1810 - loss: 0.1810\nEpoch 1: val_f1_score improved from -inf to 0.16929, saving model to best_model_fold2.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 18ms/step - f1_score: 0.1810 - loss: 0.1810 - val_f1_score: 0.1693 - val_loss: 0.1654\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1828 - loss: 0.1642\nEpoch 2: val_f1_score did not improve from 0.16929\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1828 - loss: 0.1642 - val_f1_score: 0.1693 - val_loss: 0.1653\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1813 - loss: 0.1649\nEpoch 3: val_f1_score did not improve from 0.16929\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1813 - loss: 0.1649 - val_f1_score: 0.1693 - val_loss: 0.1654\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1807 - loss: 0.1645\nEpoch 4: val_f1_score improved from 0.16929 to 0.19662, saving model to best_model_fold2.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1807 - loss: 0.1645 - val_f1_score: 0.1966 - val_loss: 0.1652\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1647\nEpoch 5: val_f1_score did not improve from 0.19662\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1824 - loss: 0.1647 - val_f1_score: 0.1693 - val_loss: 0.1653\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1808 - loss: 0.1644\nEpoch 6: val_f1_score did not improve from 0.19662\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1808 - loss: 0.1644 - val_f1_score: 0.1693 - val_loss: 0.1653\nEpoch 7/20\n\u001b[1m2130/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1826 - loss: 0.1643\nEpoch 7: val_f1_score did not improve from 0.19662\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 14ms/step - f1_score: 0.1826 - loss: 0.1643 - val_f1_score: 0.1966 - val_loss: 0.1652\nEpoch 8/20\n\u001b[1m2130/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1817 - loss: 0.1644\nEpoch 8: val_f1_score did not improve from 0.19662\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 14ms/step - f1_score: 0.1817 - loss: 0.1644 - val_f1_score: 0.1966 - val_loss: 0.1652\nEpoch 9/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - f1_score: 0.1846 - loss: 0.1643\nEpoch 9: val_f1_score did not improve from 0.19662\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 14ms/step - f1_score: 0.1846 - loss: 0.1643 - val_f1_score: 0.1966 - val_loss: 0.1652\nEpoch 9: early stopping\nRestoring model weights from the end of the best epoch: 4.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.231698   0.140239  0.666098\n1       0.10  0.313304   0.233507  0.475951\n2       0.15  0.345099   0.324344  0.368692\n3       0.20  0.341938   0.400534  0.298299\n4       0.25  0.332138   0.448517  0.263712\n5       0.30  0.319670   0.473503  0.241282\n6       0.35  0.310094   0.484674  0.227977\n7       0.40  0.255167   0.534443  0.167591\n8       0.45  0.196615   0.599909  0.117575\n9       0.50  0.196615   0.599909  0.117575\n\nFold 2 Results:\n  Best F1: 0.3451\n  Best Threshold: 0.15\n\n======================================================================\nFOLD 3/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - f1_score: 0.1816 - loss: 0.1820\nEpoch 1: val_f1_score improved from -inf to 0.19680, saving model to best_model_fold3.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 17ms/step - f1_score: 0.1816 - loss: 0.1820 - val_f1_score: 0.1968 - val_loss: 0.1646\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1835 - loss: 0.1658\nEpoch 2: val_f1_score did not improve from 0.19680\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1835 - loss: 0.1658 - val_f1_score: 0.1968 - val_loss: 0.1644\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1841 - loss: 0.1647\nEpoch 3: val_f1_score did not improve from 0.19680\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1841 - loss: 0.1647 - val_f1_score: 0.1968 - val_loss: 0.1644\nEpoch 4/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1841 - loss: 0.1650\nEpoch 4: val_f1_score did not improve from 0.19680\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1841 - loss: 0.1650 - val_f1_score: 0.1968 - val_loss: 0.1644\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1842 - loss: 0.1651\nEpoch 5: val_f1_score did not improve from 0.19680\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1842 - loss: 0.1651 - val_f1_score: 0.1968 - val_loss: 0.1644\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1826 - loss: 0.1649\nEpoch 6: val_f1_score did not improve from 0.19680\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1826 - loss: 0.1649 - val_f1_score: 0.1699 - val_loss: 0.1644\nEpoch 6: early stopping\nRestoring model weights from the end of the best epoch: 1.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.224049   0.134045  0.681915\n1       0.10  0.303223   0.217423  0.500883\n2       0.15  0.344032   0.317345  0.375620\n3       0.20  0.346143   0.360297  0.333059\n4       0.25  0.331271   0.445485  0.263671\n5       0.30  0.318791   0.470134  0.241159\n6       0.35  0.309265   0.481206  0.227851\n7       0.40  0.277719   0.506236  0.191345\n8       0.45  0.217573   0.568286  0.134542\n9       0.50  0.196804   0.597167  0.117816\n\nFold 3 Results:\n  Best F1: 0.3461\n  Best Threshold: 0.20\n\n======================================================================\nFOLD 4/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - f1_score: 0.1791 - loss: 0.1816\nEpoch 1: val_f1_score improved from -inf to 0.19858, saving model to best_model_fold4.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 17ms/step - f1_score: 0.1791 - loss: 0.1816 - val_f1_score: 0.1986 - val_loss: 0.1641\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1821 - loss: 0.1647\nEpoch 2: val_f1_score did not improve from 0.19858\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - f1_score: 0.1821 - loss: 0.1647 - val_f1_score: 0.1711 - val_loss: 0.1637\nEpoch 3/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1807 - loss: 0.1651\nEpoch 3: val_f1_score did not improve from 0.19858\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1807 - loss: 0.1651 - val_f1_score: 0.1711 - val_loss: 0.1636\nEpoch 4/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1815 - loss: 0.1655\nEpoch 4: val_f1_score did not improve from 0.19858\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1815 - loss: 0.1655 - val_f1_score: 0.1711 - val_loss: 0.1636\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1649\nEpoch 5: val_f1_score did not improve from 0.19858\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1649 - val_f1_score: 0.1986 - val_loss: 0.1636\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1644\nEpoch 6: val_f1_score did not improve from 0.19858\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1824 - loss: 0.1644 - val_f1_score: 0.1986 - val_loss: 0.1636\nEpoch 6: early stopping\nRestoring model weights from the end of the best epoch: 1.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.217324   0.128779  0.695596\n1       0.10  0.300521   0.212535  0.512822\n2       0.15  0.340555   0.300110  0.393600\n3       0.20  0.346863   0.363599  0.331599\n4       0.25  0.337298   0.434754  0.275534\n5       0.30  0.333847   0.447927  0.266080\n6       0.35  0.312403   0.484747  0.230465\n7       0.40  0.287753   0.507104  0.200867\n8       0.45  0.198582   0.600626  0.118956\n9       0.50  0.198582   0.600626  0.118956\n\nFold 4 Results:\n  Best F1: 0.3469\n  Best Threshold: 0.20\n\n======================================================================\nFOLD 5/5\n======================================================================\nApplying data augmentation...\nAugmented training samples: 136556\nTraining: 136556, Validation: 28449\nEpoch 1/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - f1_score: 0.1792 - loss: 0.1818\nEpoch 1: val_f1_score improved from -inf to 0.17095, saving model to best_model_fold5.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 17ms/step - f1_score: 0.1792 - loss: 0.1818 - val_f1_score: 0.1709 - val_loss: 0.1642\nEpoch 2/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1820 - loss: 0.1649\nEpoch 2: val_f1_score improved from 0.17095 to 0.19820, saving model to best_model_fold5.keras\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1820 - loss: 0.1649 - val_f1_score: 0.1982 - val_loss: 0.1639\nEpoch 3/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1817 - loss: 0.1656\nEpoch 3: val_f1_score did not improve from 0.19820\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1817 - loss: 0.1656 - val_f1_score: 0.1709 - val_loss: 0.1641\nEpoch 4/20\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1807 - loss: 0.1646\nEpoch 4: val_f1_score did not improve from 0.19820\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 15ms/step - f1_score: 0.1807 - loss: 0.1646 - val_f1_score: 0.1709 - val_loss: 0.1639\nEpoch 5/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1820 - loss: 0.1649\nEpoch 5: val_f1_score did not improve from 0.19820\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1820 - loss: 0.1649 - val_f1_score: 0.1709 - val_loss: 0.1640\nEpoch 6/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1810 - loss: 0.1643\nEpoch 6: val_f1_score did not improve from 0.19820\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1810 - loss: 0.1643 - val_f1_score: 0.1709 - val_loss: 0.1639\nEpoch 7/20\n\u001b[1m2133/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - f1_score: 0.1804 - loss: 0.1648\nEpoch 7: val_f1_score did not improve from 0.19820\n\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - f1_score: 0.1804 - loss: 0.1648 - val_f1_score: 0.1709 - val_loss: 0.1639\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\nThreshold Analysis:\n   threshold        f1  precision    recall\n0       0.05  0.230921   0.139590  0.667952\n1       0.10  0.313755   0.234661  0.473278\n2       0.15  0.346517   0.329728  0.365106\n3       0.20  0.343207   0.388488  0.307379\n4       0.25  0.333010   0.447198  0.265275\n5       0.30  0.320902   0.472551  0.242939\n6       0.35  0.320902   0.472551  0.242939\n7       0.40  0.272609   0.519272  0.184818\n8       0.45  0.198196   0.600274  0.118693\n9       0.50  0.198196   0.600274  0.118693\n\nFold 5 Results:\n  Best F1: 0.3465\n  Best Threshold: 0.15\n\n======================================================================\nCROSS-VALIDATION SUMMARY\n======================================================================\nFold 1: F1=0.3455, Threshold=0.15\nFold 2: F1=0.3451, Threshold=0.15\nFold 3: F1=0.3461, Threshold=0.20\nFold 4: F1=0.3469, Threshold=0.20\nFold 5: F1=0.3465, Threshold=0.15\n\nMean F1: 0.3460 ± 0.0007\nMean Threshold: 0.17 ± 0.0245\n======================================================================\n\n=== ENSEMBLE PREDICTION ===\nNumber of models in ensemble: 5\n\nLoading test data...\n\nModel 1/5 - Threshold: 0.15\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 2/5 - Threshold: 0.15\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 3/5 - Threshold: 0.20\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 4/5 - Threshold: 0.20\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nModel 5/5 - Threshold: 0.15\nGenerating predictions with 3 TTA iterations...\nTTA iteration 1/3 complete\nTTA iteration 2/3 complete\nTTA iteration 3/3 complete\nTTA averaging complete\n\nAveraging ensemble predictions...\nUsing ensemble threshold: 0.17\n","output_type":"stream"},{"name":"stderr","text":"Processing ensemble predictions: 100%|██████████| 141864/141864 [00:02<00:00, 48953.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"ENSEMBLE PREDICTIONS COMPLETE. Generated 3830257 predictions.\nK-FOLD ENSEMBLE PREDICTION COMPLETE!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### SUBMISSION FILE GENERATION ","metadata":{}},{"cell_type":"code","source":"print(\"\\nMerging submission files...\")\n\n\nsubmission2 = pd.read_csv('/kaggle/input/blast-quick-sprof-zero-pred/submission.tsv',\n                          sep='\\t', header=None, names=['Id', 'GO term', 'Confidence2'])\n\n\nsubs = pd.merge(submission_df, submission2, on=['Id', 'GO term'], how='outer')\n\n\nsubs['Confidence_combined'] = subs['Confidence2'].fillna(subs['Confidence'])\n\n\nfinal_submission = subs[['Id', 'GO term', 'Confidence_combined']]\nfinal_submission.to_csv('submission.tsv', sep='\\t', header=False, index=False)\n\nprint(\"Submission file 'submission.tsv' created successfully!\")\nprint(f\"It contains {len(final_submission)} predictions in total.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:55:05.061701Z","iopub.execute_input":"2025-10-18T15:55:05.061913Z","iopub.status.idle":"2025-10-18T15:55:51.890405Z","shell.execute_reply.started":"2025-10-18T15:55:05.061889Z","shell.execute_reply":"2025-10-18T15:55:51.8897Z"}},"outputs":[{"name":"stdout","text":"\nMerging submission files...\nSubmission file 'submission.tsv' created successfully!\nIt contains 14822472 predictions in total.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Acknowledgement: - [https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn](https://www.kaggle.com/code/momerer/cafa-6-protein-function-prediction-with-1d-cnn)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}