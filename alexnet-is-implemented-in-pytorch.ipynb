{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/samithsachidanandan/alexnet-is-implemented-in-pytorch?scriptVersionId=273803486\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"3279822f","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:18.930582Z","iopub.status.busy":"2025-11-05T20:34:18.929633Z","iopub.status.idle":"2025-11-05T20:34:23.501298Z","shell.execute_reply":"2025-11-05T20:34:23.500707Z"},"papermill":{"duration":4.577585,"end_time":"2025-11-05T20:34:23.502641","exception":false,"start_time":"2025-11-05T20:34:18.925056","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"id":"3665091d","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:23.507857Z","iopub.status.busy":"2025-11-05T20:34:23.507558Z","iopub.status.idle":"2025-11-05T20:34:23.515631Z","shell.execute_reply":"2025-11-05T20:34:23.515061Z"},"papermill":{"duration":0.011799,"end_time":"2025-11-05T20:34:23.5167","exception":false,"start_time":"2025-11-05T20:34:23.504901","status":"completed"},"tags":[]},"outputs":[],"source":["class AlexNet(nn.Module):\n","    def __init__(self):\n","        super(AlexNet, self).__init__()\n","        # convolution part\n","        self.conv1 = nn.Conv2d(\n","            in_channels=3,\n","            out_channels=96,\n","            kernel_size=11,\n","            stride=4,\n","            padding=0\n","            )\n","        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n","        self.conv2 = nn.Conv2d(\n","            in_channels=96,\n","            out_channels=256,\n","            kernel_size=5,\n","            stride=1,\n","            padding=2\n","            )\n","        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n","        self.conv3 = nn.Conv2d(\n","            in_channels=256,\n","            out_channels=384,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1\n","            )\n","        self.conv4 = nn.Conv2d(\n","            in_channels=384,\n","            out_channels=256,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1\n","            )\n","        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n","        # dense part\n","        self.fc1 = nn.Linear(\n","            in_features=9216,\n","            out_features=4096\n","            )\n","        self.dropout1 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(\n","            in_features=4096,\n","            out_features=4096\n","            )\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc3 = nn.Linear(\n","            in_features=4096,\n","            out_features=1000\n","            )\n","        \n","    def forward(self, image):\n","        # get the batch size, channels, height and width\n","        # of the input batch of images\n","        # original size: (bs, 3, 227, 227)\n","        bs, c, h, w = image.size()\n","        x = F.relu(self.conv1(image)) # size: (bs, 96, 55, 55)\n","        x = self.pool1(x) # size: (bs, 96, 27, 27)\n","        x = F.relu(self.conv2(x)) # size: (bs, 256, 27, 27)\n","        x = self.pool2(x) # size: (bs, 256, 13, 13)\n","        x = F.relu(self.conv3(x)) # size: (bs, 384, 13, 13)\n","        x = F.relu(self.conv4(x)) # size: (bs, 256, 13, 13)\n","        x = self.pool3(x) # size: (bs, 256, 6, 6)\n","        x = x.view(bs, -1) # size: (bs, 9216)\n","        x = F.relu(self.fc1(x)) # size: (bs, 4096)\n","        x = self.dropout1(x) # size: (bs, 4096)\n","        # dropout does not change size\n","        # dropout is used for regularization\n","        # 0.3 dropout means that only 70% of the nodes\n","        # of the current layer are used for the next layer\n","        x = F.relu(self.fc2(x)) # size: (bs, 4096)\n","        x = self.dropout2(x) # size: (bs, 4096)\n","        x = F.relu(self.fc3(x)) # size: (bs, 1000)\n","        # 1000 is number of classes in ImageNet Dataset\n","        # softmax is an activation function that converts\n","        # linear output to probabilities that add up to 1\n","        # for each sample in the batch\n","        x = torch.softmax(x, axis=1) # size: (bs, 1000)\n","        return x"]},{"cell_type":"code","execution_count":3,"id":"45935bef","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:23.521319Z","iopub.status.busy":"2025-11-05T20:34:23.520717Z","iopub.status.idle":"2025-11-05T20:34:24.293988Z","shell.execute_reply":"2025-11-05T20:34:24.293132Z"},"papermill":{"duration":0.776659,"end_time":"2025-11-05T20:34:24.295193","exception":false,"start_time":"2025-11-05T20:34:23.518534","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Forward pass complete!\n","Output shape: torch.Size([1, 1000])\n","Prob sum: 0.9999997615814209\n"]}],"source":["if __name__ == \"__main__\":\n","    model = AlexNet()\n","\n","    x = torch.randn(1, 3, 227, 227)\n","    y = model(x)\n","\n","    print(\"Forward pass complete!\")\n","    print(\"Output shape:\", y.shape)\n","    print(\"Prob sum:\", y.sum(dim=1).item())"]},{"cell_type":"code","execution_count":4,"id":"c1c412ea","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:24.300351Z","iopub.status.busy":"2025-11-05T20:34:24.299757Z","iopub.status.idle":"2025-11-05T20:34:29.698532Z","shell.execute_reply":"2025-11-05T20:34:29.697916Z"},"papermill":{"duration":5.402763,"end_time":"2025-11-05T20:34:29.699919","exception":false,"start_time":"2025-11-05T20:34:24.297156","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":5,"id":"325fe03c","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:29.705117Z","iopub.status.busy":"2025-11-05T20:34:29.704725Z","iopub.status.idle":"2025-11-05T20:34:30.20524Z","shell.execute_reply":"2025-11-05T20:34:30.20461Z"},"papermill":{"duration":0.504491,"end_time":"2025-11-05T20:34:30.206588","exception":false,"start_time":"2025-11-05T20:34:29.702097","status":"completed"},"tags":[]},"outputs":[],"source":["model = AlexNet()\n","model.fc3 = nn.Linear(4096, 2)"]},{"cell_type":"code","execution_count":6,"id":"b057aa4b","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:30.211311Z","iopub.status.busy":"2025-11-05T20:34:30.210816Z","iopub.status.idle":"2025-11-05T20:34:30.214459Z","shell.execute_reply":"2025-11-05T20:34:30.213931Z"},"papermill":{"duration":0.006904,"end_time":"2025-11-05T20:34:30.215478","exception":false,"start_time":"2025-11-05T20:34:30.208574","status":"completed"},"tags":[]},"outputs":[],"source":["data_dir = \"/kaggle/input/pistachio-image-dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset/\"\n","\n","transform = transforms.Compose([\n","    transforms.Resize((227, 227)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n","\n"]},{"cell_type":"code","execution_count":7,"id":"5eaf0225","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:30.219907Z","iopub.status.busy":"2025-11-05T20:34:30.219358Z","iopub.status.idle":"2025-11-05T20:34:36.493443Z","shell.execute_reply":"2025-11-05T20:34:36.49265Z"},"papermill":{"duration":6.277582,"end_time":"2025-11-05T20:34:36.49482","exception":false,"start_time":"2025-11-05T20:34:30.217238","status":"completed"},"tags":[]},"outputs":[],"source":["test_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n","\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"]},{"cell_type":"code","execution_count":8,"id":"32898327","metadata":{"execution":{"iopub.execute_input":"2025-11-05T20:34:36.499562Z","iopub.status.busy":"2025-11-05T20:34:36.49934Z","iopub.status.idle":"2025-11-05T20:34:37.913195Z","shell.execute_reply":"2025-11-05T20:34:37.912289Z"},"papermill":{"duration":1.417405,"end_time":"2025-11-05T20:34:37.914371","exception":false,"start_time":"2025-11-05T20:34:36.496966","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Logits shape: torch.Size([32, 2])\n","Predictions: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","True labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","model.eval()\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        logits = model(images)          # raw outputs\n","        probs = torch.softmax(logits, dim=1)\n","        preds = torch.argmax(probs, dim=1)\n","\n","        print(\"Logits shape:\", logits.shape)\n","        print(\"Predictions:\", preds)\n","        print(\"True labels:\", labels)\n","        break"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2033813,"sourceId":3372753,"sourceType":"datasetVersion"}],"dockerImageVersionId":31153,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":24.615279,"end_time":"2025-11-05T20:34:39.536335","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-05T20:34:14.921056","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}